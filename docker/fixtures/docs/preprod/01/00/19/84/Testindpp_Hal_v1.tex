

\documentclass[a4paper,oneside,11pt]{article}

\usepackage[T1]{fontenc}             % pour les cesures ;
\usepackage[french,english]{babel}           %
\usepackage[utf8]{inputenc}        % pour les accents ;
\usepackage{amsmath,amssymb,dsfont}  % pour le mode mathematique ;
\usepackage{textcomp}                % pour les accents sur les majuscules ;
\usepackage{listings,multicol}       % pour la mise en page ;
\usepackage{graphicx,color}          % pour inserer des images ; et afficher directement des graphes ;
\usepackage{wrapfig}                 % pour habiller une image ;
\usepackage[rightcaption]{sidecap}   % pour mettre les legendes des figures a droite des images (avec SCfigure)
\usepackage{fullpage}                % mise en page plus grande ;
%\usepackage{a4wide}                 % mise en page plus grande ;
\usepackage{amsthm}
\usepackage{ulem}                    % pour personnaliser les soulignements
\usepackage{fancyhdr}                % pour personnaliser les entetes et pieds de pages ;
%\usepackage{wasysym}                % Pour des symboles bizarres (attention)
%\usepackage{MarVoSym}               % Pour des symboles bizarres (attention)
\usepackage{subeqnarray}


\setlength{\parindent}{10pt}      % taille de l'alinea quand on saute une ligne
\newcommand{\al}{\hspace{17,5pt}} % alinea dans les environnements df, thm...

     % Entetes et pieds de page :

\pagestyle{fancy} % pour personnaliser la mise en page ;
\lhead{}          % entete gauche ;
\rhead{}          % entete droite ;
\lfoot{}          % pied de page gauche ;
\rfoot{}          % pied de page droit ;
\renewcommand{\headrulewidth}{0cm} % trait de separation entete / texte
\renewcommand{\footrulewidth}{1pt} % trait de separation pied de page / texte


     % Mise en page : 

\textwidth 17cm     % largeur du texte ;
\textheight 24cm    % hauteur du texte ;
\topmargin 0cm      % marge du haut ;
\oddsidemargin -1cm % marge des pages impaires ;
\evensidemargin 0cm % marge des pages paires ;

\setlength{\parindent}{0pt}

     % Nouvelles commandes :

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lm}{Lemma}
\newtheorem{coro}{Corollary}
\newtheorem{cond}{\sc{Condition}}

\newenvironment{dem}{\noindent{\bf Proof :}}{\hfill             % demonstration ;
  $\square$\par \noindent}

\newenvironment{preuve}{\noindent{\bf Preuve :}}{\hfill                 % preuve ;
  $\square$\par \noindent}

\newcommand{\rmq}{\underline{\textit{Remarque}} : }                     % remarque ;


     % Raccourcis :

\newcommand{\pa}[1]{\ensuremath{\left( #1 \right)}}
\newcommand{\cro}[1]{\ensuremath{\left[ #1 \right]}}
\newcommand{\ac}[1]{\ensuremath{\left\{ #1 \right\}}}
\newcommand{\abs}[1]{\ensuremath{\left| #1 \right|}}


\newcommand{\Down}[1]{\textsubscript{#1}}
\newcommand{\Up}[1]{\textsuperscript{#1}}

 % Symboles :

\newcommand{\bl}{\ $\bullet$ \ }
\newcommand{\st}{\ $\star$ \ }
\newcommand{\tr}{\ $\triangleright$ \ }

 % Fonctions :

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\pen}{\operatorname{pen}}
\newcommand{\crit}{\operatorname{crit}}

\newcommand{\Nbc}{\varphi^{coinc}}
\newcommand{\h}{\operatorname{h}}

 % Abreviations :

\newcommand{\fct}[4]{\ensuremath{\left( \begin{array}{ccc}
                                 #1 & \longrightarrow & #2 \\
                                 #3 &   \longmapsto   & #4 \\
                               \end{array} \right) }}
\newcommand{\Stirl}[2]{\begin{Bmatrix} #1 \\ #2 \end{Bmatrix}}
\newcommand{\stirl}[2]{\ac{\begin{smallmatrix} #1 \\ #2 \end{smallmatrix}}}

\newcommand{\ie}{\underline{\textit{ie}} } 
\newcommand{\ssi}{\ \underline{\textit{ssi}} \ }
\newcommand{\tq}{\ /\ }

\newcommand{\K}{\ensuremath{\mathds{K}}}
\newcommand{\N}{\ensuremath{\mathds{N}}}
\newcommand{\Z}{\ensuremath{\mathds{Z}}}
\newcommand{\Q}{\ensuremath{\mathds{Q}}}
\newcommand{\R}{\ensuremath{\mathds{R}}}
\newcommand{\C}{\ensuremath{\mathds{C}}}
\newcommand{\X}{\ensuremath{\mathds{X}}}
\newcommand{\Y}{\ensuremath{\mathds{Y}}}
\renewcommand{\L}{\ensuremath{\mathds{L}}}
\newcommand{\calX}{\ensuremath{\mathcal{X}}}
\newcommand{\calD}{\ensuremath{\mathcal{D}}}
\newcommand{\calL}{\ensuremath{\mathcal{L}}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\ine}[2]{\ensuremath{\in \text{ \textlbrackdbl} #1 ; #2 \text{\textrbrackdbl }}}


 % Polices speciales : 

\newcommand{\ds}[1]{\ensuremath{\mathds{#1}}} 			% majuscules rayées en mode mathématique.
\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}          % majuscules rondes en mode mathématique.
\newcommand{\mfk}[1]{\ensuremath{\mathfrak{#1}}}		% lettres gothiques en mode mathématique.
\newcommand{\eus}[1]{{\usefont{U}{eus}{m}{n} #1}}       % majuscules mathematiques rondes ; (!!! Pas en mode mathematique !!!)

 % Probabilites :

\newcommand{\proba}[1]{\ensuremath{\mathds{P}\left(#1\right)}}			% probabilite ;
\newcommand{\esp}[1]{\ensuremath{\mathds{E}\left[  #1 \right]}} 		% esperance ;
\newcommand{\Estar}[1]{\ensuremath{\mathds{E}^*\!\!\left[  #1 \right]}} 		% esperance conditionnellement à l'échantillon de départ;
\newcommand{\esps}[2]{\ensuremath{\mathds{E}_{#1}\left[ #2 \right]}}	% esperance sous qqch;
\newcommand{\espI}[1]{\ensuremath{\ds E_{\independent}\!\cro{ #1 }}}
\newcommand{\var}[1]{\ensuremath{\mathds{V}\text{ar}\left(  #1 \right)}} 		% esperance ;
\newcommand{\Vstar}[1]{\ensuremath{\mathds{V}\text{ar}^*\!\!\left(  #1 \right)}}
\newcommand{\1}[1]{\ensuremath{\mathds{1}_{\left\{ #1 \right\}}}}		% fonction caracteristique d'un ensemble ;
\newcommand{\set}[2]{\ensuremath{\ac{#1,\dots,#2}}}

\newcommand{\PP}{\ensuremath{\mathds{P}}}

\newcommand{\Nm}{\ensuremath{\mc{N}(0,1)}}              % pour la loi normale ;
\newcommand{\loi}[1]{\ensuremath{\mc{L}\pa{#1}}}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\I}{\lambda}                                % densite du processus de Poisson ;
\newcommand{\perm}{\ensuremath{{\pi_n}}}
\newcommand{\rperm}{\ensuremath{ {\Pi_n} }}



\newcommand{\cv}[1]{\underset{#1}{\longrightarrow}}
\newcommand{\cvf}[1]{\underset{#1}{\Longrightarrow}}
\newcommand{\cvps}[1]{\overset{a.s.}{\underset{#1}{\longrightarrow}}}
\newcommand{\cvfps}[1]{\overset{a.s.}{\underset{#1}{\Longrightarrow}}}
\newcommand{\cvproba}[1]{\overset{\PP}{\underset{#1}{\longrightarrow}}}
\newcommand{\cvfproba}[1]{\overset{\PP}{\underset{#1}{\Longrightarrow}}}
\newcommand{\cvloi}[1]{\overset{\mc{L}}{\underset{#1}{\longrightarrow}}}

\bibliographystyle{plain}

\normalem % pour que le package ulem ne modifie pas les \emph...

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\dbox}[1]{\ensuremath{\begin{array}{|c|}\hline #1\\\hline\end{array}}}
\newcommand{\indep}{\ensuremath{\perp\!\!\!\perp}}
\newcommand{\Sn}[1]{\ensuremath{\mathfrak{S}_{#1}}}


\newcommand{\comMel}[1]{\textcolor{blue}{(#1)}}
\newcommand{\comMag}[1]{\textcolor{red}{(#1)}}
\definecolor{orange}{cmyk}{0,0.5,1,0.3}
\newcommand{\comPat}[1]{\textcolor{orange}{(#1)}}

     % Details du document :
     
% -------------------------------------------------------------------------------------------------------------------------- %
     
\begin{document}

\title{Bootstrap and permutation tests of independence for point processes}
\author{M\'elisande Albert\thanks{Univ. Nice Sophia Antipolis, CNRS, LJAD, UMR 7351, 06100 Nice, France.},~ Yann Bouret\thanks{Univ. Nice Sophia Antipolis, CNRS, LPMC, UMR 7336, 06100 Nice, France.},~ Magalie Fromont\thanks{Univ. Européenne de Bretagne, Institut de Recherche Math\'ematique de Rennes (IRMAR), France.} ~and~ 
Patricia Reynaud-Bouret\thanks{Univ. Nice Sophia Antipolis, CNRS, LJAD, UMR 7351, 06100 Nice, France.}}
\maketitle

\small{{\bf Abstract:} Motivated by a neuroscience question about synchrony detection in spike trains analysis, we deal with the independence testing problem for point processes. We introduce non-parametric test statistics, which are rescaled general $U$-statistics, whose corresponding critical values are constructed from bootstrap and randomisation or permutation approaches, making as few assumptions as possible on the underlying distribution of the point processes. We derive general consistency results for the bootstrap and for the permutation w.r.t. to Wasserstein's metric, which induce weak convergence as well as convergence of second order moments. The obtained bootstrap or permutation independence tests are thus proved to be asymptotically of the prescribed size, and to be consistent against any reasonable alternative, randomisation or permutation independence tests having the further advantage to be exactly (that is non-asymptotically) of the prescribed level, even when Monte Carlo methods are used to approximate the randomised quantiles. }

\bigskip

\noindent{\bf Mathematics Subject Classification:} 62M07, 62E20, 60G55, 60F05.

\medskip

\noindent{\bf Keywords:} Independence test, $U$-statistics, point processes, bootstrap, randomisation, permutation.


%\tableofcontents
%\newpage 


% ---------------------------------------------------------------------------------------------------- %
\section{Introduction}
\label{Intro}
% ---------------------------------------------------------------------------------------------------- %


Originally motivated by a request of neuroscientists about synchrony detection in spike trains studies, we focus in the present work on independence tests for point processes. The question of testing whether two random variables are independent is of course largely encountered in the statistical literature, as it is one of the central goals of data analysis. From the historical Pearson's (see \cite{Pearson1, Pearson2}) chi-square test of independence to the recent test of Gretton et al. \cite{Gretton1, Gretton2, Gretton3} using kernel methods in the spirit of statistical learning, many non-parametric independence tests have been developed for real valued random variables or even random vectors. Among them, of particular interest are the tests based on the randomisation or permutation principle introduced by Fisher \cite{Fisher}, and covered thereafter in the series of papers by Pitman \cite{Pitman1, Pitman2, Pitman3}, Scheffe \cite{Scheffe}, Hoeffding \cite{Hoeffding52} for instance, or bootstrap approaches derived from Efron's \cite{Efron} "naive" one. Two families of such permutation or bootstrap-based independence tests may be distinguished at least : the whole family of rank tests including the tests of Hotelling and Pabst \cite{Hot}, Kendall \cite{Kendall},  Wolfowitz \cite{Wolfowitz} or Hoeffding \cite{Hoeffding48} on the one hand, the family of Kolmogorov-Smirnov type tests, like Blum, Kiefer, and Rosenblatt's \cite{BKR61}, Romano's \cite{Romano89} or Van der Vaart and Wellner's \cite{vdvwellner96} ones on the other hand. These tests are purely non-parametric that is they are completely free of the underlying distributions of the observed random variables or vectors. They are proved to achieve asymptotically the right desired size: the probability, under independence, that the independence hypothesis is rejected tends to a prescribed $\alpha$ in $]0,1[$, as the size of the original samples of observations grows to infinity. Moreover, the tests based on permutation are also known to be exactly (non-asymptotically meaning) of the desired level, that is the probability, under independence, that the independence hypothesis is rejected is smaller than the prescribed $\alpha$, for any positive size of samples. Some of these tests are also proved to be consistent against many alternatives, such as Hoeffding's \cite{Hoeffding48} one and the family of Kolmogorov-Smirnov type tests.

Detecting dependence is also a fundamental old point in the neuroscientific literature (see \cite{GerPerkel} e.g.), as understanding how different areas of the brain interact with respect to a given stimulus helps to understand how the brain works. The neuroscience problem we were initially interested in consists of detecting interactions between occurrences of action potentials on two different neurons, observed on $n$ independent trials, as described in \cite{GrunB}. Indeed, it is nowadays possible to simultaneously record the time occurrences of action potentials, which are quick and brutal variations of the electric potential of the neuron membrane, for several neurons at once, via extracellular multielectrodes. Each recorded set of time occurrences of action potentials for each neuron is usually referred to as a spike train, the spikes being the time occurrences themselves. It is commonly accepted that these spikes are one of the main components of the brain activity (see \cite{Singer93}). Therefore, when observing two spike trains coming from two different neurons, one of the main elementary problem is to assess whether these two spike trains are independent or not. Unfortunately, even if the real recordings of spike trains are discretized in time, due to the record resolution, and thus belong to finite dimensional spaces, the dimension of these spaces is so huge (from ten thousand up to one million) that it is neither realistic nor reasonable to model them by finite dimensional variables, and to apply  usual independence tests. Several methods, such as the classical Unitary Events method introduced by Gr\"un \cite{Grunt}, consists of binning the spike trains at first in order to deal with vectorial data with reduced dimension. But it has been shown (see \cite{GDGRA} e.g.) that these dimension reduction methods involve an information loss of more than 60\% in some cases, making this kind of preprocessing quite proscribed despite its simplicity of use. 

Modelling the recordings of spike trains by point processes that are only almost surely finite without any a priori bound on the total number of points, and using, constructing if needed, independence tests specifically dedicated to such point processes then appear as realistic and reasonable solutions. 

Another field of applications, where detecting dependence between point processes is crucial, is, for instance, genomics, where point processes represent positions of motifs on the DNA strand as described in \cite{GS}, or positions of Transcription Regulatory Elements on the DNA strand as described in \cite{CSWH}.

In the present work, we mainly focus on the application in neuroscience detailed above. Since the existence of any precise underlying distribution for the point processes modelling spike trains data is subject to broad debate, to construct model free independence tests for point processes is of utmost importance in this context at least (see \cite{grunrev} for more details).  To this end, bootstrap methods were used in \cite{Pipa2003, Pipaet2003, venturaboot} for binned data with relatively small dimension, without any proper mathematical justification. Besides the loss of information the binning data pre-processing involves, it appears in fact that the test statistics chosen in these papers are not suitable for consistency of the bootstrap approach, due to an incorrect centring device leading to some considerable bias. Asymptotic independence tests were furthermore introduced in \cite{MTGAUE} for point processes, but limited to homogeneous Poisson processes. Such a Poisson assumption is necessarily restrictive and even possibly inappropriate, considering spike trains analyses (see \cite{Pouzat09}), as well as other fields of applications. Moreover, these tests, of an asymptotic nature, may suffer from a lack of performance when the number of trials is small or moderate.

We therefore propose to construct new non-parametric tests of independence between two point processes, from the observation of an i.i.d. sample with the same distribution as the considered pair of point processes, making as few assumptions as possible on this underlying distribution. Our test statistics, correctly re-centred and re-scaled, are based on general $U$-statistics. The corresponding critical values are obtained from bootstrap or permutation approaches, so that the final tests that are proved to be asymptotically (with respect to the sample size) of the desired size and consistent against many alternatives, also have good performance in practice when the sample size is moderate or even small, as is often the case in neuroscience or genomics for biological or economic reasons.

A huge number of papers deal with the bootstrap or permutation of $U$-statistics of i.i.d. real valued random variables or vectors. Among them, we can cite for instance \cite{Bickel-Freedman,Bretagnolle,agbst,Dehling94}, or  \cite{LeuchtNeumann09} devoted to the bootstrap in a general framework, \cite{HorvathHuskova} or  \cite{ChungRomano} devoted to the permutation in various testing frameworks, and of course, the papers cited above (see \cite{Romano89, vdvwellner96}), which are devoted to Kolmogorov-Smirnov type tests, and based on particular $U$-statistics in an independence testing framework. 

To our knowledge, there is no previous work on the bootstrap or permutation of general $U$-statistics of i.i.d. pairs of point processes, as considered in the present work. The main difficulties lie here in the nature of the mathematical objects we handle, that is point processes and their associated point measures which are random measures on the one hand, in the general nature of the results we aim at on the other hand. The proofs of our results, although inspired by Romano's \cite{RomanoTR, Romano89} work and Hoeffding's \cite{Hoeffding52} precursor results on the permutation, are therefore more technical and complex on many aspects, that we will point out.

\smallskip

This paper is organized as follows. 

We first present the problem of testing, and introduce the main notations. Starting from existing works in neuroscience, we introduce the test statistics we propose to use, based on general kernel-based $U$-statistics, before listing and discussing the main assumptions made on the kernels and the underlying point processes. These points constitute the content of Section \ref{ft}. 

Section \ref{bootsec} is devoted to our bootstrap approach, from its description to the theoretical asymptotic properties of the corresponding  independence tests, through general results about the consistency of the bootstrap for the considered $U$-statistics, expressed in terms of Wasserstein's metric as in \cite{Bickel-Freedman}. We thus states that our tests are asymptotically of the desired size, and that they are consistent against every reasonable alternative.

Section \ref{permsec} is devoted to the randomisation or permutation approach. We also begin with a description of the approach, then, we give general results about its consistency when the kernel of the $U$-statistic has a particular common form. These results are still expressed in terms of Wasserstein's metric, and thus induce usual weak convergence as well as convergence of second order moments. They also lead to prove that the corresponding permutation independence tests satisfy the same asymptotic properties as the bootstrap ones, with the further advantage of being exactly (that is non-asymptotically) of the desired level, even when a Monte Carlo method is used to approximate the randomised quantiles. This last point is easily deduced from a very useful lemma of Romano and Wolf (see \cite[Lemma 1]{RomanoWolf}).

Finally notice that the proofs of the results are postponed in Section \ref{proofsec} at the end of the paper.

% ---------------------------------------------------------------------------------------------------- %
\section{From the neuroscience interpretation to a general test statistic}
\label{ft}
% ---------------------------------------------------------------------------------------------------- %
 
% ------------------------------------------------------------ %
\subsection{The testing problem} 
% ------------------------------------------------------------ %

We consider in this paper point processes defined on a probability space $(\Omega,\mc{A},\PP)$, and observed on $[0,1]$. More precisely, we consider finite point processes on $[0,1]$, that is random point processes on $[0,1]$, for which the total number of points is almost surely finite (see \cite{DV} for instance for more details). Typically, in a neuroscience framework, such finite point processes may represent spike trains recorded on a given finite interval of time, and rescaled so that their values may be assumed to belong to $[0,1]$. The set $\calX$ of all their possible values is equipped with a metric $d_\calX$, that we introduce in \eqref{defmetric}. This metric, issued from the Skorohod topology, makes $\calX$ separable and allows to define accordingly borelian sets on $\calX$, and by extension through the product metric, on $\calX^2$.

Let us recall that the point measure $dN_x$ associated to an element $x$ of $\calX$ is defined for all measurable real-valued function $f$ by :
$$\int_{[0,1]} f(u) dN_x(u)=\sum_{u\in x} f(u),$$
so that, in particular, the total number of points of $x$, denoted by $\# x$, is equal to $\int_{[0,1]} dN_x(u)$. 

Moreover, for a point process $X$ defined on $(\Omega,\mc{A},\PP)$ and observed on $[0,1]$, 
$\int f(u) dN_X(u)$ becomes a real random variable, defined on the same probability space $(\Omega,\mc{A},\PP)$.


Now, a pair $X=(X^1,X^2)$ of finite point processes defined on $(\Omega,\mc{A},\PP)$, and observed on $[0,1]$, has joint distribution $P$ if $P(\mc{B})=\PP(X=(X^1,X^2)\in \mc{B})$, for any borelian set $\mc{B}$ of $\calX^2$, and marginal distributions $P^1$ and $P^2$ if $P^1(\mc{B}^1)=\PP(X^1\in \mc{B}^1)$, and $P^2(\mc{B}^2)=\PP(X^2\in \mc{B}^2)$ for any borelian sets $\mc{B}^1$ and $\mc{B}^2$ of $\calX$.

Given the observation of an i.i.d. sample  $\X_n=(X_1,\ldots,X_n)$ from the same distribution $P$ as $X$, with $X_i=(X^1_i,X^2_i)$ for every $i=1\ldots n$, we aim at testing 
$$(H_0)\ X^1 \textrm{ and } X^2 \textrm{ are independent}\quad \textrm{against}\quad (H_1)\ X^1 \textrm{ and } X^2 \textrm{ are not independent},$$
which can also be written as
$$(H_0)\ P=P^1\otimes P^2 \quad \textrm{against}\quad (H_1)\ P\neq P^1\otimes P^2.$$

% ------------------------------------------------------------ %
\subsection{Independence test based on coincidences in neuroscience \label{coincsec}}
% ------------------------------------------------------------ %

Considering that the i.i.d. sample $\X_n=(X_1,\ldots,X_n)$ models pairs of rescaled spike trains recorded for two neurons during $n$ trials, the main dependence feature that needs to be detected between both neurons corresponds to synchronizations in time, referred to as coincidences (see \cite{GrunB} for instance). 
More precisely, neuroscientists expect to detect if such coincidences occur significantly, that is more than what may be due to chance. They speak in this case of a detected synchrony, which is of the utmost importance to understand how a neural network behaves.  

Formalising the approach of \cite{GDGRA}, Tuleau-Malot et al. \cite{MTGAUE} first introduced the notion of coincidence count between two point processes $X^1$ and $X^2$ with delay $\delta$ ($\delta>0$), defined by :
\begin{equation}
\label{defnbc}
\Nbc_\delta(X^1,X^2)=\int_{[0,1]^2}  coinc_\delta(u,v)dN_{X^1}(u) dN_{X^2}(v),
\end{equation}
with $coinc_\delta(u,v)=\1{|u-v|\leq \delta}$. 

Thus, $\Nbc_\delta(X^1,X^2)=\sum_{u\in X^1,v\in X^2} \1{|u-v|\leq \delta}$ is equal to the number of pairs $(u,v)$ such that: $u$ is a point of $X^1$, $v$ is point of $X^2$, and $|u-v|\leq \delta$.
Then, under the assumption that both $X^1$ and $X^2$ are homogeneous Poisson processes, Tuleau-Malot  et al. proposed an independence test of $(H_0)$ against $(H_1)$, rejecting $(H_0)$ when a test statistic based on $\sum_{i=1}^{n} \Nbc_\delta\left(X^1_i,X^2_i\right)$ is larger than a given critical value.
This critical value is deduced from the asymptotic Gaussian distribution of the test statistic, under the null hypothesis of independence. 
The test is proved to be asymptotically of the desired level, but only under the homogeneous Poisson processes assumption.

However, it is now well-known that this assumption, as well as many other model assumptions, fails to be satisfied in practice for spike trains (see  \cite{Pouzat09}).
To construct a test detecting synchrony, with as few assumptions as possible on the underlying processes, is therefore an interesting, and challenging purpose.

In the present work, we propose a new non-parametric independence test for point processes, which allows, without any Poisson or other specific model assumption on the underlying processes, to detect particular time dependence features such as synchrony, but also more general ones.

% ------------------------------------------------------------ %
\subsection{General non-degenerate $U$-statistics as independence test statistics}

In the parametric homogeneous Poisson framework of \cite{MTGAUE}, the expectation of $\Nbc_\delta\left(X^1_i,X^2_i\right)$ has a simple expression as a function of $\delta$ and the intensities $\lambda_1$ and $\lambda_2$ of $X^1$ and $X^2$. Since $\lambda_1$ and $\lambda_2$ can be easily estimated, an estimator of this expectation can thus be obtained using the plug-in principle, and subtracted from $\Nbc_\delta\left(X^1_i,X^2_i\right)$ to lead to a test statistic with a centred asymptotic distribution under the null hypothesis of independence.

In the present non-parametric framework where we want to make as few assumptions as possible on the point processes $X^1$ and $X^2$, such a centring plug-in tool is not available, and we propose to use instead a self-centring trick, which amounts to consider 
$$\sum_{i\neq i'\in\{1,\ldots,n\}} \left(\Nbc_\delta\left(X^1_i,X^2_i\right)-\Nbc_\delta\left(X^1_i,X^2_{i'}\right)\right).$$

Furthermore, it is clear that the function $\Nbc_\delta$ used in \cite{MTGAUE} suits the dependence feature the neuroscientists expect to detect in a spike trains analysis. However, it is not necessarily the best choice for other kinds of dependence features to be detected in a general point processes analysis. So we introduce a more general  interaction function defined by 
\begin{equation}
\label{defw}
\varphi^{w}_\delta(X^1,X^2)=\int_{[0,1]^2} w_\delta(u,v) dN_{X^1}(u) dN_{X^2}(v),
\end{equation}
where $w_\delta$ is an integrable function possibly depending on a real-valued vector $\delta$ of parameters. Of course, the choice $w_\delta=coinc_\delta$ with $\delta>0$ allows to get back the coincidence count function defined in \eqref{defnbc}, while the choice $w_{(j,k)}(u,v)=\psi_{j,k}(v-u)$, where $\psi_{j,k}$ is a rescaled ($j$) and translated ($k$)  Haar mother wavelet, leads to the interaction function used in \cite{LaureCh} for an interaction test with strong theoretical and practical properties in a specific Poisson framework.

Even more generally, we could choose any integrable function $\varphi$, whose aim would be to detect particular dependence features on the pair $(X^1,X^2)$ for instance, and for which we introduce:
\begin{equation}
\label{essai1}
T_{n,\varphi}=\sum_{i\neq i'\in\{1,\ldots,n\}} \left(\varphi\left(X^1_i,X^2_i\right)-\varphi\left(X^1_i,X^2_{i'}\right)\right).
\end{equation}

Since $T_{n,\varphi}/(n(n-1))$ is an unbiased estimator of $\int\int \varphi\left(x^1,x^2\right)\left(dP(x^1,x^2)-dP^1(x^1)dP^2(x^2)\right)$, it may be a reasonable independence test statistic, with expectation equal to $0$ under $(H_0)$, and this without any assumption on the underlying point processes.

Notice that if $X^1$ and $X^2$ were finite dimensional variables with continuous distributions with respect to the Lebesgue measure, this test statistic would be closely related to generalized Kolmogorov-Smirnov type tests of independence. For instance, the test statistics of Blum, Kiefer, and Rosenblatt \cite{BKR61}, Romano \cite{Romano89}, Van der Vaart and Wellner in \cite{vdvwellner96}  are equal  to
$$n^{-3/2}\sup_{v^1\in \mc{V}^1,v^2\in \mc{V}^2}\left|T_{n,\varphi_{(v^1,v^2)}}\right|, $$
where, respectively:
\begin{itemize}
\item $\mc{V}^1=\mc{V}^2=\R$ and $\varphi_{(v^1,v^2)}(x^1,x^2)=\1{]-\infty,v^1]}(x^1)\1{]-\infty,v^2]}(x^2)$,
\item $\mc{V}^1$ and $\mc{V}^2$ are countable V.-C. classes of subsets of $\R^d$, and $\varphi_{(v^1,v^2)}(x^1,x^2)=\1{v^1}(x^1)\1{v^2}(x^2)$,
\item $\mc{V}^1$ and $\mc{V}^2$ are well-chosen classes of real-valued functions and $\varphi_{(v^1,v^2)}(x^1,x^2)=v^1(x^1)v^2(x^2)$.
\end{itemize}

Now, let us notice that setting
\begin{equation}
\label{hphi}
h_\varphi(x,y)=\frac{1}{2}\left(\varphi\left(x^1,x^2\right)+\varphi\left(y^1,y^2\right)-\varphi\left(x^1,y^2\right)-\varphi\left(y^1,x^2\right)\right),
\end{equation}
 for every $x=(x^1,x^2)$, $y=(y^1,y^2)$ in $\calX^2$, then $T_{n,\varphi}$ can also be written as
$$T_{n,\varphi}=\sum_{i\neq i'\in\{1,\ldots,n\}} h_\varphi\left(X_i,X_{i'}\right), \textrm{ with } X_i=(X_i^1,X_i^2),\ X_{i'}=(X_{i'}^1,X_{i'}^2),$$ 
so $T_{n,\varphi}/(n(n-1))$ is a classical $U$-statistic with a symmetric kernel $h_\varphi$, as introduced by Hoeffding \cite{Hoeffding48bis}.

Therefore, we finally consider in the present work independence test statistics which are based on $U$-statistics of the form:
\begin{equation}\label{defstat}
U_{n,h}(\X_n)=\frac{1}{n(n-1)}\sum_{i\neq i'\in\{1,\ldots,n\}} h\left(X_i,X_{i'}\right),
 \end{equation}
for some kernel $h$ of the form $h_\varphi$ given in \eqref{hphi}, or even for some general symmetric kernel $h: (\calX^2)^2 \to \R$ such that $\esp{h(X_i,X_{i'})}=0$ for every $X_i,X_{i'}$ i.i.d. with distribution $P$ on $\calX^2$ satisfying $(H_0)$ $ P=P^1\otimes P^2$.
\medskip


Notice that assuming that $h$ is symmetric is not restrictive since for any kernel $h$, $2U_{n,h}=U_{n,h_{sym}}$ where $h_{sym}(x,y)=h(x,y)+h(y,x)$ always defines a symmetric kernel. 

\medskip

Following the works of Romano \cite{Romano89} or Van der Vaart and Wellner \cite{vdvwellner96}, the tests we propose here are based on bootstrap and permutation approaches for the above general $U$-statistics. 

\medskip

Most of our theoretical results are true, whether the considered $U$-statistics $U_{n,h}(\X_n)$ are degenerate or not. However, for degenerate $U$-statistics, the interpretation of these results is very poor. So, we will focus on non-degenerate $U$-statistics, and therefore discuss below what degeneracy means about the underlying processes.

\medskip

Since under $(H_0)$ $U_{n,h}(\X_n)$ is assumed to have zero mean, it will be degenerate if and only if for $X$ with distribution $P^1\otimes P^2$ and for $P^1\otimes P^2$-almost every $x$ in $\calX^2$,
$$\esp{h(x,X)}=0.$$

In the particular case where $h=h_\varphi$, with $h_\varphi$ defined by \eqref{hphi} and $\varphi$ some integrable function, this amounts to state that for $X=(X^1,X^2)$ with distribution $P^1\otimes P^2$ and for $P^1\otimes P^2$-almost every $x=(x^1,x^2)$,
$$\varphi(x^1,x^2)+\esp{\varphi(X^1,X^2)}-\esp{\varphi(x^1,X^2)}-\esp{\varphi(X^2,x^1)}=0.$$

This condition's fulfillment would however imply a very particular link between $\varphi$ and the distribution of the bivariate point process $X$, which is unknown. Let us look more deeply at the case where $\varphi=\varphi^{w}_\delta$ given by \eqref{defw}. In this case, the above condition can be rewritten as follows: for $P^1\otimes P^2$-almost every $x=(x^1,x^2)$, 
\begin{multline}\label{espzerogen}
\int w_\delta(u,v) dN_{x^1}(u)dN_{x^2}(v)+ \esp{\int w_\delta(u,v) dN_{X^1}(u)dN_{X^2}(v)}\\
-\esp{\int w_\delta(u,v) dN_{x^1}(u)dN_{X^2}(v)}-\esp{\int w_\delta(u,v) dN_{X^1}(u)dN_{x^2}(v)}=0.
\end{multline}
Therefore, if for instance $P^1(\{\emptyset\})>0$ and $P^2(\{\emptyset\})>0$, which is the case for Bernoulli processes (discretized processes), Poisson processes, Hawkes processes or more generally any processes with conditional intensities, this implies, by taking $x^1=x^2=\emptyset$, that 
\begin{equation}
\label{espzerow}
\esp{\int w_\delta (u,v) dN_{X^1}(u)dN_{X^2}(v)}=0.
\end{equation}
If the function $w_\delta$ is nonnegative, then this finally gives that $\int w_\delta (u,v) dN_{X^1}(u)dN_{X^2}(v)$ is almost surely equal to $0$. 
However, in practice, it is always possible to choose a function $w_\delta$ so that the observation of at least one of the $\int w_\delta (u,v) dN_{X_i^1}(u)dN_{X_i^2}(v)$'s is not equal to zero, and the $U$-statistic is not degenerate.

\medskip

When considering spike trains analysis in neuroscience, notice first that assuming that the processes may be empty (but not empty almost surely) is an obvious  assumption in practice. Indeed, there often exist trials (usually short) where, just by chance, no spikes have been detected for some trials while some spikes have been detected for other trials. Moreover, if $w_\delta=coinc_\delta$ as in \eqref{defnbc}, $\delta$ is chosen large enough so that coincidences are observed in practice, so  \eqref{espzerow} is not satisfied,   and the $U$-statistic is non-degenerate. 

However notice that when $w_\delta(u,v)=\psi_{j,k}(v-u)$, where $\delta=(j,k)$ and $\psi_{j,k} $ is a rescaled and translated Haar mother wavelet as in \cite{LaureCh}, when considering specific Poisson processes, the condition \eqref{espzerow} is fulfilled, since $\int \psi_{j,k}(u) du = 0$ is required to make the test statistic in \cite{LaureCh}  a reasonable test statistics. But in this case,  the $U$-statistic is still non-degenerate since the left-hand side of \eqref{espzerogen} has zero mean and positive variance, as soon as both Poisson processes have non zero intensities (see Section \ref{asymp} for more details).


% ---------------------------------------------------------------------------------------------------- %
\section{Bootstrap tests of independence}
\label{bootsec}
% ---------------------------------------------------------------------------------------------------- %

% ------------------------------------------------------------ %
\subsection{Description of the bootstrap approach and informal view of  the results}
% ------------------------------------------------------------ %

Recall that we observe an i.i.d. sample $\X_n=(X_1,\ldots,X_n)$ with $X_i=(X^1_i,X^2_i)$ of distribution $P$ on $\calX^2$, whose marginals  are respectively denoted $P^1$ and $P^2$, and that the test statistics we consider are based on the $U$-statistics $U_{n,h}(\X_n)$ defined by \eqref{defstat}. Since the distribution of these test statistics is not free from the unknown underlying marginal distributions $P^1$ and $P^2$ under the null hypothesis, we turn to a classical bootstrap approach, whose aim is to mimic the distribution of the test statistics under $(H_0)$, for large, but also moderate or small sample sizes. Since each $X_i=(X^1_i,X^2_i)$ is $P^1\otimes P^2$-distributed under $(H_0)$, the first and second coordinates of the elements of $\X_n$ are resampled according to the corresponding marginal empirical distributions. More precisely,  we consider, for $j=1,2$, the marginal sample distribution $P_n^j$  of $\left(X^j_1,\ldots,X^j_n\right)$ given by 
\begin{equation}
\label{pnj} 
 P_n^j=\frac{1}{n}\sum_{i=1}^n \delta_{X_i^j}.
 \end{equation}
Then the bootstrap sample $\X_n^*=\left(X_{n,1}^*,\ldots,X_{n,n}^*\right)$, with $X_{n,i}^*=\left(X_{n,i}^{1*},X_{n,i}^{2*}\right),$ is defined as an $n$ i.i.d. sample from the distribution $P_n^1\otimes P_n^2$.

In particular, we prove (see Theorem \ref{Convergence}) that, under appropriate assumptions,
 the conditional distribution of $\sqrt{n}U_{n,h}(\X_n^*)$  given $\X_n$ is asymptotically close to the one of $\sqrt{n}U_{n,h}(\X_n^{\independent})$, 
where $\X_n^{\independent}$ is an i.i.d sample from the distribution $P^1\otimes P^2$. Therefore, using the quantiles of the conditional distribution of 
$\sqrt{n}U_{n,h}(\X_n^*)$ given $\X_n$ as critical values, we can build unilateral or bilateral tests of $(H_0)$ against $(H_1)$ with asymptotic size $\alpha$. These tests also satisfy consistence properties against alternatives such that $\int h(x,x') dP(x) dP(x')\not=0$ (see Theorem~\ref{thniveauconsistance}).

\medskip

Before stating these results and to avoid any confusion, we introduce a few notations. 
\begin{itemize}
\item For any functional $Z: (\calX^2)^n \to \R$, $\calL\left(Z,Q\right)$ denotes the distribution of the variable $Z(\X_n)$, where $\X_n$ is an i.i.d.  sample from the distribution $Q$ on $\calX^2$.
\item If the distribution $Q=Q(W)$ is random and depends on a random variable $W$, then $\calL\left(\left.Z,Q\right|W\right)$ is the conditional distribution of the variable $Z(\X_n)$, where $\X_n$ is an i.i.d. sample from the distribution $Q=Q(W)$ on $\calX^2$, given $W$.
\item " $Q$-a.s. in $(X_n)_n$" at the end of a statement means that the statement only depends on the sequence $(X_n)_{n}$, where the $X_n$'s are i.i.d with distribution $Q$, and that 
there exists an event $\mathcal{C}$ only depending on $(X_n)_n$ such that $\proba{\mathcal{C}}=1$, on which the statement is true. 
\item "$Q_n \Rightarrow Q$" means that the sequence of distributions $(Q_n)_n$ converges towards $Q$ in the weak sense, that is for any real valued, continuous and bounded function $g$,$\int g(z) dQ_n(z) \cv{n\to+\infty} \int g(z) dQ(z).$
\item Finally, as we often work conditionally on $\X_n$, we will denote by $\Estar{\cdot}$ the conditional expectation given the sample $\X_n$. 
\end{itemize}

In particular, the conditional distribution of  $\sqrt{n}U_{n,h}(\X_n^*)$ given $\X_n$ is denoted by $\calL\left(\left.\sqrt{n}U_{n,h},P^1_n\otimes P^2_n\right|\X_n\right)$.



% ------------------------------------------------------------ %
\subsection{Main assumptions \label{ma}}
% ------------------------------------------------------------ %

Since the random variables we deal with are not real-valued variables but point processes, the proofs of our results will need many assumptions, which may seem to be somewhat abstract. We therefore describe and discuss them in the present section.

% ------------------------------ %
\paragraph{Centring assumptions on the kernel $h$}~\\
% ------------------------------ %

First notice that, under $(H_0)$, the $U$-statistic introduced in \eqref{defstat} has zero mean if the kernel $h$ satisfies the following assumption:

\smallskip

$\pa{\mc{A}_{Cent}}\quad \textrm{\begin{tabular}{|c}  For $X_i$ and $X_{i'}$, i.i.d. with distribution $P^1\otimes P^2$ on $\calX^2$,\\ $\esp{h\left(X_i,X_{i'}\right)}=0$.
\end{tabular}}$

\smallskip

An empirical and resampled version of this assumption is furthermore stated as:

\smallskip

$\pa{\mc{A}_{Cent}^*}\quad \textrm{\begin{tabular}{|c} For $x_1=(x_1^1,x_1^2),\dots, x_n=(x_n^1,x_n^2)$ in $\calX^2$,\\
$\sum_{i_1,i_2,i'_1,i_2'=1}^n h\pa{\pa{x_{i_1}^1,x_{i_2}^2},\pa{x_{i'_1}^1,x_{i'_2}^2}}=0.$\end{tabular}}$

\smallskip

Notice that both assumptions are fulfilled as soon as $h$ is of the form $h_\varphi$ given by \eqref{hphi}, though it is not necessary. The kernel $h:\pa{\pa{x^1,x^2},\pa{y^1,y^2}}\mapsto\# x^1\cdot \# x^2\cdot \# y^1\cdot \# y^2\pa{\# x^1-\# y^1}\pa{\# x^2-\# y^1}$ indeed satisfies both assumptions though it can not be written as an $h_\varphi$.

% ------------------------------ %
\paragraph{Moment assumptions on the kernel $h$}~\\
% ------------------------------ %

Since the metric we use in the following to prove the asymptotic validity of our bootstrap procedure is the $\L^2$-Wasserstein distance, we will need moment assumptions on the kernel $h$. In particular, we will assume that the variance of $U_{n,h}(\X_n)$ exists, i.e.

\smallskip

$\pa{\mc{A}_{Mmt}}\quad  \textrm{\begin{tabular}{|c}  For $X_i$ and $X_{i'}$, i.i.d. with distribution $P$ on $\calX^2$,\\
 $\esp{h^2\pa{X_i,X_{i'}}}<\infty.$\end{tabular}}$

\smallskip

We will also need a resampled version of this assumption as:

\smallskip

$\pa{\mc{A}_{Mmt}^{*}}\quad\textrm{\begin{tabular}{|c} 
For $X_1,X_2,X_3,X_4$ i.i.d. with distribution $P$ on $\calX^2$,
 $ i_1,i_2,i_1',i_2'$ in $\ac{1,2,3,4}$,\\
$ \esp{h^2\pa{\big(X_{i_1}^1,X_{i_2}^2\big),\big(X_{i_1'}^1,X_{i_2'}^2\big)}}<+\infty.$ 
\end{tabular}}$

\smallskip

Notice that when $\pa{\mc{A}_{Mmt}^{*}}$ is satisfied, this implies that:
\begin{itemize}
\item $\pa{\mc{A}_{Mmt}}$ is satisfied (taking $i_1=i_2$, $i_1'=i_2'$, and $i_1'\neq i_1$),
\item for  $X_i$ with distribution $P$, $\esp{h^2\pa{X_i,X_i}}<+\infty$ (taking  $i_1=i_2=i_1'=i_2'$),
\item for $X_i$, $X_{i'}$ i.i.d with distribution $P^1\otimes P^2$, $\esp{h^2\pa{X_i,X_{i'}}}<+\infty$ (taking $i_1,i_2,i_1',i_2'$ all different).
\end{itemize}

A sufficient condition for $\pa{\mc{A}_{Mmt}^{*}}$  (and consequently $\pa{\mc{A}_{Mmt}}$)  to be satisfied is that there exist some positive constants $\alpha_1, \alpha_2,C$ such that for every $x=(x^1,x^2),y=(y^1,y^2)$ in $\calX^2$,
$$ |h(x,y)|\leq C\left((\# x^1)^{\alpha_1} + (\#y^1)^{\alpha_1}\right)\left((\# x^2)^{\alpha_2} + (\#y^2)^{\alpha_2}\right),$$
with  $\esp{ (\#X^1)^{4\alpha_1}}<+\infty$ and $\esp{ (\#X^2)^{4\alpha_2}}<+\infty$.\\ 

In the particular case where $h$ is of the form $h_\varphi$ given by \eqref{hphi}, a possible sufficient condition is 
that there exist some positive constants $\alpha_1, \alpha_2,C$ such that for every $x^1,x^2$ in $\calX$, 
$|\varphi(x^1,x^2)|\leq C (\# x^1)^{\alpha_1}(\# x^2)^{\alpha_2},$
with $\esp{\pa{\#X^1}^{4\alpha_1}}<+\infty$ and $\esp{\pa{\#X^2}^{4\alpha_2}}<+\infty$.

\smallskip

Notice that the coincidence count function $\Nbc_\delta$ defined by \eqref{defnbc}, satisfies, for every $x^1,x^2$ in $ \calX$,
$$|\Nbc_\delta(x^1,x^2)|\leq (\# x^1)(\# x^2).$$ Therefore, $\pa{\mc{A}_{Mmt}^{*}}$ and  $\pa{\mc{A}_{Mmt}}$ are satisfied as soon as $\esp{\pa{\#X^1}^{4}}<+\infty$ and $\esp{\pa{\#X^2}^{4}}<+\infty$.

\medskip

Moment bounds for the total number of points of the processes, such as the ones stated above, are in fact satisfied in many situations.
Since any discretized point process at resolution $0<r<1$ (see \cite{MTGAUE} for a definition) has at most $1/r$ points, such moment bounds are obviously satisfied in the case where the considered point processes are discretized ones.
Moreover, since any Poisson process has a total number of points obeying a Poisson distribution, which has exponential moment of any order, such moment bounds are also obviously satisfied in the case of Poisson processes, as well as in the case of point processes with bounded conditional intensities, which can be constructed by thinning of homogeneous Poisson processes on $\R$ \cite{ogatathin}.
Similar moment bounds can also be obtained (see \cite{HRBR}) for linear stationary Hawkes processes with positive interaction functions, that are quite classical models in spike trains analysis (see for instance \cite{PSCR,MTGAUE}). This finally may be extended to point processes whose conditional intensities is upper bounded by intensities of linear stationary Hawkes processes with positive interaction functions, by thinning arguments again, which includes more general Hawkes processes (see \cite{BM}) and in particular Hawkes processes used to model inhibition in spike trains analysis (see \cite{HRBR, MTGAUE, ieee} or \cite{RRGT}).



% ------------------------------ %
\paragraph{Continuity of the kernel $h$}~\\
% ------------------------------ %

Let us recall that $\calX$ is the set of the possible values of finite point processes defined on a probability space $(\Omega,\mc{A},\PP)$ and observed on $[0,1]$, and notice that it can be identified with the space $\calD$ of c\`adl\`ag functions on $[0,1]$ through the identification $I:x\in\calX\mapsto \pa{t\mapsto \int_0^1\1{u\leq t} dx_u}\in\calD$. 
Now, considering the uniform Skorohod topology on $\calD$, associated with the metric $d_\calD$ defined by (see  \cite{Billingsley2} e.g.):
$$d_\calD(f,f')=\inf \ac{\varepsilon>0\ \Big/\quad \exists\lambda\in\Lambda, \ \sup_{t\in[0,1]} |\lambda(t)-t|\leq \varepsilon,\ \sup_{t\in [0,1]}|f(\lambda(t))-f'(t)|\leq \varepsilon},$$
where $\Lambda$ denotes the class of strictly increasing, continuous mappings of $[0,1]$ onto itself, $\calX$ can be endowed with the topology induced by the metric $d_{\calX}$ defined on $\calX$ by:
\begin{equation}\label{defmetric}
d_{\calX}(x,x')=d_{\calD}(I(x),I(x'))\quad \textrm{for every $x$, $x'$ in $\calX$}.\end{equation}
Since $(\calD,d_\calD)$ is a separable metric space, so are the spaces $(\calX,d_{\calX})$, $\left(\calX^2,d_{\calX^2}\right),$ where $d_{\calX^2}$ is the usual product metric defined from $d_{\calX}$ (see \cite[p32]{Dudley} e.g.),  and $\left(\calX^2\times\calX^2,d\right),$ where $d$ is the product metric defined from $d_{\calX}$,  finally given by:
\begin{equation}\label{distance}
d\big((x,y),(x',y')\big)=\sup \ac{\sup_{j=1,2}\ac{d_{\calX}(x^j,x^{'j})},\sup_{j=1,2}\ac{d_{\calX}(y^j,y^{'j}) }},
\end{equation}
 for every $x=(x^1,x^2)$, $y=(y^1,y^2)$, $x'=(x^{'1},x^{'2})$, $y'= (y^{'1},y^{'2})$ in $\calX^2$.\\


We assume that the kernel $h$ defining the $U$-statistic $U_{n,h}(\X_n)$ in \eqref{defstat} satisfies:

\smallskip

$\pa{\mc{A}_{Cont}}\quad\textrm{\begin{tabular}{|l}
There exists a subset $\mathcal{C}$ of $\calX^2\times \calX^2$, such that \\
 (i) $h$ is continuous in every $(x,y)$ in $\mathcal{C}$ for the topology 
induced by  the metric $d$ defined by \eqref{distance},  \\
(ii) $(P^1\otimes P^2)^{\otimes 2}(\mathcal{C})=1$.
\end{tabular}}$


\smallskip

Notice that in the particular case where $h$ is of the form $h_{\varphi^w_\delta}$ defined by \eqref{defw} and  \eqref{hphi}, with a continuous function $w_\delta: [0,1]^2\to \R$, then $\pa{\mc{A}_{Cont}}$  is necessarily fulfilled. 

Considering the coincidence count kernel $h_{\Nbc_\delta}$, defined by \eqref{defnbc} and \eqref{hphi}, since the function $(u,v)\in [0,1]^2\mapsto \1{|u-v|\leq \delta}$ is not continuous, it is not so clear. However, one has the following result whose proof is given in Section \ref{contcoinc}. 

\begin{prop}\label{hinH}
The coincidence count kernel $h_{\Nbc_\delta}$ defined on $\calX^2\times \calX^2$ by \eqref{defnbc} and  \eqref{hphi} is continuous w.r.t. the topology induced by $d$, defined by \eqref{distance},  on the set
\begin{equation}\label{1indep2}
\mc{C}_\delta=\ac{\pa{(x^1,x^2),(y^1,y^2)}\in \calX^2\times \calX^2\ \Big/\quad  \pa{\ac{x^1}\cup\ac{y^1}} \cap \pa{\ac{x^2 \pm \delta}\cup\ac{y^2 \pm\delta}} =\emptyset}.
\end{equation}
\end{prop}


As suggested in \cite{MTGAUE}, when dealing with discretized point processes at resolution $r$ in spike trains analysis, the right choice for $\delta$ is $dr+r/2$ for some integer $d$. Thus, in this case, the condition (ii) in  $\pa{\mc{A}_{Cont}}$ is satisfied by the set $\mc{C}_\delta$ of \eqref{1indep2}, hence $\pa{\mc{A}_{Cont}}$  is satisfied by $h_{\Nbc_\delta}$. Furthermore, when dealing with point processes that have conditional intensities, so that they may be constructed by thinning  Poisson processes (see \cite{ogatathin} e.g.), the probability $(P^1\otimes P^2)^{\otimes 2}$ of $\mc{C}_\delta$ in \eqref{1indep2} is larger than the probability that $X\cap(X'\pm\delta)=\emptyset$, when $X$ and $X'$ are two independent Poisson processes. Thus, $\pa{\mc{A}_{Cont}}$  is also satisfied by $h_{\Nbc_\delta}$ in this case.




% ------------------------------------------------------------ %
\subsection{Consistency of the bootstrap approach}
% ------------------------------------------------------------ %

As in the historical paper by Bickel et Freedman \cite{Bickel-Freedman}, the closeness between $\calL\left(\left.\sqrt{n}U_{n,h},P^1_n\otimes P^2_n\right|\X_n\right)$ and $\calL\left(\sqrt{n}U_{n,h},P^1\otimes P^2\right)$, that are both distributions on $\R$, is here measured via  the classical $\L^2$-Wasserstein's metric $d_2$ (also called Mallows' metric) on the space  $\Gamma_2$ of distributions $Q$ on $\R$ such that $\int z^2 dQ(z)<\infty$. Recall that the metric $d_2$  is defined by:
\begin{equation}\label{Wasserstein}
d_2^2(Q,Q')=\inf{} \ac{\esp{(Z-Z')^2}, \ (Z,Z') \textrm{ has marginal distributions }Q\textrm{ and }Q'},
\end{equation}
for every $Q$, $Q'$ in $\Gamma_2$,   
and that convergence w.r.t. $d_2$  is equivalent to both weak convergence and convergence of second order moments. 

\medskip

The validity of the bootstrap approach described above for our independence tests is mainly due to the following consistency result.

\begin{thm}\label{Convergence}
Let $(X_n)_n$ be a sequence of i.i.d. pairs of point processes  with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$. For every $n\geq 2$, let $\X_n=(X_1,\ldots,X_n)$,
let $P_n^j$ for $j=1,2$ be the empirical marginal distributions defined by \eqref{pnj} and let $U_{n,h}$ be defined by \eqref{defstat}, with a measurable symmetric kernel $h$ on $\mc{X}^2\times\mc{X}^2$. Assume that $\pa{\mc{A}_{Cent}}$, $\pa{\mc{A}_{Cent}^*}$,  $\pa{\mc{A}_{Mmt}^{*}}$ and $\pa{\mc{A}_{Cont}}$ are all satisfied. Then,  
$$d_2\left(\calL\left( \left. \sqrt{n} U_{n,h}, P_n^1\otimes P_n^2 \right|\X_n\right) , \calL\left( \sqrt{n}U_{n,h},P^1\otimes P^2\right)  \right) \cv{n\to+\infty} 0 \quad \textrm{$P$-a.s. in $(X_n)_n$}.$$
\end{thm}

The proof follows similar arguments to the ones of  \cite{Bickel-Freedman} for the bootstrap of the mean, or to \cite{Dehling94} and \cite{LeuchtNeumann09} for the bootstrap of $U$-statistics. The difficulty of the present proof lies in the original data being here finite point processes instead of real-valued random variables. The main point is therefore to transpose finite point processes in the separable Skorohod metric space $(\calD,d_\calD)$, where weak convergence of sample probability distributions is available (see \cite{Varadarajan58}). 

This theorem derives from the following two propositions which may be useful in various frameworks. The first one states a non-asymptotic result, while the second one gives rather natural results of convergence.


\begin{prop}\label{inegalite_Wasserstein}
Under assumptions  $\pa{\mc{A}_{Cent}}$, $(\mc{A}_{Cent}^*)$,  $\pa{\mc{A}_{Mmt}}$, and in the notation of Theorem \ref{Convergence}, there exists some constant $C>0$ such that for every  integer $n\geq2$,
\begin{multline*}
d_2^2\left(\calL\left( \left. \sqrt{n} U_{n,h}, P_n^1\otimes P_n^2 \right|\X_n\right) , \calL\left( \sqrt{n}U_{n,h},P^1\otimes P^2\right)  \right)
\leq C \inf \Big\{\Estar{\pa{h\pa{ Y_{n,1}^*, Y_{n,2}^*}-h\pa{Y_1, Y_2}}^2},\\
Y_{n,1}^*\sim P_n^1\otimes P_n^2,\  Y_1 \sim P^1\otimes P^2,\ \textrm{and } (Y_{n,2}^*,Y_2) \textrm{ is an independent copy of }(Y_{n,1}^*,Y_1)\Big\}.
\end{multline*}
\end{prop}


 \begin{prop}
\label{LGNhcarre}
Let $(X_n)_n$ be a sequence of i.i.d. pairs of point processes  with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$.  Let $\X_n=(X_1,\ldots,X_n)$ for every $n\geq 2$, let $h$ be a measurable symmetric kernel on $\mc{X}^2\times\mc{X}^2$, and let $U_{n,h}(\X_n)$ be the $U$-statistic defined by \eqref{defstat}.
\begin{itemize}
\item If $\esp{|h(X,X')|}<+\infty$, for $X$,$X'$ i.i.d with distribution $P$, then one has
\begin{equation}
\label{cvh}
U_{n,h}(\X_n) \cv{n\to+\infty} \esp{h(X,X')}=\int h(x,x')dP(x) dP(x') \quad \textrm{$P$-a.s. in $(X_n)_n$}.
\end{equation}
\item Under $\pa{\mc{A}_{Mmt}^{*}}$, one  moreover obtains
\begin{equation}
\label{cvh2}
\frac{1}{n^4} \sum_{i,j,k,l=1}^n h^2\pa{\pa{X_i^1,X_j^2},\pa{X_k^1,X_l^2}} \cv{n\to+\infty} \esp{h^2\pa{\pa{X_1^1,X_2^2},\pa{X_3^1,X_4^2}}} \quad \textrm{$P$-a.s. in $(X_n)_n$}.
\end{equation}
\end{itemize}
\end{prop}


% ------------------------------------------------------------ %
\subsection{Convergence of cumulative distribution functions and quantiles}
% ------------------------------------------------------------ %

As usual, $\mathcal{N}(m,v)$ stands for the Gaussian distribution with mean $m$ and variance $v$, $\Phi_{m,v}$ for its cumulative distribution function (c.d.f.) and $\Phi_{m,v}^{-1}$ for the corresponding quantile function.
From the results of Rubin and Vitale \cite{RubinVitale80} generalising Hoeffding's \cite{Hoeffding48bis}  Central Limit Theorem for nondegenerate $U$-statistics when the $X_i$'s are random vectors, we can also easily deduce the following Central Limit Theorem for $U_{n,h}(\X_n)$.  

\begin{prop}\label{TCLUstat}
Let $(X_n)_n$ be a sequence of i.i.d. pairs of point processes  with distribution $P^1\otimes P^2$ on $\calX^2$. Let $h$ be a measurable symmetric kernel satisfying $\pa{\mc{A}_{Cent}}$ and $\pa{\mc{A}_{Mmt}}$
and for every $n\geq 2$, let $U_{n,h}$ be the $U$-statistic defined by $\eqref{defstat}$.  
Then, if $U_{n,h}$  is nondegenerate, 
$$\calL\left(\sqrt{n}U_{n,h},P^1\otimes P^2 \right)\Rightarrow \mathcal{N}(0, \sigma^2_{h,P^1 \otimes P^2}),$$  
with  
\begin{equation}\label{sigma}
 \sigma^2_{h,P^1 \otimes P^2}= 4Var\pa{\esp{h\left(X,X'\right)|X}}=4Cov\left(h\left(X,X'\right),h\left(X,X''\right)\right) >0,
\end{equation} 
 $X,X',X''$ being i.i.d. pairs of point processes with distribution $P^1 \otimes P^2$ on $\calX^2$.
 
Moreover, since $\esp{\pa{\sqrt{n}U_{n,h}(\X_n)}^2} \to \sigma^2_{h,P^1 \otimes P^2}$, the following also holds
$$d_2\pa{\calL\left(\sqrt{n}U_{n,h},P^1\otimes P^2 \right), \mathcal{N}(0, \sigma^2_{h,P^1 \otimes P^2})} \cv{n\to\infty} 0.$$
 
\end{prop}

\emph{Comment.}  The above asymptotic normality result may lead to a rather simple test, of the desired asymptotic size. Indeed, by Slutsky's lemma and the law of large numbers for $U$-statistics of order 3, 
if 
$$S_{n,h}(\X_n)=\sqrt{n}U_{n,h}(\X_n)/\hat\sigma_{h},$$ with 
$$\hat{\sigma}^2_{h}=\frac{4}{n(n-1)(n-2)} \sum_{i,j,k\in \{1,\ldots,n\}, \#\{i,j,k\}=3} h(X_i,X_j)h(X_i,X_k),$$
then under $(H_0)$, $S_{n,h}(\X_n)$ converges in distribution towards $\mc{N}(0,1)$.

Therefore, the test that rejects $(H_0)$ when  $|S_{n,h}(\X_n)|\geq \Phi_{0,1}^{-1}(1-\alpha/2)$  is of asymptotic size $\alpha$. One can also easily prove that it is consistent (i.e. of asymptotic power $1$) against any alternative $P$ such that $\esp{h(X,X')}\neq 0$, for $X$, $X'$ i.i.d. with distribution $P$ and satisfying $\pa{\mc{A}_{Mmt}}$. 
 
\noindent Such tests, which are purely asymptotic, may of course suffer from a lack of power when the sample size $n$ is small or even moderate, which is typically the case for the application in neuroscience described in Section \ref{ft}, since the number of trials can never be very large for biological reasons (from few tens up to few hundreds at best).

This is the reason why we turn here to bootstrap and permutation approaches developed by Romano \cite{Romano89} in another framework, which are known to better fit for small or moderate sample sizes. 

\medskip 

Notice that the asymptotic normality of $\sqrt{n}U_{n,h}(\X_n)$, stated in Proposition \ref{TCLUstat}, has not been used in the proof of Theorem \ref{Convergence}.
Since it notably means that $\sqrt{n}U_{n,h}(\X_n)$ weakly converges to a distribution with continuous c.d.f., we can however use it to prove the convergence of the conditional c.d.f. or quantiles of the considered bootstrap distributions in  the following Corollary.

\begin{coro}\label{coroBoot}
Let $(X_n)_n$ be a sequence of i.i.d. pairs of point processes  with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$. For every $n\geq 2$, let $\X_n=(X_1,\ldots,X_n)$, and in the notation of Theorem \ref{Convergence}, let $\X_n^*$ be a bootstrap sample  defined as an i.i.d sample from the distribution $P^1_n\otimes P^2_n$. 
Let $\X_n^{\independent}$ be another i.i.d. sample with distribution $P^1\otimes P^2$ on $\calX^2$, with size $n$. Then under the same assumptions as in Theorem \ref{Convergence}, 
 when $U_{n,h}(\X_n)$ is non-degenerate under $(H_0)$,  then 
\begin{equation}
\label{cvcdf}\sup_{z\in\R}\left| \PP\pa{ \left.\sqrt{n} U_{n,h}\left(\X_n^*\right)\leq z\right|\X_n}-\PP\pa{\sqrt{n}U_{n,h}(\X_n^{\independent})\leq z}\right|\cv{n\to+\infty} 0 \quad \textrm{$P$-a.s. in $(X_n)_n$}.
\end{equation}
If moreover, for every $\eta\in(0,1)$, $q_{n,h,\eta}^*(\X_n)$ and $q_{n,h,\eta}^{\independent}$ respectively denote the conditional $\eta$-quantile of $\sqrt{n}U_{n,h}(\X_n^*)$ given $\X_n$ and the $\eta$-quantile of $\sqrt{n}U_{n,h}(\X_n^{\independent})$,  then
\begin{equation}
\label{cvquantBoot}
q_{n,h,\eta}^*(\X_n) \cv{n\to+\infty} \Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta) \quad \mbox{and} \quad |q_{n,h,\eta}^*(\X_n)-q_{n,h,\eta}^{\independent}| \cv{n\to+\infty} 0\quad \textrm{$P$-a.s. in $(X_n)_n$},
\end{equation}
 where $\sigma^2_{h,P^1 \otimes P^2}$ is defined as in \eqref{sigma}.
\end{coro}


% ------------------------------------------------------------ %
\subsection{\label{asymp}Asymptotic properties of the bootstrap tests}
% ------------------------------------------------------------ %

Starting from Corollary \ref{coroBoot} and with the same notation, for any fixed $\alpha$ in $(0,1)$, we introduce the following tests:
\begin{equation}\label{deftest}
\left\{
\begin{array}{c c l}
\Delta_{h,\alpha}^{*+}(\X_n)&=&\1{\sqrt{n}U_{n,h}(\X_n)>q_{n,h,1-\alpha}^*(\X_n)},\\
\Delta_{h,\alpha}^{*-}(\X_n)&=&\1{\sqrt{n}U_ {n,h}(\X_n) < q_{n,h,\alpha}^*(\X_n)},\\
\Delta_{h,\alpha}^{*}(\X_n)&=&\Delta_{h,\alpha/2}^{*+}(\X_n) + \Delta_{h,\alpha/2}^{*-}(\X_n).
\end{array}\right.
\end{equation}
Note that $q_{n,h,\eta}^*(\X_n)$ is random, depending on $\X_n$ and that it may be exactly computed by considering all possible $n^{2n}$ bootstrap samples. The algorithmic complexity of such an exact computation is usually so large, that a Monte Carlo approximation of the bootstrap quantiles, based on resampling from the original data $\X_n$, is preferred  in practice. This Monte Carlo step is considered in the next section. \\

From Corollary \ref{coroBoot}, we deduce that, under non very restrictive assumptions, $\Delta_{h,\alpha}^{*+}$, $\Delta_{h,\alpha}^{*-}$  and $\Delta_{h,\alpha}^{*}$ are asymptotically of size $\alpha$, and also consistent against particular alternatives.

\begin{thm}\label{thniveauconsistance}
Let $\alpha\in(0,1)$. Let $(X_n)_n$ be a sequence of i.i.d. pairs of point processes  with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$. For every $n\geq 2$, let $\X_n=(X_1,\ldots,X_n)$ and 
 let $\Delta^*$ be one of the three tests defined in \eqref{deftest}.  Assume that $\pa{\mc{A}_{Cent}}$, $(\mc{A}_{Cent}^*)$,  $\pa{\mc{A}_{Mmt}^{*}}$ and $\pa{\mc{A}_{Cont}}$ are satisfied and that $U_{n,h}(\X_n)$ is non-degenerate under $(H_0)$. Then, 
\begin{itemize}
\item $\Delta^*$ is  asymptotically of size $\alpha$, that is: if $P=P^1\otimes P^2$, then $\proba{\Delta^*(\X_n)=1}\cv{n\to+\infty} \alpha$.
\item  If  $\Delta^*=\Delta_{h,\alpha}^{*+}$, then $\Delta^*$ is consistent against any alternative $P$ such that $\int h(x,x') dP(x) dP(x') >0$, that is: for such $P$, $\proba{\Delta^*(\X_n)=1}\cv{n\to+\infty} 1$
\item   If  $\Delta^*=\Delta_{h,\alpha}^{*-}$, then $\Delta^*$  is consistent against any alternative $P$ such that $\int h(x,x') dP(x) dP(x') <0.$
\item  If  $\Delta^*=\Delta_{h,\alpha}^{*}$, then $\Delta^*$ is consistent against any alternative  $P$ such that $\int h(x,x') dP(x) dP(x') \neq 0.$
\end{itemize}
\end{thm}

Notice that in the case where $h=h_\varphi$ with an integrable function $\varphi$ (see \eqref{hphi}),
$$\int h(x,x') dP(x) dP({x'}) = \int \varphi(x^1,x^2)\left [dP(x^1,x^2)-dP^1(x^1)dP^2(x^2)\right].$$
This means that the bilateral  test $\Delta_{h,\alpha}^{*}$ is consistent for any alternative such that $\int \varphi(x^1,x^2) dP(x^1,x^2)$ is different from what is expected under $(H_0)$, i.e. $\int \varphi(x^1,x^2) dP^1(x^1)dP^2(x^2)$. When $\varphi=\varphi^w_\delta$ defined by \eqref{defw}, this amounts to state that
$$\beta_{w,\delta}=\int w_\delta(u,v) \left(\esp{dN_{X^1}(u)dN_{X^2}(v)}-\esp{dN_{X^1}(u)}\esp{dN_{X^2}(v)}\right)\neq 0.$$
Under the specific Poisson assumptions of Sansonnet and Tuleau-Malot \cite{LaureCh}, if $w_\delta(u,v)=\psi_{j,k}(v-u)$ where $\delta=(j,k)$ and $\psi_{j,k}$ is a rescaled and translated Haar mother wavelet, $\beta_{w,\delta}$  is linked to the coefficient in the Haar basis of the so-called interaction function, which measures the dependence between both processes $X^1$ and $X^2$. Working non asymptotically, one of the main result of \cite{LaureCh} states, after reformulation in the present setting, that if $\beta_{w,\delta}$ is larger than an explicit lower bound,  then the Type II error is less than a prescribed $\beta \in (0,1)$. Theorem  \ref{niveauconsistance} thus generalizes their result to a set-up with much less reductive assumptions on the underlying stochastic models, but in an asymptotic way.

\subsection{Bootstrap tests with Monte Carlo approximation}\label{secbootMC}

As seen above the tests defined in \eqref{deftest} have satisfactory theoretical properties, but involve an exact computation of the conditional quantiles $q_{n,h,\eta}^*\pa{\X_n}$. Though such a computation is possible, it is not often reasonable in practice even when the sample size $n$ is moderate, since computing $U_{n,h}\!\pa{\X_n^\perm}$ itself may be complex from an algorithmic point of view, from some particular choices for $h$. Therefore, we choose to approximate the conditional quantiles $q_{n,\varphi,\eta}^*\pa{\X_n}$, as usual, by a Monte Carlo method. 

We prove that, even if such a Monte Carlo method is used, the resulting tests have the same asymptotic properties as the ones defined in \eqref{deftest}, as the number of bootstrap samples grows to infinity.

\medskip

Let $B\geq 1$ be a chosen number of iterations for the Monte Carlo method, and $\pa{\X_n^{*1},\ldots,\X_n^{*B}}$ be $B$ independent bootstrap samples from $\X_n$, that is $B$ i.i.d. random variables from the distribution $P_n^1\otimes P_n^2$.

For each ${\bf b}$ in $\{1,\ldots,B\}$,we introduce $U^{* {\bf b}}=U_{n,h}\pa{\X_n^{*{\bf b}}}$. The order statistic associated with  $\pa{U^{*1},\ldots,U^{*B}}$ is now denoted as usual by
$\pa{U^{*(1)},\ldots,U^{* (B)}}$.

Given $\alpha$ in $(0,1)$, we can now introduce the Monte Carlo-based bootstrap tests defined by:
\begin{equation}\label{deftestbootMC}
\left\{
\begin{array}{l c c}
\Delta^{* +}_{B,h,\alpha}\pa{\X_n}=\1{U_{n,h}\pa{\X_n}>U^{* (\lceil (1-\alpha)B \rceil)}},\\
\Delta^{* -}_{B,h,\alpha}\pa{\X_n}=\1{U_{n,h}\pa{\X_n}<U^{* (\lfloor \alpha B \rfloor + 1 )}},\\
\Delta^{*}_{B,h,\alpha}\pa{\X_n}=\Delta^{*+}_{B,h,\alpha/2}(\X_n)+\Delta^{* -}_{B,h,\alpha/2}(\X_n).
\end{array}\right.\end{equation}


\begin{prop}\label{bootMonteCarlo}
Let $\alpha\in(0,1)$, and let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$, with marginals $P^1$ and $P^2$ on $\calX$. 
For every $n\geq 2$, let  $\X_n=(X_1,\dots,X_n)$, and for $B\geq 1$, let $\Delta_B^*$ be one of the tests defined in \eqref{deftestbootMC}. Under the same assumptions as in Theorem \ref{thniveauconsistance}, if $B_n\cv{n\to\infty} +\infty$, then 
\begin{itemize}
\item $\Delta_{B_n}^*$ is asymptotically of size $\alpha$, that is: 
if $P=P^1\otimes P^2$, then $\proba{\Delta_{B_n}^*\pa{\X_n}=1} \cv{n\to+\infty} \alpha$.
\item If $\Delta_{B_n}^*=\Delta_{B_n,h,\alpha}^{* +}$, then $\Delta_{B_n}^*$ is consistent against any alternative $P$ such that\\
 $\int h(x,x')dP(x)dP(x') >0,$ that is: for such $P$, $\PP\pa{\Delta_{B_n}^* \pa{\X_n}=1} \cv{n\to+\infty} 1$.
\item If $\Delta_{B_n}^* =\Delta_{B_n,h,\alpha}^{* -}$, then $\Delta_{B_n}^*$ is consistent against any alternative $P$ such that\\
 $\int h(x,x')dP(x)dP(x') <0.$
\item If $\Delta_{B_n}^*=\Delta_{B_n,h,\alpha}^*$, then $\Delta_{B_n}^*$ is consistent against any alternative $P$ such that\\ $\int h(x,x')dP(x)dP(x') \neq 0.$
\end{itemize}
\end{prop}

% ------------------------------------------------------------ %
\subsection{Back to spike trains analysis in neuroscience}
% ------------------------------------------------------------ %


Let us focus on the case where $h$ is of the form $h_{\Nbc_\delta}$, defined by  \eqref{hphi} and \eqref{defnbc}, which originally motivated the study.
The assumptions of  Theorem  \ref{niveauconsistance} are at least fulfilled in those three generic cases that are of major importance in neuroscience:
\begin{enumerate}
\item The processes $X^1$ and $X^2$ are discretized at resolution $r$ and satisfy  $P^1(\{\emptyset\})P^2(\{\emptyset\})>0$, like in particular Bernoulli processes. In this case, a natural choice for $\delta$ is $\delta=dr+r/2$ for some integer $d$.
\item The processes $X^1$ and $X^2$ have bounded conditional intensities, like in particular Poisson processes.
\item The processes $X^1$ and $X^2$ have conditional intensities bounded by the intensity of linear stationary Hawkes processes with positive interaction, like general  Hawkes processes used in particular to model inhibition (see \cite{HRBR} e.g.).
\end{enumerate}
In any case, $\delta$ should be chosen large enough to ensure that $\Nbc_\delta(X^1,X^2)$ is not a.s. null, but it is always possible to choose such a $\delta$ a posteriori, by looking at the coincidence count on the observed trials.\\
Theorem \label{niveauconsistance} means here that $\Delta_{h_{\Nbc_\delta},\alpha}^{*}$ is asymptotically of power 1 for any alternative $P$ such that 
$$\int \1{|v-u|\leq \delta}\esp{dN_{X^1}(u)dN_{X^2}(v)}\neq \int \1{|v-u|\leq \delta} \esp{dN_{X^1}(u)}\esp{dN_{X^2}(v)}]. $$
Notice that one cannot find such a $\delta$ if heuristically, the repartition of the delays $|v-u|$ between points of $X^1$ and $X^2$ is the same under $(H_0)$ and under $(H_1)$. This also means for neuroscientists, that the cross-correlogram\footnote{histogram of the delays, that is classically represented in neuroscience as the first description of the data} shows no different behavior between the dependent and independent case and this even if one has access to an infinite number of trials. Hence this heuristically means one cannot find such a $\delta$ if the dependence cannot be measured in terms of delay between points. Though this is quite not likely to happen for classical spike trains, the question of the choice of $\delta$ remains an open question for model free independence test.

% ---------------------------------------------------------------------------------------------------- %
\section{Permutation tests of independence}
\label{permsec}
% ---------------------------------------------------------------------------------------------------- %

% ------------------------------------------------------------ %
\subsection{Description of the permutation approach and overview of the results}
% ------------------------------------------------------------ %

We still observe an i.i.d sample $\X_n=\pa{X_1,\dots,X_n}$ with $X_i=\pa{X_i^1,X_i^2}$ of distribution $P$ on $\calX^2$, whose marginals are respectively denoted by $P^1$ and $P^2$. 

We consider here the $U$-statistic $U_{n,h}(\X_n)$ defined by \eqref{defstat}, in the particular case where $h$ equals $h_\varphi$ for some integrable function $\varphi$, as defined in \eqref{hphi}.
The permutation approach we propose to use consists of randomly permuting the second coordinates of the observed pairs of point processes. More precisely, if $\rperm$ denotes a random permutation of \set{1}{n}, the corresponding permuted sample is defined by
\begin{equation}\label{echperm}
\X_n^\rperm=\pa{X_1^\rperm,\dots,X_n^\rperm} \textrm {  with }X_i^\rperm=\pa{X_i^1,X_{\rperm(i)}^2},
\end{equation}
and we denote by $P_n^\star$ the conditional distribution of $\ds X_n^\rperm$ given $\X_n$.

Like for the bootstrap, the idea of the randomization or permutation principle is to mimic the distribution of the test statistics, assuming that $(H_0)$ is satisfied. 

We  actually prove (see Theorem \ref{consistencyperm} and Theorem \ref{TCLUstat}) that under appropriate assumptions, the conditional distribution of $\sqrt{n}U_{n,h_{\varphi}}(\X_n^\rperm)$ given $\X_n$ is asymptotically close to the distribution of $\sqrt{n}U_{n,h_{\varphi}}(\X_n^{\independent})$, where $\X_n^{\independent}$ is an i.i.d. sample from the distribution $P^1\otimes P^2$. Thus, we can propose new permutation tests of independence of the desired asymptotic size, using the conditional quantiles of $\sqrt{n}U_{n,h_{\varphi}}(\X_n^\rperm)$ given $\X_n$ as critical values, and prove that these tests are consistent against any reasonable alternative. Following the statement of our bootstrap results in Section \ref{bootsec}, we still express the closeness in distributions between the permuted and original statistics in terms of Wasserstein's metric, which distinguishes, to our knowledge, our results from previous ones in the permutation tests scene. 

At this stage, since the permutation independence tests satisfy the same asymptotic properties as the bootstrap ones, but with much more computation difficulties to prove them, one might wonder whether the introduction of the permutation tests is of genuine interest. 
Though the bootstrap approaches are known to perform well when moderate or even small sample sizes are considered in practice, there are very few non-asymptotic theoretical results giving evidence of it, since such results are based on concentration inequalities that are not always accessible in complex models (see \cite{Arlot1, Arlot2, FLRB} in testing frameworks, \cite{Fromont} in a classification framework).

One of the main advantages of the permutation approach we use here lies in the resulting tests being exactly of the desired level, even when a Monte Carlo method is used to approximate the critical values. Such non-asymptotic results, stated in Proposition \ref{niveauexact} and Proposition \ref{MonteCarlo}, can be proved without any sophisticated concentration inequality-type tool, only thanks to  \cite[Lemma 1]{RomanoWolf} and the following Proposition, at the heart of the permutation principle.


\begin{prop}
\label{exactdistribution}
Let $\perm$ be a given deterministic permutation of \set{1}{n}, and $\rperm$ be a uniformly distributed random permutation of \set{1}{n}, independent of the observed sample $\ds X_n=\pa{X_1,\dots,X_n}$. 
If $P=P^1\otimes P^2$, that is under $(H_0)$, then the permuted samples  $\ds X_n^\perm$ and  $\ds X_n^\rperm$ defined in \eqref{echperm} both have the same distribution as the original sample $\X_n$.
\end{prop}

% ------------------------------------------------------------ %
\subsection{Consistency of the permutation approach}
% ------------------------------------------------------------ %

Since we only consider in this section $U$-statistics based on symmetric kernels of the form $h_\varphi$, as defined in \eqref{hphi}, we notice that the centring assumption $(\mc{A}_{Cent})$ is satisfied. Therefore, we will only need here the following moment assumption on $\varphi$:

\medskip


$\pa{\mc{A}_{\varphi,Mmt}}\quad  \textrm{\begin{tabular}{|c}  For $X$ with distribution $P$ or $P^1\otimes P^2$ on $\calX^2$,\\
 $\esp{\varphi^4\pa{X^1,X^2}}<\infty.$\end{tabular}}$



\begin{thm}
\label{consistencyperm}
Let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$, with marginals $P^1$ and $P^2$. 
We consider an integrable function $\varphi$ satisfying $(\mc{A}_{\varphi,Mmt})$, $h_\varphi$ and $U_{n,h_\varphi}$ defined by \eqref{hphi} and \eqref{defstat}.
For every $n\geq 2$, let $\X_n=(X_1,\dots,X_n)$, let $\rperm$ be a uniformly distributed random permutation independent of $(X_n)_n$, and $\ds X_n^\rperm$ be the corresponding permuted sample defined by \eqref{echperm}.  In the notation of Section \ref{bootsec},
\begin{equation}
\label{CVstatpermH0}
d_2\pa{\loi{\sqrt{n}U_{n,h_\varphi},P_n^\star\middle|\X_n},\mc{N}\pa{0,\sigma_{h_\varphi,P^1\otimes P^2}^2}} \cvproba{n\to+\infty} 0,
\end{equation}
where $\cvproba{}$ stands for the usual convergence in $\PP$-probability.
\end{thm}


\emph{Comment.} Notice that we do not need the continuity assumption for the kernel $h_\varphi$ that we had in the bootstrap approach, but the existence of a fourth order moment for this kernel, at the price however that the convergence for Wasserstein's metric occurs in probability and not almost surely as for the bootstrap. 

We also wish to emphasize that the above result, whose proof is based on asymptotic normality results for martingale difference arrays, goes further than the ones of Romano \cite{Romano89} for instance, since we have a convergence result which is satisfied whether the independence hypothesis is satisfied or not. 

\medskip

From Theorem \ref{consistencyperm}, we deduce the following Corollary, which will be, combined with Theorem \ref{TCLUstat}, a key point to prove that we can use the conditional quantiles of $\loi{\sqrt{n}U_{n,h_\varphi},P_n^\star\middle|\X_n}$ as critical values to obtain permutation tests of independence of the desired asymptotic size.

\begin{coro}\label{coroPerm}
Let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$, with marginals $P^1$ and $P^2$. 
For every $n\geq 2$, let $\X_n=(X_1,\dots,X_n)$, and $\rperm$ be a uniformly distributed random permutation independent of the $(X_n)_n$. For $\eta$ in $(0,1)$, let $q_{n,\varphi,\eta}^\star\pa{\X_n}$ denote the $\eta$-quantile of $\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^\rperm}$ given $\X_n$ that is $\loi{\sqrt{n}U_{n,h_\varphi},P_n^\star\middle|\X_n}$. 
Then, under the same assumptions as in Theorem \ref{consistencyperm}, 
\begin{equation}
\label{cvquantPerm}
q_{n,\varphi,\eta}^\star\pa{\X_n} \cvproba{n\to+\infty} \Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta).
\end{equation}
\end{coro}


% ------------------------------------------------------------ %
\subsection{Asymptotic properties of the permutation tests}
% ------------------------------------------------------------ %

In the notation of Corollary \ref{coroPerm}, for any fixed $\alpha$ in $(0,1)$, we introduce the following tests:
\begin{equation}\label{deftestperm}
\left\{
\begin{array}{c c l}
\Delta_{\varphi,\alpha}^{\star +}\pa{\X_n} &=& \1{\sqrt{n}U_{n,h_\varphi}\pa{\X_n} > q_{n,\varphi,1-\alpha}^\star\pa{\X_n}},\\
\Delta_{\varphi,\alpha}^{\star -}\pa{\X_n} &=& \1{\sqrt{n}U_{n,h_\varphi}\pa{\X_n}  < q_{n,\varphi,\alpha}^\star\pa{\X_n}},\\
\Delta_{\varphi,\alpha}^\star(\X_n)&=&\Delta_{\varphi,\alpha/2}^{\star +}\pa{\X_n} + \Delta_{\varphi,\alpha/2}^{\star -}\pa{\X_n}.\\
\end{array}\right.
\end{equation}

Notice that like for the bootstrap approach, the critical values used in the tests defined by \eqref{deftestperm} are random, depending on the observed sample $\X_n$. 


Recall that $\rperm$ is a uniformly distributed random permutation on the set $\Sn{n}$ of all the permutations of $\{1,\ldots,n\}$ and independent of $\X_n$. Hence, given $\X_n$, the conditional distribution of $\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^\rperm}$ is discrete and takes the values $\ac{\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^\perm}}_{\perm\in\Sn{n}}$. Let $\sqrt{n}U_{n,h_\varphi}^{(1)}\!\pa{\X_n}\leq \ldots\leq \sqrt{n}U_{n,h_\varphi}^{(n!)}\!\pa{\X_n}$ be these ordered values.
Then, $q^\star_{n,\varphi,\eta}\pa{\X_n}=\sqrt{n}U_{n,h_\varphi}^{(\lceil n!\eta\rceil)}\!\pa{\X_n}$, and it is therefore possible to compute it exactly. \\


\begin{thm}
\label{NivConsistencyPerm}
Let $\alpha\in(0,1)$. Let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$. 
For every $n\geq 2$, let $\X_n=(X_1,\dots,X_n)$, $\Delta^\star$ be one of the permutation tests defined by \eqref{deftestperm}, and assume that $(\mc{A}_{\varphi,Mmt})$  is satisfied. Then,
\begin{itemize}
\item $\Delta^\star$ is asymptotically of size $\alpha$, that is: 
if $P=P^1\otimes P^2$, then $\proba{\Delta^\star\pa{\X_n}=1} \cv{n\to+\infty} \alpha$.
\item If $\Delta^\star=\Delta_{\varphi,\alpha}^{\star +}$, then $\Delta^\star$ is consistent against any alternative $P$ such that\\
 $\int h_\varphi(x,x')dP(x)dP(x') >0,$ or equivalently $\int\varphi\pa{x^1,x^2}\pa{dP(x^1,x^2)-dP^1(x^1)dP^2(x^2)} >0$ that is: for such $P$, $\PP\pa{\Delta^\star \pa{\X_n}=1} \cv{n\to+\infty} 1$.
\item If $\Delta^\star =\Delta_{\varphi,\alpha}^{\star -}$, then $\Delta^\star$ is consistent against any alternative $P$ such that\\
 $\int h_\varphi(x,x')dP(x)dP(x') <0.$
\item If $\Delta^\star=\Delta_{\varphi,\alpha}^\star$, then $\Delta^\star$ is consistent against any alternative $P$ such that\\ $\int h_\varphi(x,x')dP(x)dP(x') \neq 0.$
\end{itemize}
\end{thm}


% ------------------------------------------------------------ %
\subsection{Non-asymptotic properties of the permutation tests}
% ------------------------------------------------------------ %


As seen above, one of the advantages of the permutation approach, as compared with the bootstrap one, lies on the result of Proposition \ref{exactdistribution}, which allows in fact to prove that the tests also satisfy non-asymptotic properties.

Considering for instance $\Delta^\star=\Delta_{\varphi,\alpha}^{\star +}$, then under $(H_0)$, 
\begin{eqnarray}
\nonumber \proba{\Delta^\star \pa{\X_n}=1}&=&\proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n} > q_{n,\varphi,1-\alpha}^\star\pa{\X_n}}\\ 
\label{eq4}
&=&\frac{1}{n!}\sum_{\perm\in \Sn{n}}\proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\perm}} >q_{n,\varphi,1-\alpha}^\star\pa{\X_n^{\perm}}}\\
\label{eq5} &=&\frac{1}{n!}\sum_{\perm\in \Sn{n}}\proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\perm}} >q_{n,\varphi,1-\alpha}^\star\pa{\X_n}},
\end{eqnarray}
where \eqref{eq4} comes from Proposition \ref{exactdistribution}, and  the equality $q_{n,\varphi,1-\alpha}^\star\pa{\X_n^{\perm}}= q_{n,\varphi,1-\alpha}^\star\pa{\X_n}$ in \eqref{eq5} is explained from the fact that the conditional distribution of $\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\rperm\circ \perm}}$ given $\X_n$ is the same as the one of $\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\rperm}}.$
Therefore,
\begin{eqnarray*}
\proba{\Delta^\star \pa{\X_n}=1}&=& \sum_{\perm\in \Sn{n}}\proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\rperm}} >q_{n,\varphi,1-\alpha}^\star\pa{\X_n} \middle | \rperm=\perm} \proba{\rperm=\perm}\\
 &=& \proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\rperm}} >q_{n,\varphi,1-\alpha}^\star\pa{\X_n}} \\ 
&=& \esp{\proba{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^{\rperm}} >q_{n,\varphi,1-\alpha}^\star\pa{\X_n} \middle | \X_n}}\\ 
&\leq& \alpha.
\end{eqnarray*}

These arguments may be adapted to every permutation test defined in \eqref{deftestperm}, leading to the following Proposition.

\begin{prop}\label{niveauexact}
Let $\alpha\in(0,1)$. Let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$ and with marginals $P^1$ and $P^2$ on $\calX$. 
For every $n\geq 2$, let $\X_n=(X_1,\dots,X_n)$, and $\Delta^\star$ be one of the permutation tests defined by \eqref{deftestperm}. Then $\Delta^\star$ is exactly of level $\alpha$, that is: 
if $P=P^1\otimes P^2$, $\proba{\Delta^\star\pa{\X_n}=1}\leq \alpha$.
\end{prop}

\subsection{Permutation tests with Monte Carlo approximation}

The tests defined in \eqref{deftestperm} involve an exact computation of the conditional quantiles $q_{n,\varphi,\eta}^\star\pa{\X_n}$. Such a computation is possible by sorting the $n!$ values of $\ac{U_{n,h_\varphi}\!\pa{\X_n^\perm}}_{\perm\in\Sn{n}}$, but, as for the bootstrap approach, it has not often a reasonable algorithmic complexity. Therefore, we also choose to approximate the conditional quantiles $q_{n,\varphi,\eta}^\star\pa{\X_n}$  by a Monte Carlo method, and we prove that the resulting tests have the same asymptotic properties as the ones defined in \eqref{deftestperm}.

Moreover, it is interesting to point out that, even if they are based on a Monte Carlo approximation, these tests are still exactly of the desired level. This is proved using Proposition \ref{exactdistribution} and \cite[Lemma 1]{RomanoWolf} again.

\medskip

Let $B\geq 1$ be a chosen number of iterations for the Monte Carlo method, and $\pa{\Pi_n^1,\ldots,\Pi_n^B}$ be a sample of i.i.d. random permutations uniformly distributed on $\Sn{n}$.
For each ${\bf b}$ in $\{1,\ldots,B\}$, we introduce $U^{\star {\bf b}}=U_{n,h_\varphi}\pa{\X_n^{\Pi_n^{\bf b}}}$, 
and we denote by $U^{\star (B+1)}$ the statistic $U_{n,h_\varphi}\pa{\X_n}$ computed on the original sample $\X_n$. The order statistic associated with  $\pa{U^{\star 1},\ldots,U^{\star (B+1)}}$ is now denoted as usual by
$\pa{U^{\star (1)},\ldots,U^{\star ((B+1))}}$.

Given $\alpha$ in $(0,1)$, we can now introduce the Monte Carlo-based permutation tests defined by:
\begin{equation}\label{deftestMC}
\left\{
\begin{array}{l c c}
\Delta^{\star +}_{B,\varphi,\alpha}\pa{\X_n}=\1{U_{n,h_\varphi}\pa{\X_n}>U^{\star (\lceil (1-\alpha)(B+1) \rceil)}},\\
\Delta^{\star -}_{B,\varphi,\alpha}\pa{\X_n}=\1{U_{n,h_\varphi}\pa{\X_n}<U^{\star (\lfloor \alpha (B+1) \rfloor + 1 )}},\\
\Delta^{\star}_{B,\varphi,\alpha}\pa{\X_n}=\Delta^{\star +}_{B,\varphi,\alpha/2}(\X_n)+\Delta^{\star -}_{B,\varphi,\alpha/2}(\X_n).
\end{array}\right.\end{equation}


\begin{prop}\label{MonteCarlo}
Let $\alpha\in(0,1)$, and let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$, with marginals $P^1$ and $P^2$ on $\calX$. 
For every $n\geq 2$, let  $\X_n=(X_1,\dots,X_n)$, and for $B\geq 1$, let $\Delta_B^\star$ be one of the tests defined in \eqref{deftestMC}. If $B_n\cv{n\to\infty} +\infty$, if $U_{n,h_\varphi}$ is non degenerate, and if $(\mc{A}_{\varphi,Mmt})$ is satisfied, then 
\begin{itemize}
\item $\Delta_{B_n}^\star$ is asymptotically of size $\alpha$, that is: 
if $P=P^1\otimes P^2$, then $\proba{\Delta_{B_n}^\star\pa{\X_n}=1} \cv{n\to+\infty} \alpha$.
\item If $\Delta_{B_n}^\star=\Delta_{B_n,\varphi,\alpha}^{\star +}$, then $\Delta_{B_n}^\star$ is consistent against any alternative $P$ such that\\
 $\int h_\varphi(x,x')dP(x)dP(x') >0,$ that is: for such $P$, $\PP\pa{\Delta_{B_n}^\star \pa{\X_n}=1} \cv{n\to+\infty} 1$.
\item If $\Delta_{B_n}^\star =\Delta_{B_n,\varphi,\alpha}^{\star -}$, then $\Delta_{B_n}^\star$ is consistent against any alternative $P$ such that\\
 $\int h_\varphi(x,x')dP(x)dP(x') <0.$
\item If $\Delta_{B_n}^\star=\Delta_{B_n,\varphi,\alpha}^\star $, then $\Delta_{B_n}^\star$ is consistent against any alternative $P$ such that\\ $\int h_\varphi(x,x')dP(x)dP(x') \neq 0.$
\end{itemize}
\end{prop}

\begin{prop}\label{MonteCarloniveauexact}
Let $\alpha\in(0,1)$, and let $\pa{X_n}_n$ be a sequence of i.i.d. pairs of point processes with distribution $P$ on $\calX^2$, with marginals $P^1$ and $P^2$ on $\calX$. 
For every $n\geq 2$, let $\X_n=(X_1,\dots,X_n)$, and $\Delta_B^\star$ be one of the tests defined in \eqref{deftestMC}, for some $B\geq 1$. Then $\Delta_B^\star$ is exactly of level $\alpha$, that is: 
if $P=P^1\otimes P^2$, $\proba{\Delta_B^\star\pa{\X_n}=1}\leq \alpha$.
\end{prop}



% ---------------------------------------------------------------------------------------------------- %
\section{Proofs}
\label{proofsec}
% ---------------------------------------------------------------------------------------------------- %

All along the proofs in this section, $C$ and $C'$ will denote universal positive constants, that may vary from one line to another one.

% ------------------------------------------------------------ %
\subsection{Proof of Proposition \ref{inegalite_Wasserstein}}
% ------------------------------------------------------------ %

For some integer $n\geq 2$,  let $\pa{Y_{n,i}^*,Y_i}_{1\leq i\leq n}$ be an i.i.d. sample such that for every $i=1\ldots n$, $Y_{n,i}^*\sim P_n^1\otimes P_n^2$,  and $Y_i\sim P^1\otimes P^2$, and so that in particular:
$$\frac{\sqrt{n}}{n(n-1)} \sum_{i\neq i'} h\pa{ Y_{n,i}^*, Y_{n,i'}^*} \sim \calL\left( \left. \sqrt{n} U_n, P_n^1\otimes P_n^2 \right|\X_n\right), $$
and 
$$\frac{\sqrt{n}}{n(n-1)} \sum_{i\neq i'}h\pa{ Y_i, Y_{i'}} \sim \calL\left( \sqrt{n}U_n,P^1\otimes P^2\right) .$$

From the definition of Wasserstein's metric $d_2$, recalled in \eqref{Wasserstein}, we then deduce that:
\begin{equation}\label{eq1}
d_2^2\left(\calL\left( \left. \sqrt{n} U_n, P_n^1\otimes P_n^2 \right|\X_n\right) , \calL\left( \sqrt{n}U_n,P^1\otimes P^2\right)  \right)
\leq  \frac{1}{n(n-1)^2}\ \Estar{\pa{\sum_{i\neq i'} \pa{h\pa{ Y_{n,i}^*, Y_{n,i'}^*} - 
 h\pa{ Y_i, Y_{i'}}}}^2},
\end{equation}
where the upper bound is finite under $\pa{\mc{A}_{Mmt}^{*}}$. 

Introducing the quantities $$\text{E}_{(i,i',j,j')}=
\Estar{ \Big(h\big(Y_{n,i}^*, Y_{n,i'}^*\big) - h( Y_i, Y_{i'})\Big)\Big( h\big(Y_{n,j}^*, Y_{n,j'}^*\big) - h(Y_j, Y_{j'})\Big)},$$
for $(i,i',j,j')$ in $\ac{1,2,\dots,n}^4$, and the sets $$I_{m}=\ac{(i,i',j,j')\in\ac{1,2,\dots,n}^4 \tq i\neq i',\ j\neq j',\ \#\ac{i,i',j,j'}=m},$$ 
for $m$ in $\{2,3,4\}$, where $\#\ac{i,i',j,j'}$ denotes the number of different elements in $\ac{i,i',j,j'}$, we easily see that:
\begin{equation}\label{eq2}
\Estar{\pa{\sum_{i\neq i'} h\pa{ Y_{n,i}^*, Y_{n,i'}^*} - 
 \sum_{i\neq i'} h\pa{ Y_i, Y_{i'}}}^2}=\sum_{(i,i',j,j')\in I_4} \text{E}_{(i,i',j,j')} + \sum_{(i,i',j,j')\in I_3} \text{E}_{(i,i',j,j')} + \sum_{(i,i',j,j')\in I_2} \text{E}_{(i,i',j,j')}.
\end{equation}
Let us now upper bound each term of this sum separately.
\begin{enumerate}
\item If $(i,i',j,j')\in I_4$, then by independence, 
\begin{eqnarray*}
\text{E}_{(i,i',j,j')} 
&=& \Estar{h\pa{ Y_{n,i}^*, Y_{n,i'}^*} - h\pa{ Y_i, Y_{i'}}}\Estar{h\pa{ Y_{n,j}^*, Y_{n,{j'}}^*} - h\pa{ Y_j, Y_{j'}}}\\
&=& \big(\Estar{h\pa{ Y_{n,i}^*, Y_{n,i'}^*}} - \esp{h\pa{ Y_i, Y_{i'}}}\big)\big(\Estar{h\pa{ Y_{n,j}^*, Y_{n,j'}^*}} - \esp{h\pa{ Y_j, Y_{j'}}}\big).
\end{eqnarray*}
Yet, under assumptions $\pa{\mc{A}_{Cent}}$ and $(\mc{A}_{Cent}^*)$, $\esp{h\pa{ Y_i, Y_{i'}}}=\Estar{h\pa{ Y_{n,i}^*, Y_{n,i'}^*}}=0$, so $\text{E}_{(i,i',j,j')}=0$. 

\item If $(i,i',j,j')\in I_3$, by the Cauchy-Schwarz inequality, 
\begin{eqnarray*}
\text{E}_{(i,i',j,j')} 
&\leq & \sqrt{\Estar{\Big(h\big(Y_{n,i}^*,Y_{n,i'}^*\big) - h(Y_i, Y_{i'})\Big)^2}}\sqrt{\Estar{\Big(h\big(Y_{n,j}^*, Y_{n,j'}^*\big) - h(Y_j, Y_{j'})\Big)^2}}\\
&=& \Estar{\Big(h\big( Y_{n,1}^*, Y_{n,2}^*\big) - h(Y_1, Y_2)\Big)^2}.
\end{eqnarray*}

\item If $(i,i',j,j')\in I_2$, we immediately obtain that
\begin{eqnarray*}
\text{E}_{(i,i',j,j')} 
&=& \Estar{\pa{h\pa{ Y_{n,1}^*,Y_{n,2}^*} - h\pa{ Y_1, Y_2}}^2}.
\end{eqnarray*}

\end{enumerate}

Since  $\#I_3 = 4 n(n-1)(n-2)$ and $\#I_2 = 2 n(n-1)$, we thus obtain that
\begin{equation}\label{eq3}
\sum\limits_{\substack{i\neq i'\\ j\neq j'}} \text{E}_{(i,i',j,j')} \leq  4 n(n-1)^2\ \Estar{\Big(h\big(Y_{n,1}^*,Y_{n,2}^*\big) - h\big(Y_1,Y_2\big)\Big)^2}.
\end{equation}

From \eqref{eq1}, \eqref{eq2}, \eqref{eq3}, we finally derive that

\begin{equation*}
d_2^2\left(\calL\left( \left. \sqrt{n} U_n, P_n^1\otimes P_n^2 \right|\X_n\right) , \calL\left( \sqrt{n}U_n,P^1\otimes P^2\right)  \right)
\leq 4\ \Estar{\Big(h\big(Y_{n,1}^*,Y_{n,2}^*\big) - h\big(Y_1,Y_2\big)\Big)^2},
\end{equation*}
where the i.i.d sample $\pa{ Y_{n,i}^*,  Y_i}_{1\leq 2}$ may be arbitrarily chosen such that for every $i$ in $\{1,2\}$, $ Y_{n,i}^* \thicksim P_n^1\otimes P_n^2$,  $Y_i \thicksim P^1\otimes P^2$. This ends the proof of Proposition \ref{inegalite_Wasserstein}.


% ------------------------------------------------------------ %
\subsection{Proof of Proposition \ref{LGNhcarre}}
% ------------------------------------------------------------ %

Let us first notice that  \eqref{cvh} is a direct application of the strong law of large numbers for $U$-statistics, stated for instance in \cite{Hoeffding61}.

Next, let us notice that if
$$g_{m}\pa{X_{i_1},\dots,X_{i_{m}}}=
\sum_{\pa{i,j,k,l}\in I_{\{i_1,\ldots i_m\}}}h^2\pa{\pa{X_i^1,X_j^2},\pa{X_k^1,X_l^2}},$$
with 
$I_{\{i_1,\ldots i_m\}}=\ac{\pa{i,j,k,l}\in \ac{i_1,\ldots i_m}^4 / \# \ac{i,j,k,l}=m},$  and $m$ in $\ac{1,\ldots,4}$, then
$$\frac{1}{n^4}\sum_{i,j,k,l=1}^n h^2\pa{\pa{X_i^1,X_j^2},\pa{X_k^1,X_l^2}} = \sum_{m=1}^4 \frac{1}{m!} \pa{\frac{1}{n^4} \sum\limits_{\substack{\pa{i_1,\dots,i_m}\in \{1,\ldots,n\}^m\\ i_1,\dots,i_m \textrm{ all different} }} g_{m}\pa{X_{i_1},\dots,X_{i_{m}}}}.$$
Each of the four terms in the right hand side of the above decomposition being, up to a multiplicative factor, some classical $U$-statistic, and since under $\pa{\mc{A}_{Mmt}^{*}}$, $\esp{|g_{m}(X_{i_1},\dots,X_{i_m})|}<+\infty$, we can now apply the strong law of large numbers for $U$-statistics proved by Hoeffding in \cite{Hoeffding61} for instance.
Therefore $P$-a.s. in $(X_n)_n$,
$$\frac{1}{n(n-1)\dots(n-m+1)}\sum\limits_{\substack{\pa{i_1,\dots,i_m}\in \{1,\ldots,n\}^m\\ i_1,\dots,i_m \textrm{ all different} }}g_{m}\pa{X_{i_1},\dots,X_{i_{m}}}\cv{n\to+\infty} \esp{g_{m}\pa{X_1,\dots,X_{m}}}.$$
In particular, for $m\in\ac{1,2,3}$, $P$-a.s. in $(X_n)_n$,
$$\frac{1}{n^4}\sum\limits_{\substack{\pa{i_1,\dots,i_m}\in \{1,\ldots,n\}^m\\ i_1,\dots,i_m \textrm{ all different} }}g_{m}\pa{X_{i_1},\dots,X_{i_{m}}} \cv{n\to+\infty} 0,$$
and 
$$ \frac{1}{n^4}\sum\limits_{\substack{\pa{i_1,\dots,i_4}\in \{1,\ldots,n\}^4\\ i_1,\dots,i_4 \textrm{ all different} }} g_4\pa{X_{i_1},X_{i_2},X_{i_3},X_{i_4}} \cv{n\to+\infty} \esp{g_4\pa{X_1,X_2,X_3,X_4}}.$$

Finally noticing that $\esp{g_4\pa{X_1,X_2,X_3,X_4} }=  4!\ \esp{h^2\pa{\pa{X_{1}^1,X_{2}^2},\pa{X_{3}^1,X_{4}^2}}},$
this concludes the proof.

% ------------------------------------------------------------ %
\subsection{Proof of Theorem \ref{Convergence}}
% ------------------------------------------------------------ %

By Proposition \ref{inegalite_Wasserstein},  for all $n\geq 2$, 
\begin{multline*}
d_2\Big(\loi{\sqrt{n}U_{n,h},P_n^1\otimes P_n^2\middle | \X_n},\loi{\sqrt{n}U_{n,h},P^1\otimes P^2}\Big) \\
\leq 
C \inf\limits_{\substack{\pa{Y_{n,1}^*,Y_1}, \pa{Y_{n,2}^*,Y_2}\ i.i.d \tq\\
Y_{n,1}^*,Y_{n,2}^* \thicksim P_n^1\otimes P_n^2,\ 
Y_1, Y_2 \thicksim P^1\otimes P^2}}
\Estar{\Big(h\pa{Y_{n,1}^*,Y_{n,2}^*}-h\pa{Y_1,Y_2}\Big)^2}.
\end{multline*}

Our goal is here to construct, for almost all $\omega$ in $\Omega$, a sequence of random variables $\pa{\bar{Y}_{n,\omega,1}^*}_{n\geq 1}$ such that for every $n\geq 1$, $\bar{Y}_{n,\omega,1}^*\sim P_{n,\omega}^1\otimes P_{n,\omega}^2$, where $P_{n,\omega}^j=n^{-1}\sum_{i=1}^n \delta_{X_i^j(\omega)}$ ($j=1,2$) are the empirical measures corresponding to the realisation $\X_n(\omega)$,
a random variable $\bar{Y}_{\omega,1} \sim P^1\otimes P^2$, and $\ac{\pa{\bar{Y}_{n,\omega,2}^*}_{n\geq 1},\bar{Y}_{\omega,2}}$ an independent copy of $\ac{\pa{\bar{Y}_{n,\omega,1}^*}_{n\geq 1},\bar{Y}_{\omega,1}}$ on some probability space $\pa{\Omega_\omega', \mc{A}_\omega',\PP_\omega'}$ depending on $\omega$ such that
\begin{equation}
\label{CVL2}
\mathds{E}'_{\omega}\cro{\Big(h\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} - h\pa{\bar{Y}_{\omega,1},\bar{Y}_{,\omega2}}\Big)^2} \cv{n\to+\infty} 0,
\end{equation}
where $\mathds{E}'_{\omega}$ denotes the expectation corresponding to $\PP_\omega'$.
From this, we shall directly deduce that, for almost all $\omega$ in $\Omega$, 
\begin{multline*}
\inf\limits_{\substack{\pa{Y_{n,1}^*,Y_1}, \pa{Y_{n,2}^*,Y_2}\ i.i.d \tq\\
Y_{n,1}^*,Y_{n,2}^* \thicksim P_{n,\omega}^1\otimes P_{n,\omega}^2, \ 
Y_1, Y_2 \thicksim P^1\otimes P^2}}\Estar{\Big(h\pa{Y_{n,1}^*,Y_{n,2}^*}-h\pa{Y_1,Y_2}\Big)^2}(\omega) \\
\leq \mathds{E}'_{\omega}\cro{\Big(h\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} - h\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\Big)^2} \cv{n\to+\infty} 0,
\end{multline*}
which will conclude the proof of the theorem. \\ 

Consider $(\Omega,\mc{A},\PP)$ the probability space on which all the $X_n$'s are defined. In what follows, we can keep in mind that $\Omega$ represents the randomness in the original sequence $(X_n)_n$. Thus, a given $\omega$ in $\Omega$ represents a given realisation of $(X_n)_n$.

As a preliminary step, from Proposition \ref{LGNhcarre}, there exists some subset $\Omega_1$ of $\Omega$ such that $\PP(\Omega_1)=1$ and for every $\omega$ in $\Omega_1$, 
\begin{equation}
\label{LLNh2}
\frac{1}{n^4} \sum_{i,j,k,l=1}^n h^2\pa{\pa{X_i^1(\omega),X_j^2(\omega)},\pa{X_k^1(\omega),X_l^2(\omega)}} \cv{n\to+\infty} \esp{h^2\pa{\pa{X_1^1,X_2^2},\pa{X_3^1,X_4^2}}}.
\end{equation}

Applying Theorem 3 in \cite{Varadarajan58}, since $\pa{\calX,d_{\calX}}$ defined by \eqref{defmetric} is  separable,  $P$-a.s. in $(X_n)_n$, $P_n^1 \Rightarrow P^1$ and $P_n^2 \Rightarrow P^2$. We deduce that
there exists some subset $ \Omega_2$ of $\Omega$ such that $ \PP(\Omega_2)=1$ and for every $\omega$ in $\Omega_2,$ 
\begin{equation}
\label{CVfaible}
P_{n,\omega}^1\otimes P_{n,\omega}^2 \Rightarrow P^1\otimes P^2,
\end{equation} 

Now, let us consider $\Omega_0=\Omega_1\cap\Omega_2$, and fix $\omega$ in $\Omega_0$. \\
Following the proof of Skorokhod's representation theorem in \cite[Theorem 11.7.2  p.415]{Dudley}, since $(\calX^2, d_{\calX^2})$, as defined from \eqref{defmetric} in Section \ref{ma}, is a separable space, it is possible to construct some probability space $\pa{\Omega_\omega', \mc{A}_\omega',\PP_\omega'}$, and some random variables 
$\bar{Y}_{n,\omega,1}^*:\Omega_\omega'\rightarrow  \calX^2$, $\bar{Y}_{n,\omega,2}^*:\Omega_\omega'\rightarrow  \calX^2$ with distribution  $P_{n,\omega}^1\otimes P_{n,\omega}^2$, and 
$\bar{Y}_{\omega,1}:\Omega_\omega'\rightarrow \calX^2$, $\bar{Y}_{\omega,2}:\Omega_\omega'\rightarrow \calX^2$  with distribution $P^1\otimes P^2$ such that
$$\PP_\omega'\mbox{-a.s.},\quad \bar{Y}_{n,\omega,1}^* \cv{n\to+\infty} \bar{Y}_{\omega,1},$$
$$\PP_\omega'\mbox{-a.s.},\quad \bar{Y}_{n,\omega,2}^* \cv{n\to+\infty} \bar{Y}_{\omega,2},$$
$\left\{\pa{\bar{Y}_{n,\omega,1}^*}_{n\geq 1},  \bar{Y}_{\omega,1}\right\}$ and $\left\{\pa{\bar{Y}_{n,\omega,2}^*}_{n\geq 1},  \bar{Y}_{\omega,2}\right\}$ being independent, so that:

\begin{equation}
\label{cvpsbar}
\PP_\omega'\mbox{-a.s.},\quad \pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} \cv{n\to+\infty} \pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}},
\end{equation}
with respect to the product metric $d$ defined by \eqref{distance}.

But under $\pa{\mc{A}_{Cont}}$, $h$ is continuous on $\mathcal{C}$ such that $\PP_\omega'\pa{\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\in \mathcal{C}}=\pa{P^1\otimes P^2}^{\otimes 2}(\mathcal{C})=1$,  hence
$$\PP_\omega'\mbox{-a.s.}, \quad  h\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} \cv{n\to+\infty} h\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}.$$

Since $\PP_\omega'$-a.s. convergence implies convergence in probability, in order to obtain the $\ds L^2$-convergence in \eqref{CVL2}, according to Theorem 16.6  p. 165 of \cite{Schilling05}, we only need to prove that the sequence $\pa{ h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}}_{n\geq 1}$ is uniformly integrable, or that 
$$\mathds{E}'_{\omega}\left[h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}\right]\cv{n\to+\infty}\mathds{E}'_{\omega}\left[ h^2\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\right].$$

Moreover by \eqref{LLNh2} we have
\begin{eqnarray*}
\mathds{E}'_{\omega}\cro{h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}}
&=&\frac{1}{n^4}\sum_{i,j,k,l=1}^n h^2\pa{(X_i^1(\omega),X_j^2(\omega)),(X_k^1(\omega),X_l^2(\omega))} \\
&\cv{n\to+\infty}& \esp{h^2\pa{\pa{X_1^1,X_2^2},\pa{X_3^1,X_4^2}}} 
= \mathds{E}'_{\omega}\left[ h^2\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\right].
\end{eqnarray*}
We finally obtained \eqref{CVL2} for any $\omega$ in $\Omega_0$, with $\PP(\Omega_0)=1$, which ends the proof.



%By Proposition \ref{inegalite_Wasserstein},  for all $n\geq 2$, 
%\begin{multline*}
%d_2\Big(\loi{\sqrt{n}U_{n,h},P_n^1\otimes P_n^2\middle | \X_n},\loi{\sqrt{n}U_{n,h},P^1\otimes P^2}\Big) \\
%\leq 
%C \inf\limits_{\substack{\pa{Y_{n,1}^*,Y_1}, \pa{Y_{n,2}^*,Y_2}\ i.i.d \tq\\
%Y_{n,1}^*,Y_{n,2}^* \thicksim P_n^1\otimes P_n^2,\ 
%Y_1, Y_2 \thicksim P^1\otimes P^2}}
%\Estar{\Big(h\pa{Y_{n,1}^*,Y_{n,2}^*}-h\pa{Y_1,Y_2}\Big)^2}.
%\end{multline*}
%
%Our goal is here to construct a sequence of random variables $\pa{\bar{Y}_{n,1}^*}_{n\geq 1}$ such that for every $n\geq 1$, $\bar{Y}_{n,1}^*\sim P_n^1\otimes P_n^2$, a random variable $\bar{Y}_1 \sim P^1\otimes P^2$, and $\ac{\pa{\bar{Y}_{n,2}^*}_{n\geq 1},\bar{Y}_2}$ an independent copy of $\ac{\pa{\bar{Y}_{n,1}^*}_{n\geq 1},\bar{Y}_1}$ such that $P$-a.s. in $(X_n)_n$,
%\begin{equation}
%\label{CVL2}
%\Estar{\Big(h\pa{\bar{Y}_{n,1}^*,\bar{Y}_{n,2}^*} - h\pa{\bar{Y}_{1},\bar{Y}_{2}}\Big)^2} \cv{n\to+\infty} 0.
%\end{equation}
%From this, we shall directly deduce that 
%$$\inf\limits_{\substack{\pa{Y_{n,1}^*,Y_1}, \pa{Y_{n,2}^*,Y_2}\ i.i.d \tq\\
%Y_{n,1}^*,Y_{n,2}^* \thicksim P_n^1\otimes P_n^2, \ 
%Y_1, Y_2 \thicksim P^1\otimes P^2}}\Estar{\Big(h\pa{Y_{n,1}^*,Y_{n,2}^*}-h\pa{Y_1,Y_2}\Big)^2} \leq \Estar{\Big(h\pa{\bar{Y}_{n,1}^*,\bar{Y}_{n,2}^*} - h\pa{\bar{Y}_{1},\bar{Y}_{2}}\Big)^2} \cv{n\to+\infty} 0,$$
%which will conclude the proof of the theorem. \\ 
%
%Consider $(\Omega,\mc{A},\PP)$ the probability space on which all the $X_n$'s are defined. In what follows, we can keep in mind that $\Omega$ represents the randomness in the original sequence $(X_n)_n$. Thus, a given $\omega$ in $\Omega$ represents a given realisation of $(X_n)_n$.
%
%As a preliminary step, from Proposition \ref{LGNhcarre}, there exists some subset $\Omega_1$ of $\Omega$ such that $\PP(\Omega_1)=1$ and for every $\omega$ in $\Omega_1$, 
%\begin{equation}
%\label{LLNh2}
%\frac{1}{n^4} \sum_{i,j,k,l=1}^n h^2\pa{\pa{X_i^1(\omega),X_j^2(\omega)},\pa{X_k^1(\omega),X_l^2(\omega)}} \cv{n\to+\infty} \esp{h^2\pa{\pa{X_1^1,X_2^2},\pa{X_3^1,X_4^2}}}.
%\end{equation}
%
%Applying Theorem 3 in \cite{Varadarajan58}, since $\pa{\calX,d_{\calX}}$ defined by \eqref{defmetric} is  separable,  $P$-a.s. in $(X_n)_n$, $P_n^1 \Rightarrow P^1$ and $P_n^2 \Rightarrow P^2$. We deduce that
%there exists some subset $ \Omega_2$ of $\Omega$ such that $ \PP(\Omega_2)=1$ and for every $\omega$ in $\Omega_2,$ 
%\begin{equation}
%\label{CVfaible}
%P_{n,\omega}^1\otimes P_{n,\omega}^2 \Rightarrow P^1\otimes P^2,
%\end{equation} 
%where $P_{n,\omega}^j=n^{-1}\sum_{i=1}^n \delta_{X_i^j(\omega)}$ ($j=1,2$) are the empirical measures corresponding to the realisation $\X_n(\omega)$.\\
%
%Now, let us consider $\Omega_0=\Omega_1\cap\Omega_2$, and fix $\omega$ in $\Omega_0$. \\
%Following the proof of Skorokhod's representation theorem in \cite[Theorem 11.7.2  p.415]{Dudley}, since $(\calX^2, d_{\calX^2})$, as defined from \eqref{defmetric} in Section \ref{ma}, is a separable space, it is possible to construct some probability space $\pa{\Omega_\omega', \mc{A}_\omega',\PP_\omega'}$, and some random variables 
%$\bar{Y}_{n,\omega,1}^*:\Omega_\omega'\rightarrow  \calX^2$, $\bar{Y}_{n,\omega,2}^*:\Omega_\omega'\rightarrow  \calX^2$ with distribution  $P_{n,\omega}^1\otimes P_{n,\omega}^2$, and 
%$\bar{Y}_{\omega,1}:\Omega_\omega'\rightarrow \calX^2$, $\bar{Y}_{\omega,2}:\Omega_\omega'\rightarrow \calX^2$  with distribution $P^1\otimes P^2$ such that
%$$\PP_\omega'\mbox{-a.s.},\quad \bar{Y}_{n,\omega,1}^* \cv{n\to+\infty} \bar{Y}_{\omega,1},$$
%$$\PP_\omega'\mbox{-a.s.},\quad \bar{Y}_{n,\omega,2}^* \cv{n\to+\infty} \bar{Y}_{\omega,2},$$
%$\left\{\pa{\bar{Y}_{n,\omega,1}^*}_{n\geq 1},  \bar{Y}_{\omega,1}\right\}$ and $\left\{\pa{\bar{Y}_{n,\omega,2}^*}_{n\geq 1},  \bar{Y}_{\omega,2}\right\}$ being independent, so that:
%
%\begin{equation}
%\label{cvpsbar}
%\PP_\omega'\mbox{-a.s.},\quad \pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} \cv{n\to+\infty} \pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}},
%\end{equation}
%with respect to the product metric $d$ defined by \eqref{distance}.
%
%But under $\pa{\mc{A}_{Cont}}$, $h$ is continuous on $\mathcal{C}$ such that $\PP_\omega'\pa{\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\in \mathcal{C}}=\pa{P^1\otimes P^2}^{\otimes 2}(\mathcal{C})=1$,  hence
%$$\PP_\omega'\mbox{-a.s.}, \quad  h\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*} \cv{n\to+\infty} h\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}.$$
%
%Since $\PP_\omega'$-a.s. convergence implies convergence in probability, in order to obtain the $\ds L^2$-convergence in \eqref{CVL2}, according to Theorem 16.6  p. 165 of \cite{Schilling05}, we only need to prove that the sequence $\pa{ h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}}_{n\geq 1}$ is uniformly integrable, or that 
%$$\mathds{E}'_{\omega}\left[h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}\right]\cv{n\to+\infty}\mathds{E}'_{\omega}\left[ h^2\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\right].$$
%\comPat{J'ai change les esperance, Melisande est d'accord et j'ai un peu modifie ci-dessous du coup. Reverifier que ca vous va.}
%Yet,  as we fixed $\omega$ in $\Omega_0\subset\Omega_1$,we have that
%$$\mathds{E}'_{\omega}\left[h^2\pa{\bar{Y}_{n,\omega,1}^*,\bar{Y}_{n,\omega,2}^*}\right]=\Estar{h^2\pa{\bar{Y}_{n,1}^*,\bar{Y}_{n,2}^*}}$$
%and $$\mathds{E}'_{\omega}\left[ h^2\pa{\bar{Y}_{\omega,1},\bar{Y}_{\omega,2}}\right]=\esp{h^2\pa{\bar{Y}_1,\bar{Y}_2}}.$$
%
%Moreover by \eqref{LLNh2} we have
%\begin{eqnarray*}
%\Estar{h^2\pa{\bar{Y}_{n,1}^*,\bar{Y}_{n,2}^*}}
%&=&\frac{1}{n^4}\sum_{i,j,k,l=1}^n h^2\pa{(X_i^1(\omega),X_j^2(\omega)),(X_k^1(\omega),X_l^2(\omega))} \\
%&\cv{n\to+\infty}& \esp{h^2\pa{\pa{X_1^1,X_2^2},\pa{X_3^1,X_4^2}}} = \esp{h^2\pa{\bar{Y}_1,\bar{Y}_2}}.
%\end{eqnarray*}
%We finally obtained \eqref{CVL2} for any $\omega$ in $\Omega_0$, with $\PP(\Omega_0)=1$, which ends the proof.
%

% ------------------------------------------------------------ %
\subsection{Proof of Proposition \ref{TCLUstat}}
% ------------------------------------------------------------ %

Let $(X_n)_n$ be a sequence of i.i.d pairs of point processes with distribution $P^1\otimes P^2$ on $\calX^2$. 
Then, according to $\pa{\mc{A}_{Cent}}$, for all $i\neq j$, $\esp{h(X_i,X_j)}=0$. \\
For a better readability, we introduce $\esp{h\middle | X_i} = \esp{h(X_i,X)\middle| X_i} = \esp{h(X,X_i)\middle| X_i}$ for some $X$ with distribution $P^1\otimes P^2$, and independent of $X_i$.
By Hoeffding's decomposition for non degenerate $U$-statistics, we obtain
\begin{eqnarray*}
\sqrt{n} U_{n,h}(\X_n) &=& \frac{2}{\sqrt{n}(n-1)}\sum_{i<j} h(X_i,X_j) \\
&=& \frac{2}{\sqrt{n}(n-1)}\underbrace{ \sum_{i<j}\pa{\esp{h\middle | X_i}+\esp{h\middle | X_j}}}_{T_n} + \frac{2}{\sqrt{n}(n-1)}\underbrace{\sum_{i<j}g(X_i,X_j)}_{M_n},
\end{eqnarray*}
with $g(X_i,X_j) = h(X_i,X_j) - \esp{h\middle | X_i}- \esp{h\middle | X_j}$.  

\begin{itemize}
\item Let us prove that 
\begin{equation}
\label{EqCVM_n}
\frac{2}{\sqrt{n}(n-1)}M_n \cvproba{n\to+\infty}0.
\end{equation}
We first notice that for $i< j$,  $\esp{g(X_i,X_j)}=0$, $\esp{g^2(X_i,X_j)}<+\infty$, and give a simpler expression for
 $\esp{M_n^2} = \sum_{i<j}\sum_{k<l} \esp{g(X_i,X_j)g(X_k,X_l)}.$
If $\ac{i,j}\cap \ac{k,l}=\emptyset$, $i<j$, $k<l$, then $\esp{g(X_i,X_j)g(X_k,X_l)}=\left(\esp{g(X_i,X_j)}\right)^2=0$.
If $\#(\ac{i,j}\cap \ac{k,l})=1$, with for instance $k=i,\ j\neq l$, ($i<j$, $i<l$) (the other cases may be treated similarly), then $$\esp{g(X_i,X_j)g(X_i,X_l)}=\esp{\esp{g(X_i,X_j)\middle | X_i} \esp{  g(X_i,X_l)\middle | X_i}}=0.$$
Finally, we obtain 
$$\esp{M_n^2} = \sum_{i<j} \esp{g^2(X_i,X_j)} = \frac{n(n-1)}{2} \esp{g^2(X_1,X_2)}. $$
As a consequence, for $\varepsilon>0$, Chebychev's inequality leads to
\begin{eqnarray*} 
\proba{\left|\frac{2}{\sqrt{n}(n-1)}M_n\right| >\varepsilon} 
&=& \proba{\left|M_n\right| > \sqrt{n}(n-1)\frac{\varepsilon}{2}} \\ 
&\leq & \frac{4\esp{M_n^2}}{n(n-1)^2 \varepsilon^2} 
= \frac{2 \esp{g^2(X_1,X_2)}}{(n-1)\varepsilon^2} \cv{n\to+\infty} 0.
\end{eqnarray*}
\item Now, let us prove that 
\begin{equation}
\label{EqCVT_n}
\frac{2}{\sqrt{n}(n-1)}T_n \cvloi{n\to \infty} \mc N\pa{0,\sigma_{h,P^1\otimes P^2}^2}.
\end{equation}
Notice that $T_n = \sum_{i=1}^n (n-i) \esp{h\middle|X_i} + \sum_{j=1}^{n} (j-1) \esp{h\middle|X_j} 
= (n-1) \sum_{i=1}^n \esp{h\middle|X_i}. $ \\ 
Thus, $$\frac{2}{\sqrt{n}(n-1)}T_n = \frac{2}{\sqrt{n}}\sum_{i=1}^n \esp{h\middle|X_i}.$$ 
Since the $\esp{h\middle|X_i}$ are i.i.d, with $\esp{\esp{h\middle|X_i}}=0$ and $\var{\esp{h\middle|X_i}}= {\sigma_{h,P^1\otimes P^2}^2}/4,$ thanks to $\pa{\mc{A}_{Mmt}}$, the Central Limit Theorem leads to \eqref{EqCVT_n}. 
\end{itemize}

Finally, combining \eqref{EqCVM_n} and \eqref{EqCVT_n}, Slutsky's lemma ensures the convergence in distribution of $\sqrt{n}U_{n,h}(\X_n)$ towards $\mc N\pa{0,\sigma_{h,P^1\otimes P^2}^2}$.

\medskip


Now, notice that
$$\esp{\pa{\sqrt{n}U_{n,h}(\X_n)}^2} = \frac{1}{n(n-1)^2} \sum_{i\neq i'}\sum_{j\neq j'} \esp{h(X_i,X_{i'})h(X_j,X_{j'})}.$$
Let us consider all the cases where $i\neq i'$ and $j\neq j'$.

If $\#\{i,i',j,j'\} = 4$, then by independence,
$\esp{h(X_i,X_{i'})h(X_j,X_{j'})} = \esp{h(X_i,X_{i'})}\esp{h(X_j,X_{j'})}=0.$

If $\#\{i,i',j,j'\} = 3$, then, by symmetry of $h$,
$\esp{h(X_i,X_{i'})h(X_j,X_{j'})} = \esp{h(X_1,X_2) h(X_1,X_2)} = \sigma^2_{h,P^1 \otimes P^2}/4$.

Finally, if $\#\{i,i',j,j'\} = 2$,
$\esp{h(X_i,X_{i'})h(X_j,X_{j'})} = \esp{\pa{h(X_1,X_2)}^2}$.\\
Therefore,
$$\esp{\pa{\sqrt{n}U_{n,h}(\X_n)}^2} = \frac{1}{n(n-1)^2} \pa{ 4n(n-1)(n-2) \frac{\sigma^2_{h,P^1 \otimes P^2}}{4} + 2 n(n-1)\esp{\pa{h(X_1,X_2)}^2}}
\cv{n\to\infty} \sigma^2_{h,P^1 \otimes P^2},$$
which ends the proof of Proposition \ref{TCLUstat}.

% ------------------------------------------------------------ %
\subsection{Proof of Corollary \ref{coroBoot}}
% ------------------------------------------------------------ %
By Proposition \ref{TCLUstat}, we have that
\begin{equation}\label{macvenloi}
\calL\left(\sqrt{n}U_{n,h},P^1\otimes P^2 \right)\Rightarrow \mathcal{N}(0, \sigma^2_{h,P^1 \otimes P^2}),
\end{equation}
where $\mathcal{N}(0, \sigma^2_{h,P^1 \otimes P^2})$ has a continuous c.d.f. Therefore, by application of \cite[Lemma 2.11]{vandervaart}, $$\sup_{z\in \R} \left|\proba{\sqrt{n}U_{n,h}(\X_n^{\independent})\leq z}-\Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right|\cv{n\to+\infty} 0.$$
Furthermore, since convergence w.r.t the $d_2$ distance implies weak convergence, Theorem \ref{Convergence} combined with \eqref{macvenloi} leads to
\begin{equation}\label{macvenloistar}
\calL\left(\left.\sqrt{n}U_{n,h},P_n^1\otimes P_n^2 \right|\X_n\right)\Rightarrow \mathcal{N}(0, \sigma^2_{h,P^1 \otimes P^2})\quad \textrm{$P$-a.s. in $(X_n)_n$},
\end{equation}
so
\begin{equation}
\label{cvsupCondGauss}
\sup_{z\in \R} \left|\proba{\sqrt{n}U_{n,h}(\X_n^*)\leq z|\X_n}-\Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right|\cv{n\to+\infty} 0\quad \textrm{$P$-a.s. in $(X_n)_n$}.
\end{equation}
As a consequence,
$$\sup_{z\in \R} \left|\proba{\sqrt{n}U_{n,h}(\X_n^*)\leq z|\X_n}-\proba{\sqrt{n}U_{n,h}(\X_n^{\independent})\leq z}\right|\cv{n\to+\infty} 0\quad \textrm{$P$-a.s. in $(X_n)_n$}.$$
As for the second part of the result, since $\Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}$ is continuous on $[0,1]$, it is sufficient to apply \cite[Lemma 21.2]{vandervaart} to both \eqref{macvenloi} and \eqref{macvenloistar}, on the event where \eqref{macvenloistar} holds,  to obtain that $q_{n,h,\eta}^*(\X_n)$ and $q_{n,h,\eta}^{\independent}$ both converge towards $\Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta)$ on the same event. This ends the proof.

% ------------------------------------------------------------ %
\subsection{Proof of Theorem \ref{thniveauconsistance}}
% ------------------------------------------------------------ %
Let us focus on the case $\Delta=\Delta_{h,\alpha}^{*+}$, the proof in the other cases being similar. 
Given $\alpha$ in $(0,1)$, we deduce from Corollary \ref{coroBoot} that
\begin{equation}\label{cvqstar}
q_{n,h,1-\alpha}^*\pa{\X_n}\cv{n\to+\infty}\Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(1-\alpha)\quad \textrm{$P$-a.s. in $(X_n)_n$}.
\end{equation}
Then, from Proposition \ref{TCLUstat} and Slutsky's lemma, under $(H_0)$, $\pa{\sqrt{n} U_{n,h}(\X_n), q_{n,h,1-\alpha}^*\pa{\X_n}}$ converges in distribution towards $(Z,\Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(1-\alpha))$, where $Z\sim \mathcal{N}(0,\sigma^2_{h,P^1 \otimes P^2})$. Therefore, under $(H_0)$,
$$\proba{\sqrt{n} U_{n,h}(\X_n)>q_{n,h,1-\alpha}^*\pa{\X_n}}\cv{n\to+\infty}\alpha.$$

Under $(H_1)$, by Proposition \ref{LGNhcarre},
$$U_{n,h}(\X_n)\cv{n\to+\infty} \int h(x,x') dP(x) dP(x') > 0 \quad \textrm{$P$-a.s. in $(X_n)_n$}.$$
Furthermore, from \eqref{cvqstar}, we deduce that
$$q_{n,h,1-\alpha}^*\pa{\X_n}/\sqrt{n}\cv{n\to+\infty} 0 \quad \textrm{$P$-a.s. in $(X_n)_n$},$$ as a consequence, there exists an integer $n_0$ large enough such that for every $n\geq n_0$,
$$\proba{\sqrt{n} U_{n,h}(\X_n)\leq q_{n,h,1-\alpha}^*\pa{\X_n}}\leq \proba{\frac{1}{2}\int h(x,x') dP(x) dP(x')\leq q_{n,h,1-\alpha}^*\pa{\X_n}/\sqrt{n}}\cv{n\to+\infty}0,$$
since $\int h(x,x') dP(x) dP(x') >0.$


% ------------------------------------------------------------ %
\subsection{Proof of Proposition \ref{bootMonteCarlo}}
% ------------------------------------------------------------ %


Let us focus again on the case $\Delta_{B_n}^*=\Delta_{B_n,h,\alpha}^{* +}$, the other cases being similar. 
First notice that $\Delta_{B_n}^*\pa{\X_n} = \1{\sqrt{n}U_{n,h}\pa{\X_n}>\sqrt{n}U^{* (\lceil (1-\alpha)B_n \rceil)}}$. 

Let us introduce the c.d.f $F^*_{n,\X_n}$ of $\loi{\sqrt{n}U_{n,h_\varphi},P_n^1\otimes P_n^2 | \X_n}$ defined by$F^*_{n,\X_n}(z) = \proba{\sqrt{n}U_{n,h}(\X_n^*)\leq z|\X_n}$,
and let $F^{*,B_n}_{n,\X_n}$ be the empirical c.d.f of $\loi{\sqrt{n}U_{n,h_\varphi},P_n^1\otimes P_n^2 | \X_n}$ associated with the bootstrap sample $\pa{\X_n^{*1},\ldots,\X_n^{*B_n}}$, that is 
$$\forall z\in\R,\quad 
F^{* B_n}_{n,\X_n}(z) = \frac{1}{B_n}\sum_{{\bf b}=1}^{B_n} \1{\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^{*{\bf b}}}\leq z}.$$

Then, using the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality (see \cite{vandervaart} p.268 for instance), we derive that for all $n\geq 2$, all  possible realisation $\bold{x}_n$ of $\X_n$, and for all $\varepsilon >0$, 
$$\proba{\sup_{z\in\R}\left|F^{* B_n}_{n,\bold{x}_n}(z) - F^*_{n,\bold{x}_n}(z)\right| > \varepsilon} \leq 2 e^{-2 B_n \varepsilon^2}.$$
Therefore, we obtain that
\begin{eqnarray*}
\proba{\sup_{z\in\R}\left|F^{* B_n}_{n,\X_n}(z) - F^*_{n,\X_n}(z)\right| > \varepsilon}
&=& \esp{\proba{\sup_{z\in\R}\left|F^{*,B_n}_{n,\X_n}(z) - F^*_{n,\X_n}(z)\right| > \varepsilon \middle| \X_n}} \\
&\leq &  2 e^{-2 B_n \varepsilon^2} \cv{n\to\infty}0,
\end{eqnarray*}
that is exactly
\begin{equation}
\label{cvProbaSupEmpVraieboot}
\sup_{z\in\R}\left|F^{* B_n}_{n,\X_n}(z) - F^*_{n,\X_n}(z)\right| \cvproba{n\to\infty} 0.
\end{equation}

Moreover, by \eqref{cvsupCondGauss},
$\sup_{z\in\R}\left|F^*_{n,\X_n}(z)-\Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right|\cv{n\to+\infty} 0\quad \textrm{$P$-a.s. in $(X_n)_n$}.$

Therefore, we obtain that 
\begin{equation}
\label{cvProbaSupEmpVraieboot}
\sup_{z\in\R}\left|F^{*B_n}_{n,\X_n}(z) - \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right| \cvproba{n\to\infty} 0.
\end{equation}


We finish the proof using the similar arguments as in \cite[Lemma 21.2]{vandervaart}, combined with a subsequence argument since the convergence in \eqref{cvProbaSupEmpVraieboot} occurs in probability, and not almost surely.
Let $\phi_0$ be an extraction. 
Then, by \eqref{cvProbaSupEmpVraieboot}, there exists an extraction $\phi_1$, and some $\Omega_0\subset\Omega$ such that $\proba{\Omega_0}= 1$, and for every $ \omega$ in $\Omega_0$, 
$$\sup_{z\in\R}\left|F^{*B_{\phi_1\circ\phi_0(n)}}_{{\phi_1\circ\phi_0(n)},\X_{\phi_1\circ\phi_0(n)}}(\omega)(z) - \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right| \cv{n\to\infty} 0.$$

From now on, fix $\omega$ in $\Omega_0$. Notice that this comes down to fix a realisation of $\X_n$, and a realisation of $\pa{\X_n^{*1},\ldots,\X_n^{*B_n}}$, and in particular, $F^{* B_n}_{n,\X_n}(\omega)$ is deterministic. 

Consider $V\sim\Nm$. Then, we obtain that $F^{*,B_{\phi_1\circ\phi_0(n)}}_{{\phi_1\circ\phi_0(n)},\X_{\phi_1\circ\phi_0(n)}}(\omega)(V) \cvps{n\to\infty} \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(V).$
In particular, for all $\eta$ in $ (0,1)$, 
\begin{eqnarray*}
\Phi_{0,1} \pa{\pa{F^{*B_{\phi_1\circ\phi_0(n)}}_{{\phi_1\circ\phi_0(n)},\X_{\phi_1\circ\phi_0(n)}}(\omega)}^{-1}(\eta)} &=& \proba{F^{*B_{\phi_1\circ\phi_0(n)}}_{{\phi_1\circ\phi_0(n)},\X_{\phi_1\circ\phi_0(n)}}(\omega)(V) < \eta} \\
&\cv{n\to\infty}& \proba{\Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(V)<\eta} \\
&=& \Phi_{0,1} \pa{\pa{\Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}}^{-1}(\eta)}.
\end{eqnarray*}
Finally, as $\Phi_{0,1}$ is a one-to-one function and $\Phi_{0,1}^{-1}$ is continuous, we obtain that 
$$\sqrt{\phi_1\circ\phi_0(n)}U^{* (\lceil \eta(B_{\phi_1\circ\phi_0(n)}) \rceil)} (\omega)= \pa{F^{*B_{\phi_1\circ\phi_0(n)}}_{{\phi_1\circ\phi_0(n)},\X_{\phi_1\circ\phi_0(n)}}(\omega)}^{-1}(\eta) 
\cv{n\to\infty} \Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta),$$
and this for all $\omega$ in $\Omega_0$, and any initial extraction $\phi_0$. Therefore, we obtain 
$$\sqrt{n}U^{* (\lceil \eta B_n \rceil)} \cvproba{n\to\infty}
\Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta),$$
and the proof is ended exactly as the proof of Theorem \ref{thniveauconsistance}.



% ------------------------------------------------------------ %
\subsection{Proof of Proposition \ref{exactdistribution}}
% ------------------------------------------------------------ %

As the sigma-algebra considered on $\pa{\calX^2}^{\otimes n}$ is generated by the family containing all products $\prod_{i=1}^n \pa{A_i^1\times A_i^2}$, with $A_i^1$ and $A_i^2$ Borel sets of $\pa{\calX,d_\calX}$, we just need to verify that for all $A=\prod_{i=1}^n \pa{A_i^1\times A_i^2}$ of this form, $\proba{\ds X_n^\perm \in A} = \proba{\ds X_n \in A}$, and $\proba{\ds X_n^\rperm \in A} = \proba{\ds X_n \in A}$. 
We first have that under $(H_0)$,
\begin{eqnarray*}
\proba{\ds X_n^\perm \in A}
%&=& \proba{\bigcap_{i=1}^n \cro{\ac{X_i^1\in A_i^1} \cap \ac{X_{\perm(i)}^2\in A_i^2}}}\\
&=& \proba{\bigcap_{i=1}^n \ac{X_i \in A_i^1 \times A_{\pi_n^{-1}(i)}^2}} \\
%\scriptstyle{\pa{X_i\textrm{ i.i.d. } \sim P^1\otimes P^2}\ \rightarrow}
%&=& \prod_{i=1}^n P\pa{A_i^1\times A_{\perm^{-1}(i)}^2} \\
%\scriptstyle{\pa{P=P^1\otimes P^2}\ \rightarrow}
&=& \prod_{i=1}^n\cro{ P^1\pa{A_i^1}P^2\pa{A_{\pi_n^{-1}(i)}^2} }\\
%&=& \prod_{i=1}^n P^1\pa{A_i^1}\prod_{j=1}^n P^2\pa{A_{\perm^{-1}(j)}^2} \\
&=& \prod_{i=1}^n \cro{P^1\pa{A_i^1}P^2\pa{A_i^2}} \\ 
%\scriptstyle{\pa{P=P^1\otimes P^2}\ \rightarrow}
%&=& \prod_{i=1}^n P\pa{A_i^1\times A_i^2} \\
%\scriptstyle{\pa{X_i\textrm{ i.i.d. }\sim P^1\otimes P^2}\ \rightarrow}
&=& \proba{\bigcap_{i=1}^n \ac{X_i \in A_i^1 \times A_i^2}} \\
&=& \proba{\ds X_n\in A}.
\end{eqnarray*}

Now consider a random permutation $\rperm$, uniformly distributed on $\pa{\Sn{n}}$, and independent of $\X_n$. 
Then 
\begin{eqnarray*}
\proba{\ds X_n^\rperm \in A}
&=& \sum_{\perm\in\Sn{n}} \proba{\ds X_n^\perm \in A}\proba{\rperm=\perm} \\
&=& \frac{1}{n!}\sum_{\perm\in\Sn{n}}\proba{\ds X_n\in A} \\
&=& \proba{\ds X_n\in A},
\end{eqnarray*}
which ends the proof.\\

% ------------------------------------------------------------ %
\subsection{Proof of Theorem \ref{consistencyperm}}
% ------------------------------------------------------------ %

% -------------------------------------------------- %
\paragraph{Sketch of proof.}
% -------------------------------------------------- %

Let $d_{BL}$ denote the bounded Lipschitz metric, which metrizes the weak convergence (see \cite[Prop. 11.3.2 and Th. 11.3.3]{Dudley} for instance), and defined by
$$d_{BL}(\mu,\nu) = \sup_{f\in BL,\ \norm{f}_{BL}\leq 1} \abs{\int_\R f\ (d\mu-d\nu)},$$
where $BL$ denotes the set of bounded Lipschitz function on \R, and 
$\norm{f}_{BL} = \norm{f}_\infty + \sup_{x\neq y}\frac{|f(x)-f(y)|}{|x-y|}$. 


In the following, when $Z_n$ is real-valued random variable depending on $\X_n$ and $\rperm$, we will denote by $\loi{ Z_n \middle |\X_n}$ the conditional distribution of $Z_n$ given $\X_n$ with respect to the Lebesgue measure.


\medskip

The first step of the proof consists of decomposing $\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm}$ in three terms:
$$\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm}=\frac{n}{n-1} \pa{M_n^\rperm\pa{\X_n}+\frac{R_n^\rperm\pa{\X_n}}{\sqrt{n}}-\frac{T_n\pa{\X_n}}{\sqrt{n}} },$$
where $M_n^\rperm\pa{\X_n}$ is associated with a martingale difference  array, and:
$$\esp{\frac{\left|R_n^\rperm\pa{\X_n}\right|}{\sqrt{n}} \middle| \X_n}\cvproba{n\to \infty} 0 \textrm{ and }  \frac{T_n\pa{\X_n}}{\sqrt{n}} \cvproba{n\to \infty} 0.$$
So,
$$d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n}, \loi{ \frac{n}{n-1}M_n^\rperm\pa{\X_n}\middle | \X_n }}\cvproba{n\to \infty} 0.$$

The second step will be to prove, from the Cramér-Wold device and the Central Limit Theorem for martingales, that
 $$d_{BL}\pa{\loi{M_n^\rperm\pa{\X_n}\middle| \X_n},\mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}}} \cvproba{n\to \infty} 0,$$
 so that 
$$d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle| \X_n}, \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvproba{n\to \infty} 0.$$

The third step will be to obtain the convergence:
$$\esp{\pa{\sqrt{n} U_{n,h_\varphi}\pa{\X_n^\rperm} }^2\middle |\X_n}\cvproba{n\to \infty} \sigma^2_{h_\varphi,P^1 \otimes P^2},$$
so that finally
$$d_{2}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle |\X_n}, \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvproba{n\to \infty} 0.$$

% -------------------------------------------------- %
\paragraph{First step: decomposition of $\sqrt{n} U_{n,h_\varphi}\pa{\X_n^\rperm} $.} $\ $\\
% -------------------------------------------------- %

It is obvious that by the definition \eqref{hphi} of $h_{\varphi}$, 
\begin{equation}\label{simpleU}
U_{n,h_\varphi}\pa{\X_n^\rperm} =\frac{1}{n-1} U_n^\rperm,
\end{equation}
 where
\begin{eqnarray*}
U_n^\rperm&=&\sum_{i=1}^n \varphi\pa{X_i^1,X_{\rperm(i)}^2}-\frac{1}{n}\sum_{i,j=1}^n  \varphi\pa{X_i^1,X_j^2}\\
&=&\sum_{i=1}^n \varphi\pa{X_i^1,X_{\rperm(i)}^2}-\frac{1}{n}\sum_{i,j=1}^n \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} -\frac{1}{n}\sum_{i,j=1}^n  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2} + \frac{1}{n}\sum_{i,j=1}^n \esp{\varphi\pa{X_i^1,X_j^2}}\\
&&-\frac{1}{n}\sum_{i,j=1}^n \pa{ \varphi\pa{X_i^1,X_j^2}- \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} -\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2} +   \esp{\varphi\pa{X_i^1,X_j^2}}}.
\end{eqnarray*}

On the one hand, if $\mathds{E}_P\cro{f}$ and $\mathds{E}_{P^1\otimes P^2}\cro{f}$ respectively denote $\esp{f\pa{X_i^1,X_i^2}}$, and $\esp{f\pa{X_i^1,X_j^2}}$ with $j\neq i$, for any integrable function $f$, then
\begin{eqnarray*}
 \frac{1}{n}\sum_{i,j=1}^n \esp{\varphi\pa{X_i^1,X_j^2}}&=& \sum_{i,j=1}^n \1{\rperm(i)=j}   \esp{\varphi\pa{X_i^1,X_j^2}}   - \sum_{i,j=1}^n \pa{\1{\rperm(i)=j}-\frac{1}{n}}\esp{\varphi\pa{X_i^1,X_j^2}}\\
 &=&\sum_{i,j=1}^n \1{\rperm(i)=j} \esp{\varphi\pa{X_i^1,X_j^2}}-\pa{\mathds{E}_P\cro{\varphi}-\mathds{E}_{P^1\otimes P^2}\cro{\varphi}} 
\sum_{i=1}^n \pa{\1{\rperm(i)=i}-\frac{1}{n}}.
\end{eqnarray*}
On the other hand,
\begin{eqnarray*}
 \frac{1}{n}\sum_{i,j=1}^n  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1}&=& \sum_{i,j=1}^n \1{\rperm(i)=j}  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1}- \sum_{i,j=1}^n \pa{\1{\rperm(i)=j}-\frac{1}{n}}  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1}\\
 &=&\sum_{i,j=1}^n \1{\rperm(i)=j}  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1}\\
 &&-\sum_{i=1}^n \pa{\1{\rperm(i)=i}-\frac{1}{n}}  \pa{ \esp{\varphi\pa{X_i^1,X_i^2}\middle| X_i^1}-\esp{\varphi\pa{X_i^1,X^2}\middle| X_i^1}} ,
 \end{eqnarray*}
 where $X=(X^1,X^2)$ is assumed to be $P$-distributed and independent of $(X_n)_n$, and in the same way,
 \begin{eqnarray*}
 \frac{1}{n}\sum_{i,j=1}^n  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2}&=&\sum_{i,j=1}^n \1{\rperm(i)=j}  \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2}\\
 &&-\sum_{j=1}^n \pa{\1{\rperm(j)=j}-\frac{1}{n}}  \pa{ \esp{\varphi\pa{X_j^1,X_j^2}\middle| X_j^2}-\esp{\varphi\pa{X^1,X_j^2}\middle| X_j^2}}.
 \end{eqnarray*}
 Therefore,
\begin{eqnarray*}
U_n^\rperm&=&\sum_{i,j=1}^n \1{\rperm(i)=j} \pa{\varphi\pa{X_i^1,X_j^2}-\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} - \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2}+  \esp{\varphi\pa{X_i^1,X_j^2}}}\\
&&+\sum_{i=1}^n \pa{\1{\rperm(i)=i}-\frac{1}{n}} \Bigg( \esp{\varphi\pa{X_i^1,X_i^2}\middle| X_i^1}+\esp{\varphi\pa{X_i^1,X_i^2}\middle| X_i^2}\\
&&-\esp{\varphi\pa{X_i^1,X^2}\middle| X_i^1}-\esp{\varphi\pa{X^1,X_i^2}\middle| X_i^2}-
\mathds{E}_P\cro{\varphi}+\mathds{E}_{P^1\otimes P^2}\cro{\varphi} \Bigg)\\
&&-\frac{1}{n}\sum_{i,j=1}^n \Big( \varphi\pa{X_i^1,X_j^2}- \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} -\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2} +   \esp{\varphi\pa{X_i^1,X_j^2}}\Big).
\end{eqnarray*}
As a consequence, 
\begin{equation}\label{decomp}
\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm}= \frac{n}{n-1} \pa{M_n^\rperm\pa{\X_n}+\frac{R_n^\rperm\pa{\X_n}}{\sqrt{n}}-\frac{T_n\pa{\X_n}}{\sqrt{n}}},
\end{equation}
with:
\begin{eqnarray*}
M_n^\rperm\pa{\X_n}&=&\frac{1}{\sqrt{n}}\sum_{i\neq j} \1{\rperm(i)=j} \pa{\varphi\pa{X_i^1,X_j^2}-\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} - \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2}+  \esp{\varphi\pa{X_i^1,X_j^2}}},\\
R_n^\rperm\pa{\X_n}&=&\sum_{i=1}^n \pa{\1{\rperm(i)=i}-\frac{1}{n}} \pa{\varphi\pa{X_i^1,X_i^2}- \esp{\varphi\pa{X_i^1,X^2}\middle| X_i^1}-\esp{\varphi\pa{X^1,X_i^2}\middle| X_i^2}+\mathds{E}_{P^1\otimes P^2}\cro{\varphi}},\\
T_n\pa{\X_n}&=&\frac{1}{n} \sum_{i\neq j} \pa{ \varphi\pa{X_i^1,X_j^2}- \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} -\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2} +   \esp{\varphi\pa{X_i^1,X_j^2}} }.
\end{eqnarray*}

Let us now prove that 
$$d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n}, \loi{ \frac{n}{n-1}M_n^\rperm\pa{\X_n}\middle | \X_n }}\cvproba{n\to \infty} 0.$$

To do this, we first notice that for every function $f$ in $BL$, and such that $\norm{f}_{BL} \leq 1$,
\begin{eqnarray*}
\left| \esp{f\pa{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm}} \middle| \X_n}  -\esp{f\pa{ \frac{n}{n-1} M_n^\rperm\pa{\X_n}}\middle| \X_n} \right|&\leq & \esp{\left| \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm}-\frac{n}{n-1} M_n^\rperm\pa{\X_n}\right| \middle| \X_n}\\
&\leq & \frac{n}{n-1} \pa{ \esp{\frac{\left|R_n^\rperm\pa{\X_n}\right|}{\sqrt{n}} \middle| \X_n}+ \frac{\left|T_n\pa{\X_n}\right|}{\sqrt{n}} },
\end{eqnarray*}
hence,
\begin{equation}\label{majdbl}
d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n}, \loi{ \frac{n}{n-1}M_n^\rperm\pa{\X_n}\middle | \X_n }}\leq \frac{n}{n-1} \pa{ \esp{\frac{\left|R_n^\rperm\pa{\X_n}\right|}{\sqrt{n}} \middle| \X_n}+ \frac{\left|T_n\pa{\X_n}\right|}{\sqrt{n}} }.
\end{equation}

Moreover, on the one hand, by Cauchy-Schwarz and Jensen's inequalities, and $\rperm$ is independent of $(X_n)_n$,
\begin{eqnarray*}
\esp{\pa{\esp{\frac{\left|R_n^\rperm\pa{\X_n}\right|}{\sqrt{n}} \middle| \X_n}}^2} &\leq& \frac{1}{n}\esp{\pa{R_n^\rperm\pa{\X_n}}^2}\\
&\leq &\frac{1}{n}\Bigg(\sum_{i,j=1}^n \esp{\pa{\1{\rperm(i)=i}-\frac{1}{n}}\pa{\1{\rperm(j)=j}-\frac{1}{n}}} \times\\
&&\mathds{E}\Big[ \Big(\varphi\pa{X_i^1,X_i^2}- \esp{\varphi\pa{X_i^1,X^2}\middle| X_i^1}-\esp{\varphi\pa{X^1,X_i^2}\middle| X_i^2}+\mathds{E}_{P^1\otimes P^2}\cro{\varphi}\Big)\times\\
&& \Big(\varphi\pa{X_j^1,X_j^2}- \esp{\varphi\pa{X_j^1,X^2}\middle| X_j^1}-\esp{\varphi\pa{X^1,X_j^2}\middle| X_j^2}+\mathds{E}_{P^1\otimes P^2}\cro{\varphi}\Big)\Big]\Bigg)\\
&\leq& \frac{C}{n}\pa{\esps{P}{\varphi^2}+\esps{P^1 \otimes P^2}{\varphi^2}}\sum_{i,j=1}^n \pa{\esp{\1{\rperm(i)=i}\1{\rperm(j)=j}}-\frac{1}{n^2}}\\
&\leq&  \frac{C}{n}\pa{\esps{P}{\varphi^2}+\esps{P^1 \otimes P^2}{\varphi^2}}\pa{\sum_{i=1}^n \pa{\frac{1}{n}-\frac{1}{n^2}}+\sum_{i\neq j} \pa{\frac{1}{n(n-1)}-\frac{1}{n^2}}}\\
&\leq & \frac{C}{n}\pa{\esps{P}{\varphi^2}+\esps{P^1 \otimes P^2}{\varphi^2}}\cv{n\to \infty} 0.\\
\end{eqnarray*}
Therefore, from Markov's inequality, we deduce that 
 $$\esp{\frac{\left|R_n^\rperm\pa{\X_n}\right|}{\sqrt{n}} \middle| \X_n}\cvproba{n\to \infty} 0.$$
On the other hand, denoting by $C_{i,j}$ the centred random variable
$$C_{i,j}=\varphi\pa{X_i^1,X_j^2}- \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} -\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2} +   \esp{\varphi\pa{X_i^1,X_j^2}},$$ then
$$T_n\pa{\X_n}=\frac{1}{n}\sum_{i\neq j} C_{i,j}.$$
Hence,
$$\esp{\pa{\frac{T_n\pa{\X_n}}{\sqrt{n}} }^2}= \frac{1}{n^3}\sum_{i\neq j}\sum_{k\neq l}\esp{C_{i,j} C_{k,l}}.$$
Notice that for $i\neq j$, $\esp{C_{i,j} \middle|X_i}=\esp{C_{i,j} \middle|X_j}=0$.

If $\# \ac{i,j,k,l}=4$, then $\esp{C_{i,j} C_{k,l}}=\pa{\esp{C_{i,j}}}^2=0$.

If $i,j,l$ are all different,  then 
\begin{eqnarray*}
\esp{C_{i,j}C _{i,l}}&=&\esp{\esp{C_{i,j}C_{i,l} \middle| X_i,X_l}}\\
&=&\esp{\esp{C_{i,j} \middle| X_i}C_{i,l}}\\
&=&0.
\end{eqnarray*}

In the same way, for $i,j,k$ all different, then $\esp{C_{i,j} C_{k,i}}=0$.

For $i\neq j$, then 
\begin{equation}\label{sig}
\esp{C_{i,j}^2}=\sigma^{2}_{h_\varphi,P^1\otimes P^2},\textrm{ and } \esp{C_{i,j}C_{j,i}}\leq \sigma^{2}_{h_\varphi,P^1\otimes P^2},
\end{equation}
by the Cauchy-Schwarz inequality.

Combining the above computations, we obtain that
$$\esp{\pa{\frac{T_n\pa{\X_n}}{\sqrt{n}} }^2}\leq 2\frac{n(n-1)}{n^3} \sigma^{2}_{h_\varphi,P^1\otimes P^2}\cv{n\to \infty} 0,$$
therefore, 
 $$\frac{T_n\pa{\X_n}}{\sqrt{n}} \cvproba{n\to \infty} 0.$$

Finally, from \eqref{majdbl}, we derive that:

\begin{equation}\label{dbl}
d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n}, \loi{ \frac{n}{n-1}M_n^\rperm\pa{\X_n}\middle | \X_n }}\cvproba{n\to \infty} 0.
\end{equation}

\paragraph{Second step: asymptotic normality of $M_n^\rperm\pa{\X_n}$ given $\X_n$, in probability.} $\ $\\

Recall that 
$$M_n^\rperm\pa{\X_n}=\frac{1}{\sqrt{n}}\sum_{i\neq j} \1{\rperm(i)=j} \pa{\varphi\pa{X_i^1,X_j^2}-\esp{\varphi\pa{X_i^1,X_j^2}\middle| X_i^1} - \esp{\varphi\pa{X_i^1,X_j^2}\middle| X_j^2}+  \esp{\varphi\pa{X_i^1,X_j^2}}},$$
that is, with the above notation,
$$M_n^\rperm\pa{\X_n}=\frac{1}{\sqrt{n}}\sum_{i\neq j} \1{\rperm(i)=j} C_{i,j}=\frac{1}{\sqrt{n}}\sum_{i=2}^n\sum_{j=1}^{i-1} \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }.$$

Let ${\Pi_n'}$ be another uniformly distributed random permutation with values in $\Sn{n}$, independent of $\rperm$ and $\X_n$, 
and 
$$M_n^{{\Pi_n'}} \pa{\X_n}=\frac{1}{\sqrt{n}}\sum_{i\neq j} \1{{\Pi_n'}(i)=j} C_{i,j}=\frac{1}{\sqrt{n}}\sum_{i=2}^n\sum_{j=1}^{i-1} \pa{\1{{\Pi_n'}(i)=j} C_{i,j}+\1{{\Pi_n'}(j)=i} C_{j,i} }.$$

\begin{lm}\label{lm1}
Considering the above notation, 
$$\loi{\pa{M_n^\rperm\pa{\X_n},M_n^{{\Pi_n'}} \pa{\X_n}}'}\Rightarrow  \mc{N}_2\pa{0, \pa{\begin{array}{cc}\sigma^{2}_{h_\varphi,P^1\otimes P^2} & 0\\
0 & \sigma^{2}_{h_\varphi,P^1\otimes P^2} \end{array}}},$$ 
where $\mc{N}_2\pa{M,V}$ denotes the $2$-dimensional Gaussian distribution with expectation $M$ and variance-covariance matrix $V$.
\end{lm}

\begin{proof}

According to the Cramér-Wold device, given $a,b$ in $\R$, we aim at proving that  
$$\loi{a M_n^\rperm\pa{\X_n}+b M_n^{{\Pi_n'}} \pa{\X_n}}\Rightarrow  \mc{N}\pa{0, \pa{a^2+b^2}\sigma^{2}_{h_\varphi,P^1\otimes P^2}}.$$ 

In order to deal with simpler mathematical expressions, we introduce below a few notations more.

\begin{itemize}
\item For any fixed integer $n\geq 2$, $\mc{F}_{n,k}$ denotes the filtration $\sigma\pa{\rperm,{\Pi_n'},X_1,X_2,\ldots,X_k}$.
\item Let
 $$Y_{n,i}= \frac{1}{\sqrt{n}}\sum_{j=1}^{i-1} \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} },$$ 
 $$Y_{n,i}'=\frac{1}{\sqrt{n}}\sum_{j=1}^{i-1} \pa{\1{{\Pi_n'}(i)=j} C_{i,j}+\1{{\Pi_n'}(j)=i} C_{j,i} },$$ $M_{n,k}=\sum_{i=2}^k Y_{n,i}$ and $M_{n,k}'=\sum_{i=2}^k Y_{n,i}'$, so that $M_n^\rperm\pa{\X_n}=M_{n,n}$ and $M_n^{{\Pi_n'}}\pa{\X_n}=M_{n,n}'$.
 \end{itemize}
 
Let us first prove that for a fixed integer $n\geq 2$, $\pa{aM_{n,k}+bM_{n,k}',\mc{F}_{n,k}}_{2\leq k\leq n}$ is a martingale difference array.


Note that for $2\leq i\leq n$, 
\begin{eqnarray*}
\esp{Y_{n,i} \middle| \mc{F}_{n,i-1}}&=&\frac{1}{\sqrt{n}} \sum_{j=1}^{i-1} \esp{ \1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i}\middle| \mc{F}_{n,i-1}}\\
&=& \frac{1}{\sqrt{n}} \sum_{j=1}^{i-1}\pa{ \1{\rperm(i)=j} \esp{C_{i,j} \middle|X_{j}}+\1{\rperm(j)=i} \esp{C_{j,i} \middle|X_{j}}}\\
&=&0.\\
\end{eqnarray*}
In the same way, we have that $\esp{Y_{n,i}' \middle| \mc{F}_{n,i-1}}=0$, so
$$\esp{aM_{n,k}+bM_{n,k}' \middle| \mc{F}_{n,k-1}}=aM_{n,k_1}+bM'_{n,k-1}.$$

We now use the following result which is commonly attributed to Brown \cite{Brown}.

\begin{thm}
\label{diffmart}
Let $(X_{n,k})_{k\in\{1,...,p_n\},n\in \mathds{N}^* }$ be a martingale difference array, i.e. such that
there exists an array of $\sigma$-algebra $(\mathcal{F}_{n,k})_{k\in\{1,...,p_n\},n\in \mathds{N}^* }$ that is increasing w.r.t. $k$ such that for all $k=1,...,p_n$,
$$ \esp{X_{n,k}|\mathcal{F}_{n,k-1}}=0. $$
Let $A_n= \sum_{k=1}^{p_n} \esp{X_{n,k}^2|\mathcal{F}_{n,k-1}}$.
Assume that
\begin{itemize}
\item $A_n\cvproba{n\to\infty} \sigma^2>0$. 
\item For all $\varepsilon>0$, $\sum_{k=1}^{p_n} \esp{X_{n,k}^2 \1{|X_{n,k}|>\varepsilon}}\to_{n\to\infty} 0$
\end{itemize}
Then $Z_n=\sum_{k=1}^{p_n} X_{n,k}$  converges in distribution towards $\mathcal{N}(0,\sigma^2)$.
\end{thm}



From  Theorem \ref{diffmart}, we deduce that if:
\begin{eqnarray*}
(i)&& \sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2 \middle| \mc{F}_{n,i-1}} \cvproba{n\to \infty} (a^2+b^2) \sigma^{2}_{h_\varphi,P^1\otimes P^2},\\
(ii)&&\sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2\1{\left| aY_{n,i}+b Y_{n,i}'\right|>\varepsilon}}\to 0 \textrm{ for any $\varepsilon>0$},\end{eqnarray*}
then
$$\loi{a M_n^\rperm\pa{\X_n}+b M_n^{{\Pi_n'}} \pa{\X_n}}\Rightarrow  \mc{N}\pa{0, \pa{a^2+b^2}\sigma^{2}_{h_\varphi,P^1\otimes P^2}}.$$ 

Let us now check that both $(i)$ and $(ii)$ are satisfied.\\


{\bf Assumption $(i)$}.  Noticing that 
\begin{equation}\label{decompab2}
 \sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2 \middle| \mc{F}_{n,i-1}} =(a^2+b^2)\sum_{i=2}^n \esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}} +2ab \sum_{i=2}^n\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}},
\end{equation}
the proof of $(i)$ can be decomposed into two points.

The first point consists of deriving that
$$\sum_{i=2}^n \esp{Y_{n,i}^2}\cv{n\to \infty} \sigma^2_{h_\varphi,P^1\otimes P^2},\quad \textrm{and}\quad
\var{\sum_{i=2}^n \esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}}\cv{n\to \infty}0,$$
which leads, thanks to Chebychev's inequality, to
$$\sum_{i=2}^n \esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}\cvproba{n\to\infty} \sigma^2_{h_\varphi,P^1\otimes P^2}.$$

The second point consists of proving that
$$\esp{\pa{\sum_{i=2}^n\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}}}^2} \cv{n\to \infty} 0,$$
so
$$\sum_{i=2}^n\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}}\cvproba{n\to\infty} 0.$$

{\bf First point}.\\

On the one hand,
$$\sum_{i=2}^{n} \esp{Y_{n,i}^2}=\frac{1}{n} \sum_{i=2}^{n} \sum_{j,k=1}^{i-1} \esp{ \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }\pa{\1{\rperm(i)=k} C_{i,k}+\1{\rperm(k)=i} C_{k,i} }}.$$

Furthermore, if $1\leq j\neq k \leq i-1$, 
\begin{eqnarray*}
&&\esp{\pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }\pa{\1{\rperm(i)=k} C_{i,k}+\1{\rperm(k)=i} C_{k,i} }}\\
&&=\esp{\esp{ \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }\pa{\1{\rperm(i)=k} C_{i,k}+\1{\rperm(k)=i} C_{k,i} }\middle| X_i,X_j,\rperm}}\\
&&=\esp{\pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }\pa{\1{\rperm(i)=k} \esp{C_{i,k}\middle |X_i} +\1{\rperm(k)=i} \esp{C_{k,i} \middle |X_i}}}\\
&&=0.
\end{eqnarray*}


Thus,
\begin{eqnarray*}
\sum_{i=2}^{n} \esp{Y_{n,i}^2}&=&\frac{1}{n} \sum_{i=2}^{n} \sum_{j=1}^{i-1}  \esp{ \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }^2}\\
&=&\frac{1}{n} \sum_{i=2}^{n} \sum_{j=1}^{i-1}   \esp{\1{\rperm(i)=j} C_{i,j}^2+\1{\rperm(j)=i} C_{j,i}^2 +2\1{\rperm(i)=j} \1{\rperm(j)=i} C_{i,j}C_{j,i}}\\
&=&\frac{1}{n} \sum_{i=2}^{n} \sum_{j=1}^{i-1}\pa{ \frac{2}{n}\esp{C_{i,j}^2} +\frac{2}{n(n-1) }\esp{C_{i,j}C_{j,i}}}\\
&=& \frac{2}{n^2} \sum_{i=2}^{n} (i-1)\pa{ \esp{C_{1,2}^2} +\frac{1}{n-1 }\esp{C_{1,2}C_{2,1}}}\\
\end{eqnarray*}
so
\begin{equation}\label{espAn}
\sum_{i=2}^{n} \esp{Y_{n,i}^2}=\frac{n-1}{n}\esp{C_{1,2}^2}+\frac{1}{n} \esp{C_{1,2}C_{2,1}}.
\end{equation}
From \eqref{sig}, we finally derive that
\begin{equation}\label{part1}
\sum_{i=2}^{n} \esp{Y_{n,i}^2}\cv{n\to \infty} \sigma^2_{h_\varphi,P^1\otimes P^2}.
\end{equation}

On the other hand, we have that:
\begin{eqnarray*}
\esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}&=&\frac{1}{n}\sum_{j,k=1}^{i-1}\esp{\pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i}}\pa{\1{\rperm(i)=k} C_{i,k}+\1{\rperm(k)=i} C_{k,i}}\middle| \mc{F}_{n,i-1}}\\
&=&\frac{1}{n} \sum_{j=1}^{i-1} \esp{\1{\rperm(i)=j} C_{i,j}^2+2\1{\rperm(i)=j}\1{\rperm(j)=i} C_{i,j}C_{j,i}+\1{\rperm(j)=i} C_{j,i}^2 \middle| \mc{F}_{n,i-1}}\\
&&+\frac{1}{n}\sum_{1\leq j\neq k \leq i-1}\esp{\1{\rperm(i)=j}\1{\rperm(k)=i} C_{i,j}C_{k,i}+\1{\rperm(j)=i}\1{\rperm(i)=k} C_{j,i}C_{i,k}\middle| \mc{F}_{n,i-1}}\\
&=&\frac{1}{n} \sum_{j=1}^{i-1} \1{\rperm(i)=j} \esp{C_{i,j}^2\middle| X_j}+\frac{1}{n} \sum_{j=1}^{i-1} \1{\rperm(j)=i} \esp{C_{j,i}^2\middle| X_j}\\
&&+\frac{2}{n} \sum_{j=1}^{i-1} \1{\rperm(i)=j} \1{\rperm(j)=i}  \esp{C_{i,j}C_{j,i}\middle| X_j}\\
&& +\frac{2}{n} \sum_{1\leq j\neq k \leq i-1} \1{\rperm(i)=j} \1{\rperm(k)=i}  \esp{C_{i,j}C_{k,i}\middle| X_j,X_k}.
\end{eqnarray*}

Then, 
$$\sum_{i=2}^n \pa{\esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}-\esp{Y_{n,i}^2}}=A_{n,1}+A_{n,2}+2A_{n,3}+2A_{n,4},$$
with
\begin{eqnarray*}
A_{n,1}&=&\frac{1}{n}\sum_{1\leq j<i\leq n} \pa{ \1{\rperm(i)=j} \esp{C_{i,j}^2\middle| X_j} -\frac{1}{n}\esp{C_{i,j}^2}},\\
A_{n,2}&=&\frac{1}{n}\sum_{1\leq j<i\leq n} \pa{ \1{\rperm(j)=i} \esp{C_{j,i}^2\middle| X_j} -\frac{1}{n}\esp{C_{i,j}^2}},\\
A_{n,3}&=&\frac{1}{n}\sum_{1\leq j<i\leq n} \pa{ \1{\rperm(i)=j}\1{\rperm(j)=i} \esp{C_{i,j}C_{j,i}\middle| X_j} -\frac{1}{n(n-1)}\esp{C_{i,j}C_{j,i}}},\\
A_{n,4}&=&\frac{1}{n}\sum_{1\leq j\neq k<i\leq n} \pa{ \1{\rperm(i)=j}\1{\rperm(k)=i} \esp{C_{i,j}C_{k,i}\middle| X_j,X_k}}.
\end{eqnarray*}
Thus, 
\begin{eqnarray}\label{controlA}
\nonumber \var{\sum_{i=2}^n \pa{\esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}}}&=&\esp{\pa{A_{n,1}+A_{n,2}+2A_{n,3}+2A_{n,4}}^2}\\
&\leq& 4\pa{\esp{A_{n,1}^2}+\esp{A_{n,2}^2}+4\esp{A_{n,3}^2}+4\esp{A_{n,4}^2}}.
\end{eqnarray}
Let us now control each term of the above right-hand side.\\

{\bf Convergence of $\esp{A_{n,1}^2}$ and $\esp{A_{n,2}^2}$}.
\begin{eqnarray*}
\esp{A_{n,1}^2}&=&\frac{1}{n^2} \sum_{1\leq j<i\leq n} \sum_{1\leq l<k\leq n} \esp{\pa{ \1{\rperm(i)=j} \esp{C_{i,j}^2\middle| X_j} -\frac{1}{n}\esp{C_{i,j}^2}}\pa{\1{\rperm(k)=l} \esp{C_{k,l}^2\middle| X_l} -\frac{1}{n}\esp{C_{k,l}^2}}}\\
&=&\frac{1}{n^2} \sum_{1\leq j<i\leq n} \sum_{1\leq l<k\leq n} \pa{\esp{ \1{\rperm(i)=j}\1{\rperm(k)=l}}\esp{\esp{C_{i,j}^2\middle| X_j}\esp{C_{k,l}^2\middle| X_l} }-\frac{1}{n^2}\pa{\esp{C_{k,l}^2}}^2}.
\end{eqnarray*}
Let us now consider all the cases where $1\leq j<i\leq n$, and $1\leq l<k\leq n$.

If $i=k$ and $j=l$, then 
$$\esp{ \1{\rperm(i)=j}\1{\rperm(k)=l}}\esp{\esp{C_{i,j}^2\middle| X_j}\esp{C_{k,l}^2\middle| X_l}}=\frac{1}{n}\esp{\pa{\esp{C_{i,j}^2\middle| X_j}}^2}.$$
If $i=k$ and $j\neq l$, or if $i\neq k$ and $j=l$, then
$$\esp{ \1{\rperm(i)=j}\1{\rperm(k)=l}}\esp{\esp{C_{i,j}^2\middle| X_j}\esp{C_{k,l}^2\middle| X_l}}=0.$$
If $i\neq k$ and $j\neq l$, then 
$$\esp{ \1{\rperm(i)=j}\1{\rperm(k)=l}}\esp{\esp{C_{i,j}^2\middle| X_j}\esp{C_{k,l}^2\middle| X_l}}=\frac{1}{n(n-1)}\pa{\esp{C_{i,j}^2}}^2.$$
By combining these results, from \eqref{sig} and under the assumption $\pa{\mc{A}_{\varphi,Mmt}}$, we obtain that:
$$\esp{A_{n,1}^2}\leq \frac{n-1}{2n^2}\pa{\esp{\pa{\esp{C_{i,j}^2\middle| X_j}}^2}-\frac{\sigma^4_{h_\varphi,P^1\otimes P^2}}{n}}+Cn^2 \pa{\frac{1}{n(n-1)}-\frac{1}{n^2}}\sigma^4_{h_\varphi,P^1\otimes P^2}\cv{n\to \infty} 0.$$
One can prove in the same way that 
$$\esp{A_{n,2}^2}\cv{n\to \infty} 0.$$
{\bf Convergence of $\esp{A_{n,3}^2}$}.
\begin{eqnarray*}
\esp{A_{n,3}^2}&=&\frac{1}{n^2}\sum_{1\leq j<i\leq n} \sum_{1\leq l<k\leq n}\mathds{E}\Bigg[\pa{ \1{\rperm(i)=j}\1{\rperm(j)=i} \esp{C_{i,j}C_{j,i}\middle| X_j} -\frac{1}{n(n-1)}\esp{C_{i,j}C_{j,i}}} \times\\
&& \pa{ \1{\rperm(k)=l}\1{\rperm(l)=k} \esp{C_{k,l}C_{l,k}\middle| X_l} -\frac{1}{n(n-1)}\esp{C_{k,l}C_{l,k}}}\Bigg]\\
&=&\frac{1}{n^2}\sum_{1\leq j<i\leq n} \sum_{1\leq l<k\leq n}\Bigg(\esp{\1{\rperm(i)=j}\1{\rperm(j)=i}\1{\rperm(k)=l}\1{\rperm(l)=k}}\esp{\esp{C_{i,j}C_{j,i}\middle |X_j} \esp{C_{k,l}C_{l,k}\middle |X_j} }\\
&& -\frac{1}{n^2(n-1)^2} \pa{\esp{C_{i,j}C_{j,i}}}^2\Bigg)\\
&=&\frac{1}{n^2}\sum_{1\leq j<i\leq n} \sum_{1\leq l<k\leq n}\esp{\1{\rperm(i)=j}\1{\rperm(j)=i}\1{\rperm(k)=l}\1{\rperm(l)=k}}\esp{\esp{C_{i,j}C_{j,i}\middle |X_j} \esp{C_{k,l}C_{l,k}\middle |X_j} }\\
&&-\frac{1}{4n^2} \pa{\esp{C_{1,2}C_{2,1}}}^2.
\end{eqnarray*}
Let us again consider all the cases where $1\leq j<i\leq n$, and $1\leq l<k\leq n$.

If $i=k$ and $j=l$, then 
$$\esp{\1{\rperm(i)=j}\1{\rperm(j)=i}\1{\rperm(k)=l}\1{\rperm(l)=k}}\esp{\esp{C_{i,j}C_{j,i}\middle |X_j} \esp{C_{k,l}C_{l,k}\middle |X_j} }=\frac{1}{n(n-1)} \esp{\pa{\esp{C_{i,j}C_{j,i}\middle| X_j}}^2}.$$
If $i=k$ and $j\neq l$, or if $i\neq k$ and $j=l$, then
$$\esp{\1{\rperm(i)=j}\1{\rperm(j)=i}\1{\rperm(k)=l}\1{\rperm(l)=k}}\esp{\esp{C_{i,j}C_{j,i}\middle |X_j} \esp{C_{k,l}C_{l,k}\middle |X_j} }=0.$$
If $i\neq k$ and $j\neq l$, then 
$$\esp{\1{\rperm(i)=j}\1{\rperm(j)=i}\1{\rperm(k)=l}\1{\rperm(l)=k}}\esp{\esp{C_{i,j}C_{j,i}\middle |X_j} \esp{C_{k,l}C_{l,k}\middle |X_j} }=\frac{\pa{\esp{C_{1,2}C_{2,1}}}^2}{n(n-1)(n-2)(n-3)}.$$
Thus, under $\pa{\mc{A}_{\varphi,Mmt}}$, we finally have that:
$$\esp{A_{n,3}^2}\leq \frac{1}{2n^2}\esp{\pa{\esp{C_{1,2}C_{2,1}\middle| X_1}}^2}+C\frac{n\pa{\esp{C_{1,2}C_{2,1}}}^2}{(n-1)(n-2)(n-3)}\cv{n\to \infty} 0.$$

{\bf Convergence of $\esp{A_{n,4}^2}$}.

\begin{multline*}
\esp{A_{n,4}^2}=\frac{1}{n^2}\sum_{1\leq j\neq k<i\leq n} \sum_{1\leq p\neq q<l \leq n}\esp{\1{\rperm(i)=j}\1{\rperm(k)=i}\1{\rperm(l)=p}\1{\rperm(q)=l}}\times\\
 \esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}.
 \end{multline*}

Let us now consider all the cases where $1\leq j\neq k<i\leq n$, and $1\leq p\neq q<l \leq n$.

If $\#\{j,k,p,q\}\geq 3$, there exists at least one element in $\{j,k,p,q\}$, $j$ for instance (the other cases are studied in the same way), which differs from the other ones. Then,
\begin{eqnarray*}
\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}&=&\esp{\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}\middle| X_k,X_p,X_q}}\\
&=&\esp{\esp{C_{i,j}C_{k,i}\middle| X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}\\
&=&\esp{\esp{\esp{C_{i,j}C_{k,i}\middle|X_i,X_k} \middle| X_k}\esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}\\
&=&\esp{\esp{C_{k,i}\esp{C_{i,j}\middle|X_i} \middle| X_k}\esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}.
\end{eqnarray*}
Since $\esp{C_{i,j}\middle|X_i}=0$, this leads to:
\begin{equation}\label{eqstar}
\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}=0.
\end{equation}



If $j=p$, $k=q$, and $i=l$, then,

$$\esp{\1{\rperm(i)=j}\1{\rperm(k)=i}\1{\rperm(l)=p}\1{\rperm(q)=l}}=\frac{1}{n(n-1)},$$
and
\begin{eqnarray*}
\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}&=&\esp{\pa{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k}}^2}\\
&=&\esp{\pa{\esp{C_{3,1}C_{2,3}\middle| X_1,X_2}}^2}\\
&<&+\infty \textrm{ \quad under $\pa{\mc{A}_{\varphi,Mmt}}$}.
\end{eqnarray*}


If $j=p$, $k=q$, and $i\neq l$, then $\1{\rperm(k)=i}\1{\rperm(q)=l}=0$, so
$$\esp{\1{\rperm(i)=j}\1{\rperm(k)=i}\1{\rperm(l)=p}\1{\rperm(q)=l}}=0.$$

If $j=q$, $k=p$, and $i=l$, then $\1{\rperm(i)=j}\1{\rperm(l)=p}=0$, so
$$\esp{\1{\rperm(i)=j}\1{\rperm(k)=i}\1{\rperm(l)=p}\1{\rperm(q)=l}}=0.$$

If $j=q$, $k=p$, and $i\neq l$, then 
$$\esp{\1{\rperm(i)=j}\1{\rperm(k)=i}\1{\rperm(l)=p}\1{\rperm(q)=l}}=\frac{(n-4)!}{n !},$$
and
\begin{eqnarray*}
\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,p}C_{q,l}\middle| X_p,X_q}}&=&\esp{\esp{C_{i,j}C_{k,i}\middle| X_j,X_k} \esp{C_{l,k}C_{j,l}\middle| X_j,X_k}}\\
&=&\esp{\esp{C_{i,j}C_{k,i}C_{l,k}C_{j,l}\middle| X_j,X_k}}\\
&=& \esp{C_{1,4}C_{4,2}C_{3,1}C_{2,3}}\\
&<&+\infty \textrm{ \quad under $\pa{\mc{A}_{\varphi,Mmt}}$}.
\end{eqnarray*}

By combining these results, we obtain that:

$$\esp{A_{n,4}^2}\leq C\frac{n^3}{n^2} \frac{\esp{\pa{\esp{C_{3,1}C_{2,3}\middle| X_1,X_2}}^2}}{n(n-1)}+
C'\frac{n^4}{n^2} \frac{(n-4)!}{n !}\esp{C_{1,4}C_{4,2}C_{3,1}C_{2,3}} \cv{n\to \infty} 0.$$

From \eqref{controlA}, and the above results of convergence towards $0$ for $\esp{A_{n,1}^2}$, $\esp{A_{n,2}^2}$, $\esp{A_{n,3}^2}$, and $\esp{A_{n,4}^2}$, we firstly derive that:

$$\var{\sum_{i=2}^n \pa{\esp{Y_{n,i}^2\middle| \mc{F}_{n,i-1}}}}\cv{n\to \infty} 0.$$

\newpage

{\bf Second point}.\\

Notice that
\begin{eqnarray*}
\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}}&=&\frac{1}{n}\sum_{j,k=1}^{i-1} \esp{ \pa{\1{\rperm(i)=j} C_{i,j}+\1{\rperm(j)=i} C_{j,i} }\pa{\1{{\Pi_n'}(i)=k} C_{i,k}+\1{{\Pi_n'}(k)=i} C_{k,i} }\middle| \mc{F}_{n,i-1}}\\
&=&B_{n,1}+B_{n,2}+B_{n,3}+B_{n,4},
\end{eqnarray*}
with
\begin{eqnarray*}
B_{n,1}&=&\frac{1}{n} \sum_{1\leq j<i\leq n}\pa{\1{\rperm(i)=j} \1{{\Pi_n'}(i)=j} \esp{ C_{i,j}^2\middle| X_j} },\\
B_{n,2}&=&\frac{1}{n} \sum_{1\leq j<i\leq n} \pa{ \1{\rperm(j)=i} \1{{\Pi_n'}(j)=i} \esp{C_{j,i}^2\middle|X_j}},\\
B_{n,3}&=&\frac{1}{n} \sum_{1\leq j<i\leq n}\pa{\pa{ \1{\rperm(i)=j} \1{{\Pi_n'}(j)=i} +\1{\rperm(j)=i} \1{{\Pi_n'}(i)=j}}\esp{ C_{i,j} C_{j,i}\middle|X_j}},\\
B_{n,4}&=&\frac{1}{n} \sum_{1\leq j\neq k<i\leq n}  \Bigg(\1{\rperm(i)=j} \1{{\Pi_n'}(i)=k} \esp{ C_{i,j}C_{i,k}\middle| X_j,X_k}+\1{\rperm(i)=j} \1{{\Pi_n'}(k)=i} \esp{ C_{i,j} C_{k,i}\middle|X_j,X_k}\\
&&+\1{\rperm(j)=i} \1{{\Pi_n'}(i)=k} \esp{ C_{j,i}C_{i,k}\middle| X_j,X_k}+\1{\rperm(j)=i} \1{{\Pi_n'}(k)=i} \esp{C_{j,i}C_{k,i} \middle| X_j,X_k}\Bigg).
\end{eqnarray*}

Thus,
\begin{equation}\label{controlB}
\esp{\pa{\sum_{i=2}^n\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}}}^2} \leq 4 \pa{\esp{B_{n,1}^2}+\esp{B_{n,2}^2}+\esp{B_{n,3}^2}+\esp{B_{n,4}^2}}.
\end{equation}

{\bf Convergence of $\esp{B_{n,1}^2}$ and $\esp{B_{n,2}^2}$.} 
\begin{eqnarray*}
\esp{B_{n,1}^2}&=&\frac{1}{n^2} \sum_{1\leq j<i\leq n}\sum_{1\leq l<k\leq n}\mathds{E}\Bigg[\pa{\1{\rperm(i)=j} \1{{\Pi_n'}(i)=j} \esp{ C_{i,j}^2\middle| X_j}}\pa{\1{\rperm(k)=l} \1{{\Pi_n'}(k)=l} \esp{ C_{k,l}^2\middle| X_l} }\Bigg]\\
&=&\frac{1}{n^2} \sum_{1\leq j<i\leq n}\sum_{1\leq l<k\leq n} \esp{\1{\rperm(i)=j} \1{\rperm(k)=l}}\esp{\1{{\Pi_n'}(i)=j} \1{{\Pi_n'}(k)=l}}\esp{\esp{ C_{i,j}^2\middle| X_j} \esp{ C_{k,l}^2\middle| X_l} }\\
&\leq& \frac{1}{n^3} \sum_{1\leq j<i\leq n}\sum_{1\leq l<k\leq n} \esp{\1{\rperm(i)=j} \1{\rperm(k)=l}}\esp{\esp{ C_{i,j}^2\middle| X_j} \esp{ C_{k,l}^2\middle| X_l} }.
\end{eqnarray*}

Then, with the same computations as for the convergence of $ \esp{A_{n,1}^2}$ above, we prove that:

$$\esp{B_{n,1}^2}\leq \frac{n-1}{2n^3}\esp{\pa{\esp{C_{1,2}^2\middle| X_2}}^2}+C\frac{\sigma^4_{h_\varphi,P^1\otimes P^2}}{n-1}\cv{n\to\infty}0$$

In the same way, we also prove that:

$$\esp{B_{n,2}^2}\cv{n\to\infty}0$$

{\bf Convergence of $\esp{B_{n,3}^2}$.} 
\begin{eqnarray*}
\esp{B_{n,3}^2}&\leq &\frac{4}{n^2} \sum_{1\leq j<i\leq n}\sum_{1\leq l<k\leq n} \mathds{E}\Bigg[\pa{ \1{\rperm(i)=j} \1{{\Pi_n'}(j)=i}\esp{ C_{i,j} C_{j,i}\middle|X_j} }\pa{ \1{\rperm(k)=l} \1{{\Pi_n'}(l)=k}\esp{ C_{k,l} C_{l,k}\middle|X_l}}\Bigg]\\
&\leq &\frac{4}{n^2} \sum_{1\leq j<i\leq n}\sum_{1\leq l<k\leq n} \esp{\1{\rperm(i)=j} \1{\rperm(k)=l}}\esp{\1{{\Pi_n'}(j)=i}\1{{\Pi_n'}(l)=k}} \esp{\esp{ C_{i,j} C_{j,i}\middle|X_j}\esp{ C_{k,l} C_{l,k}\middle|X_l}}.\\
\end{eqnarray*}

Now, with similar computations as for the convergence of $ \esp{A_{n,1}^2}$ above again, we prove that:
$$\esp{B_{n,3}^2}\leq 2\frac{n-1}{n^3}\esp{\pa{\esp{C_{1,2}C_{2,1}\middle| X_2}}^2}+C\frac{ \pa{\esp{C_{1,2}C_{2,1}}}^2}{n-1}\cv{n\to\infty}0.$$

{\bf Convergence of $\esp{B_{n,4}^2}$.} 

Setting 
$$B_{n,4,1}=\frac{1}{n} \sum_{1\leq j\neq k<i\leq n} \1{\rperm(i)=j} \1{{\Pi_n'}(i)=k} \esp{ C_{i,j}C_{i,k}\middle| X_j,X_k},$$
$$B_{n,4,2}=\frac{1}{n} \sum_{1\leq j\neq k<i\leq n} \1{\rperm(i)=j} \1{{\Pi_n'}(k)=i} \esp{ C_{i,j} C_{k,i}\middle| X_j,X_k},$$

$$B_{n,4,3}=\frac{1}{n} \sum_{1\leq j\neq k<i\leq n} \1{\rperm(j)=i} \1{{\Pi_n'}(i)=k} \esp{ C_{j,i}C_{i,k}\middle| X_j,X_k},$$

$$B_{n,4,4}=\frac{1}{n} \sum_{1\leq j\neq k<i\leq n} \1{\rperm(j)=i} \1{{\Pi_n'}(k)=i} \esp{C_{j,i}C_{k,i} \middle| X_j,X_k},$$

then $B_{n,4}=B_{n,4,1}+B_{n,4,2}+B_{n,4,3}+B_{n,4,4}.$

\begin{eqnarray*}
\esp{B_{n,4,1}^2}&=&\frac{1}{n^2} \sum_{1\leq j\neq k<i\leq n} \sum_{1\leq p\neq q<l\leq n} \esp{\1{\rperm(i)=j} \1{\rperm(l)=p}}\esp{ \1{{\Pi_n'}(i)=k} \1{{\Pi_n'}(l)=q} } \times\\
&&\esp{\esp{ C_{i,j}C_{i,k}\middle| X_j,X_k}\esp{C_{l,p}C_{l,q}\middle| X_p,X_q}}.
\end{eqnarray*}

Let us consider all the cases where $1\leq j\neq k<i\leq n$ and $1\leq p\neq q<l\leq n$.

If $\#\{j,k,p,q\}=3$, using a similar argument as in \eqref{eqstar}, we obtain that: 
$$\esp{\esp{ C_{i,j}C_{i,k}\middle| X_j,X_k}\esp{C_{l,p}C_{l,q}\middle| X_p,X_q}}=0.$$

If $j=p$, $k=q$, and $i=l$, then,
$$\esp{\1{\rperm(i)=j} \1{\rperm(l)=p}}\esp{ \1{{\Pi_n'}(i)=k} \1{{\Pi_n'}(l)=q} } =\frac{1}{n^2},$$
and
\begin{eqnarray*}
\esp{\esp{C_{i,j}C_{i,k}\middle| X_j,X_k} \esp{C_{l,p}C_{l,q}\middle| X_p,X_q}}&=&\esp{\pa{\esp{C_{i,j}C_{i,k}\middle| X_j,X_k}}^2}\\
&=&\esp{\pa{\esp{C_{3,1}C_{3,2}\middle| X_1,X_2}}^2}\\
&<&+\infty \textrm{ \quad under $\pa{\mc{A}_{\varphi,Mmt}}$}.
\end{eqnarray*}


If $j=p$, $k=q$, and $i\neq l$, or if $j=q$, $k=p$, and $i=l$, then $\1{\rperm(i)=j}\1{\rperm(l)=p}=0$, so
$$\esp{\1{\rperm(i)=j} \1{\rperm(l)=p}}\esp{ \1{{\Pi_n'}(i)=k} \1{{\Pi_n'}(l)=q} } =0.$$

If $j=q$, $k=p$, and $i\neq l$, then 
$$\esp{\1{\rperm(i)=j} \1{\rperm(l)=p}}\esp{ \1{{\Pi_n'}(i)=k} \1{{\Pi_n'}(l)=q} } =\frac{1}{n^2(n-1)^2},$$
and
\begin{eqnarray*}
\esp{\esp{C_{i,j}C_{i,k}\middle| X_j,X_k} \esp{C_{l,p}C_{l,q}\middle| X_p,X_q}}&=&\esp{\esp{C_{i,j}C_{i,k}\middle| X_j,X_k} \esp{C_{l,k}C_{l,j}\middle| X_j,X_k}}\\
&=&\esp{\esp{C_{i,j}C_{i,k}C_{l,k}C_{l,j}\middle| X_j,X_k}}\\
&=& \esp{C_{1,4}C_{1,3}C_{2,3}C_{2,4}}\\
&<&+\infty \textrm{ \quad under $\pa{\mc{A}_{\varphi,Mmt}}$}.
\end{eqnarray*}


By combining these results, we obtain that:
$$\esp{B_{n,4,1}^2}\leq C\frac{\esp{\pa{\esp{C_{3,1}C_{3,2}\middle| X_1,X_2}}^2}   }{n} +
C'\frac{\esp{C_{1,4}C_{1,3}C_{2,3}C_{2,4}}}{(n-1)^2}\cv{n\to \infty} 0.$$


Following the same lines of proof, we furthermore obtain that $\esp{B_{n,4,2}^2}$, $\esp{B_{n,4,3}^2}$, and $\esp{B_{n,4,4}^2}$ also converge towards $0$.

As a consequence, $$\esp{B_{n,4}^2}\cv{n\to\infty}0.$$

From \eqref{controlB}, and the convergence towards $0$ for $\esp{B_{n,1}^2}$, $\esp{B_{n,2}^2}$, $\esp{B_{n,3}^2}$, and $\esp{B_{n,4}^2}$, we derive that:
$$\esp{\pa{\sum_{i=2}^n\esp{Y_{n,i}Y_{n,i}'\middle| \mc{F}_{n,i-1}}}^2}\cv{n\to\infty}0,$$
which finally allows to conclude that assumption $(i)$ is satisfied.\\


{\bf Assumption $(ii)$.} Given $\varepsilon>0$, let us prove that 
$$\sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2\1{\left| aY_{n,i}+b Y_{n,i}'\right|>\varepsilon}}\cv{n\to\infty} 0.$$
Since $\1{\left| aY_{n,i}+b Y_{n,i}'\right|>\varepsilon}\leq \pa{\frac{aY_{n,i}+b Y_{n,i}'}{\varepsilon}}^2$,
\begin{eqnarray*}
\sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2\1{\left| aY_{n,i}+b Y_{n,i}'\right|>\varepsilon}}&\leq &\frac{1}{\varepsilon^2}
\sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^4}\\
&\leq &\frac{2^3}{\varepsilon^2} \sum_{i=2}^n \pa{a^4 \esp{Y_{n,i}^4}+b^4 \esp{{Y_{n,i}'}^4}}\\
&\leq &\frac{2^3(a^4+b^4)}{\varepsilon^2} \sum_{i=2}^n \esp{Y_{n,i}^4}.
\end{eqnarray*}

But it is easy to see that 
$$Y_{n,i}=\frac{1}{\sqrt{n}} \pa{C_{i,\rperm(i)}\1{\rperm(i)<i}+ C_{\Pi_n^{-1}(i),i}\1{\Pi_n^{-1}(i)<i} }.$$
Hence,
\begin{eqnarray*}
\esp{Y_{n,i}^4}&\leq&\frac{2^3}{n^2} \esp{C_{i,\rperm(i)}^4\1{\rperm(i)<i}+ C_{\Pi_n^{-1}(i),i}^4\1{\Pi_n^{-1}(i)<i} }.\\
&\leq&\frac{2^3}{n^2} \sum_{j=1}^{i-1} \pa{\esp{C_{i,j}^4\1{\rperm(i)=j}}+\esp{C_{j,i}^4\1{\Pi_n^{-1}(i)=j}}}\\
&\leq&\frac{2^4}{n^2} \esp{C_{1,2}^4}.
\end{eqnarray*}
We thus obtain that:
$$\sum_{i=2}^n\esp{\pa{aY_{n,i}+b Y_{n,i}'}^2\1{\left| aY_{n,i}+b Y_{n,i}'\right|>\varepsilon}}\leq \frac{2^7(a^4+b^4)}{\varepsilon^2 n} \esp{C_{1,2}^4},$$
where the right-hand side tends to $0$ as soon as $\esp{C_{1,2}^4}<+\infty$.

This last condition is ensured by $\pa{\mc{A}_{\varphi,Mmt}}$, which allows to check that assumption $(ii)$ is also checked, and that

$$\loi{a M_n^\rperm\pa{\X_n}+b M_n^{{\Pi_n'}} \pa{\X_n}}\Rightarrow  \mc{N}\pa{0, \pa{a^2+b^2}\sigma^{2}_{h_\varphi,P^1\otimes P^2}}.$$ 

This ends the proof of Lemma \ref{lm1}.

\end{proof}

Recall that we aim at proving that 
 $$d_{BL}\pa{\loi{M_n^\rperm\pa{\X_n}\middle| \X_n},\mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}}} \cvproba{n\to \infty} 0.$$
 
From Lemma \ref{lm1},  we deduce that for every $t$ in $\R$,
\begin{equation*}
\left\{ \begin{array}{l}
\proba{M_n^\rperm\pa{\X_n}  \leq t} \cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(t), \\
\proba{M_n^\rperm\pa{\X_n}  \leq t, M_n^{\Pi_n'}\pa{\X_n}  \leq t} \cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}^2(t).
\end{array}\right.
\end{equation*}

Setting $M_n=M_n^\rperm\pa{\X_n}$, and $M_n'=M_n^{\Pi_n'}\pa{\X_n}$ for the sake of simplicity, this leads to\begin{equation}\label{cvfdr}
\left\{ \begin{array}{l}
\esp{\esp{\1{M_n \leq t} \middle |\X_n}}\cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(t), \\
\esp{\pa{\esp{\1{M_n \leq t}\middle| \X_n}}^2}\cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}^2(t).
\end{array}\right.
\end{equation}

It is well-known (see \cite[Th. 9.2.1]{Dudley} for instance), that in a separable metric space, convergence in probability is metrizable, and therefore is equivalent to almost-sure convergence of a sub-sequence of any initial subsequence.

So, let us fix an initial extraction $\phi_0:\N\to\N$, which defines  a sub-sequence $\pa{M_{\phi_0(n)}}_{n\in\N}$ of $\pa{M_n}_{n\in\N}$.

Let us denote by $(q_m)_{m\in\N}$ a sequence such that $\{q_m,m\in \mathds{N}\}=\mathds{Q}$. For any $m$ in $\N$, from \eqref{cvfdr}, we derive that 
\begin{equation*}
\left\{ \begin{array}{l}
\esp{\esp{\1{M_{\phi_0(n)} \leq q_m} \middle |\X_{\phi_0(n)}}}\cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_m), \\
\esp{\pa{\esp{\1{M_{\phi_0(n)} \leq q_m}\middle| \X_{\phi_0(n)}}^2}}\cv{n\to+\infty} \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}^2(q_m),
\end{array}\right.
\end{equation*}
which leads (by Chebychev's inequality) to 
\begin{equation}\label{eq1cvprob}
\esp{\1{M_{\phi_0(n)} \leq q_m} \middle |\X_{\phi_0(n)}}\cvproba{n\to\infty}  \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_m).
\end{equation}

Therefore, there exists an extraction $\phi_1$ and a subset $\Omega_1$ of $\Omega$ such that: $\proba{\Omega_1}=1$, and for every $\omega\in\Omega_1$, 
$$\esp{\1{M_{\phi_1\circ\phi(n)} \leq q_1} \middle |\X_{\phi_1\circ\phi(n)}}(\omega)\cv{n\to\infty}  \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_1).$$

Now, let $m\geq 1$ for which there exists an extraction $\phi_m$ and a subset $\Omega_m$ of $\Omega$ such that: $\proba{\Omega_m}=1$, and for every $\omega\in\Omega_m$, 
$$\esp{\1{M_{\phi_m\circ \phi_{m-1}\circ \ldots\circ\phi_0(n)} \leq q_m} \middle |\X_{\phi_m\circ \phi_{m-1}\circ \ldots\circ\phi_0(n)}}(\omega)\cv{n\to\infty}  \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_m).$$
Then, from \eqref{eq1cvprob}, there also exist an extraction $\phi_{m+1}$ and a subset $\Omega_{m+1}$ of $\Omega$ such that: $\proba{\Omega_{m+1}}=1$, and for every $\omega\in\Omega_{m+1}$, 
$$\esp{\1{M_{\phi_{m+1}\circ\phi_m\circ \phi_{m-1}\circ \ldots\circ\phi_0(n)} \leq q_{m+1}} \middle |\X_{\phi_{m+1}\circ\phi_m\circ  \ldots\circ\phi_0(n)}}(\omega)\cv{n\to\infty}  \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_{m+1}).$$

Setting $\tilde{\Omega}= \bigcap_{m\in\N} \Omega_m$ and for every $n$ in $\N$, $\tilde\phi(n)=\phi_n\circ\ldots \circ \phi_2\circ\phi_1(n)$, then $\proba{\tilde\Omega}=1$, and for every $\omega$ in $\tilde\Omega$,  and $m$ in $\N$, 

$$\esp{\1{M_{\tilde{\phi}\circ\phi_0(n)} \leq q_{m}} \middle |\X_{ \tilde{\phi}\circ\phi_0(n) }}(\omega)\cv{n\to\infty}  \Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}(q_{m}).$$
Since $\Phi_{0,\sigma^2_{h_\varphi,P^1\otimes P^2}}$ is a continuous distribution function, this means that:

$$d_{BL}\pa{\loi{M_{\tilde{\phi}\circ\phi_0(n)}  \middle| \X_{ \tilde{\phi}\circ\phi_0(n)}},\mc{N}\pa{0, \sigma^2_{h_\varphi,P^1\otimes P^2} }} \cvps{n\to \infty} 0.$$

To conclude, we actually proved that
$$d_{BL}\pa{\loi{M_n^\rperm\pa{\X_n}\middle| \X_n},\mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}}} \cvproba{n\to \infty} 0,$$
which, combined with \eqref{dbl}, leads to:
$$d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n},   \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvproba{n\to \infty} 0.$$

\paragraph{Third step: convergence of conditional second order moments.} $\ $\\

Recall that from \eqref{simpleU}, $U_{n,h_\varphi}\pa{\X_n^\rperm} =\frac{1}{n-1} U_n^\rperm,$ where
\begin{eqnarray*}
U_n^\rperm&=&\sum_{i=1}^n \varphi\pa{X_i^1,X_{\rperm(i)}^2}-\frac{1}{n}\sum_{i,j=1}^n  \varphi\pa{X_i^1,X_j^2}\\
&=& \sum_{i,j=1}^n \pa{\1{ \rperm(i)=j}-\frac{1}{n}}  \varphi\pa{X_i^1,X_j^2}.
\end{eqnarray*}
Therefore,
\begin{equation}\label{dereq}
\esp{\pa{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} }^2\middle| \X_n}=\frac{n^2}{(n-1)^2}\pa{ \frac{1}{n} \esp{\pa{U_n^\rperm}^2 \middle |\X_n}}.
\end{equation}

\begin{eqnarray*}
\frac{1}{n} \esp{\pa{U_n^\rperm}^2 \middle |\X_n}&=&\frac{1}{n}\sum_{i,j=1}^n\sum_{k,l=1}^n \esp{\pa{\1{ \rperm(i)=j}-\frac{1}{n}} \pa{\1{ \rperm(k)=l}-\frac{1}{n}}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}\\
&=&\frac{1}{n}\sum_{i,j=1}^n\sum_{k,l=1}^n \pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}.
\end{eqnarray*}

Firstly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=4}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}=\frac{(n-2)(n-3)}{n^2} U_{n,1},$$
where
$$U_{n,1}=\frac{(n-4)!}{n!} \sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=4}}  \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2},$$
is clearly a $U$-statistic of order $4$.
From the strong law of large numbers of H{\oe}ffding \cite{Hoeffding61}, we thus have that 
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=4}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}\cvps{n\to \infty} \pa{\esp{\varphi\pa{X_1^1,X_2^2}}}^2.$$
Secondly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=3\\
i=j,i=l, j=k, \textrm{ or } k=l}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= \frac{2(n-2)}{n^2} U_{n,2},$$
where
$$U_{n,2}=\frac{(n-3)!}{n!} \sum_{\substack{i,k,l\in\{1,\ldots,n\}\\
\#\{i,k,l\}=3}} \pa{ \varphi\pa{X_i^1,X_i^2}\varphi\pa{X_k^1,X_l^2}+\varphi\pa{X_i^1,X_l^2}\varphi\pa{X_k^1,X_i^2}},$$
is a $U$-statistic of order $3$, so $\frac{2(n-2)}{n^2} U_{n,2}\cvps{n\to\infty} 0$.

Thirdly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=3\\
i=k, \textrm{ or } j=l}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= -\frac{n(n-1)(n-2)}{n^3} U_{n,3},$$
where
$$U_{n,3}=\frac{(n-3)!}{n!} \sum_{\substack{i,k,l\in\{1,\ldots,n\}\\
\#\{i,k,l\}=3}} \pa{ \varphi\pa{X_i^1,X_k^2}\varphi\pa{X_i^1,X_l^2}+\varphi\pa{X_i^1,X_l^2}\varphi\pa{X_k^1,X_l^2}},$$
is a $U$-statistic of order $3$.
So, 
$$-\frac{n(n-1)(n-2)}{n^3} U_{n,3}\cvps{n\to\infty}  -\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_1}}^2}-\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_2}}^2}.$$
Fourthly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=2\\
i=j=k, i=j=l,\\ i=k=l,\textrm{ or } j=k=l}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= -\frac{2(n-1)}{n^2} U_{n,4},$$
where
$$U_{n,4}=\frac{1}{n(n-1)} \sum_{1\leq i\neq j\leq n} \pa{ \varphi\pa{X_i^1,X_i^2}\varphi\pa{X_i^1,X_j^2}+\varphi\pa{X_i^1,X_i^2}\varphi\pa{X_j^1,X_i^2}},$$
is a $U$-statistic of order $2$, so $-\frac{2(n-1)}{n^2} U_{n,4}\cvps{n\to\infty}  0.$

Fifthly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=2\\
i=j\neq k=l , \textrm{ or } i=l\neq j=k}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= \frac{1}{n^2} U_{n,5},$$
where
$$U_{n,5}=\frac{1}{n(n-1)} \sum_{1\leq i\neq j\leq n} \pa{ \varphi\pa{X_i^1,X_i^2}\varphi\pa{X_j^1,X_j^2}+\varphi\pa{X_i^1,X_j^2}\varphi\pa{X_j^1,X_i^2}},$$
is a $U$-statistic of order $2$, so $\frac{1}{n^2}U_{n,5}\cvps{n\to\infty}  0.$

Sixthly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=2\\
i=k\neq j=l}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= \frac{(n-1)^2}{n^2} U_{n,6},$$
where
$$U_{n,6}=\frac{1}{n(n-1)} \sum_{1\leq i\neq j\leq n} \varphi^2\pa{X_i^1,X_j^2},$$
is a $U$-statistic of order $2$, so $\frac{(n-1)^2}{n^2}U_{n,6}\cvps{n\to\infty}  \esp{\varphi^2\pa{X_i^1,X_j^2}}.$

Seventhly,
$$\frac{1}{n}\sum_{\substack{i,j,k,l\in\{1,\ldots,n\}\\
\#\{i,j,k,l\}=1}}\pa{\esp{\1{ \rperm(i)=j}\1{ \rperm(k)=l}}-\frac{1}{n^2}} \varphi\pa{X_i^1,X_j^2}\varphi\pa{X_k^1,X_l^2}= \frac{n-1}{n^3} \sum_{i=1}^n \varphi\pa{X_i^1,X_i^2},$$
which almost-surely tends to $0$ thanks to the strong law of large numbers.

By combining all these results, we finally obtain that:
$$\frac{1}{n} \esp{\pa{U_n^\rperm}^2 \middle |\X_n}\cvps{n\to\infty} \pa{\esp{\varphi\pa{X_1^1,X_2^2}}}^2 -\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_1}}^2}-\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_2}}^2}+\esp{\varphi^2\pa{X_i^1,X_j^2}}.$$
Noticing that 
$$\pa{\esp{\varphi\pa{X_1^1,X_2^2}}}^2 -\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_1}}^2}-\esp{\pa{\esp{\varphi(X_1^1,X_2^2)\middle | X_2}}^2}+\esp{\varphi^2\pa{X_i^1,X_j^2}}=\sigma^2_{h_\varphi,P^1\otimes P^2},$$
from \eqref{dereq}, we deduce that
$$\esp{\pa{\sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} }^2\middle| \X_n}\cvps{n\to\infty} \sigma^2_{h_\varphi,P^1\otimes P^2}.$$
Since 
$$d_{BL}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n},   \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvproba{n\to \infty} 0,$$ this allows to conclude that:
$$d_{2}\pa{ \loi{ \sqrt{n}U_{n,h_\varphi}\pa{\X_n^\rperm} \middle | \X_n},   \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvproba{n\to \infty} 0.$$

% ------------------------------------------------------------ %
\subsection{Proof of Corollary \ref{coroPerm}}
% ------------------------------------------------------------ %

Let us fix $\eta$ in $(0,1)$, and recall that we aim at proving that, under the same assumptions as in Theorem \ref{consistencyperm}, 
$$q_{n,\varphi,\eta}^\star\pa{\X_n} \cvproba{n\to+\infty} \Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(\eta),$$
where
$q_{n,\varphi,\eta}^\star\pa{\X_n}$ denotes the $\eta$-quantile of $\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^\rperm}$ given $\X_n$. 

Here, unlikely for the bootstrap approach, we only have in Theorem \ref{consistencyperm} a consistency result in probability. Thus, we use the same argument of sub-sequences as in the proof of the Theorem itself, in order to come down to an almost-sure convergence. 

So let us consider an extraction $\phi_0:\N\to\N$ which defines a sub-sequence.

Then, by Theorem \ref{consistencyperm}, there exists an extraction $\phi_1$ such that

\begin{equation}
\label{cvBLpsPerm}
d_{BL}\pa{ \loi{ \sqrt{\phi_1\circ\phi_0(n)}U_{\phi_1\circ\phi_0(n),h_\varphi}\pa{\X_{\phi_1\circ\phi_0(n)}^{\Pi_{\phi_1\circ\phi_0(n)}}} \middle | \X_{\phi_1\circ\phi_0(n)}},   \mc{N}\pa{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}} }\cvps{n\to \infty} 0.
\end{equation}
In particular, applying \cite[Lemma 21.2]{vandervaart} on the event where the convergence above is true, we directly obtain that 
$$q_{\phi_1\circ\phi_0(n),\varphi,\eta}^\star\pa{ \X_{\phi_1\circ\phi_0(n)}}\cvps{n\to+\infty} \Phi^{-1}_{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}}(\eta),$$
which ends the proof. \\


% ------------------------------------------------------------ %
\subsection{Proof of Theorem \ref{NivConsistencyPerm}}
% ------------------------------------------------------------ %

As for the bootstrap approach, we focus on the case $\Delta^\star=\Delta_{\varphi,\alpha}^{\star +}\pa{X_n}$, the proof in the other cases being similar. 

Given $\alpha$ in $(0,1)$, we deduce from Corollary \ref{coroPerm} that
\begin{equation}\label{cvqstar2}
q_{n,\varphi,1-\alpha}^\star\pa{\X_n}\cvproba{n\to+\infty}\Phi^{-1}_{0,\sigma^2_{h_{\varphi},P^1 \otimes P^2}}(1-\alpha)
\end{equation}
Then, from Proposition \ref{TCLUstat} and Slutsky's lemma, under $(H_0)$, $\pa{\sqrt{n} U_{n,h}(\X_n), q_{n,\varphi,1-\alpha}^\star\pa{\X_n}}$ converges in distribution towards $(Z,\Phi^{-1}_{0,\sigma^2_{h_\varphi,P^1 \otimes P^2}}(1-\alpha))$, where $Z\sim \mathcal{N}(0,\sigma^2_{h_\varphi,P^1 \otimes P^2})$. Therefore, under $(H_0)$,
$$\proba{\sqrt{n} U_{n,h_\varphi}\pa{\X_n}>q_{n,\varphi,1-\alpha}^\star\pa{\X_n}}\cv{n\to+\infty}\alpha.$$

As for the consistency under $(H_1)$, we can argue exactly in the same way as for the bootstrap, to end the proof.


\subsection{Proof of Proposition \ref{MonteCarlo}}


In order to obtain the asymptotic results in Proposition \ref{MonteCarlo}, it is sufficient to prove that\begin{equation}
\label{cvValCritPermMC}
\sqrt{n}U^{\star (\lceil (1-\alpha)(B_n+1) \rceil)} \cvproba{n\to\infty} \Phi^{-1}_{0,\sigma^2_{h,P^1 \otimes P^2}}(1-\alpha).
\end{equation} 
Then, we conclude using exactly the same arguments as in the proof of Theorem \ref{NivConsistencyPerm}. 

To obtain \eqref{cvValCritPermMC}, we follow the proof of \cite[Lemma 21.2]{vandervaart}.

Let us introduce some more notations. 

\begin{itemize}
\item Let $F^\star_{n,\X_n}$ be the conditional cumulative distribution function (c.d.f) of 
$\loi{\sqrt{n}U_{n,h_\varphi},P_n^{\star} | \X_n}$, and let us first show that 
\begin{equation}
\label{cvProbaSupVraieGauss}
\sup_{z\in\R}\left|F^\star_{n,\X_n}(z) - \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right| \cvproba{n\to\infty} 0.
\end{equation}

As Theorem \ref{consistencyperm} provides only a convergence in probability, similar arguments of subsequences as in the proof of Corollary \ref{coroPerm}, have to be used.
Let $\phi_0$ be an initial extraction and $\phi_1$ be the extraction such that \eqref{cvBLpsPerm} is satisfied. 
As convergence in the $d_{BL}$ metric is equivalent to a weak convergence, and as the limit is continuous, by \cite[Lemma 2.11]{vandervaart} we obtain that 
$\sup_{z\in\R} \left|F^\star_{\phi_1\circ\phi_0(n),\X_{\phi_1\circ\phi_0(n)}}(z) - \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right|\cvps{n\to\infty} 0. $
This being true for any initial subsequence $\phi_0$, we obtain \eqref{cvProbaSupVraieGauss}.


\item Let $F^{\star B_n}_{n,\X_n}$ denote the empirical c.d.f of $\loi{\sqrt{n}U_{n,h_\varphi},P_n^{\star} | \X_n}$ associated to the sample $\pa{\Pi_n^1,\ldots,\Pi_n^{B_n}}$, that is 
$$\forall z\in\R,\quad 
F^{\star B_n}_{n,\X_n}(z) = \frac{1}{B_n}\sum_{{\bf b}=1}^{B_n} \1{\sqrt{n}U_{n,h_\varphi}\!\pa{\X_n^{\Pi_n^{\bf b}}}\leq z}.$$

Then, by the DKW inequality (see \cite{vandervaart} p.268 for instance), we obtain as in the proof of Proposition \ref{bootMonteCarlo},
\begin{equation}
\label{cvProbaSupEmpVraie}
\sup_{z\in\R}\left|F^{\star,B_n}_{n,\X_n}(z) - F^\star_{n,\X_n}(z)\right| \cvproba{n\to\infty} 0.
\end{equation}

\item Finally, let
$$G^{\star B_n}_{n,\X_n}(z) = \frac{1}{B_n+1} \sum_{{\bf b}=1}^{B_n+1} \1{\sqrt{n} U^{\star b} \leq z}.$$
Since $G^{\star B_n}_{n,\X_n}(z) = \frac{1}{B_n+1}\pa{\1{\sqrt{n}U_{n,h_\varphi}(\X_n)\leq z} + B_n F^{\star B_n}_{n,\X_n}(z)}$, 
\begin{equation}
\label{cvpsSupEmpValCrit}
\sup_{z\in\R}\left|G_{n,\X_n}^{\star B_n}(z) - F^{\star,B_n}_{n,\X_n}(z)\right|\leq \frac{2}{B_n+1} \cvps{n\to\infty} 0
\end{equation}
\end{itemize} 

Finally, combining \eqref{cvProbaSupVraieGauss}, \eqref{cvProbaSupEmpVraie} and \eqref{cvpsSupEmpValCrit}, we obtain that 
\begin{equation}
\label{cvProbaSupValCritGauss}
\sup_{z\in\R}\left|G_{n,\X_n}^{\star B_n}(z) - \Phi_{0,\sigma^2_{h,P^1 \otimes P^2}}(z)\right| \cvproba{n\to\infty} 0.
\end{equation}

Since
$$\sqrt{n}U^{\star (\lceil (1-\alpha)(B_n+1) \rceil)} = \pa{G_{n,\X_n}^{\star B_n}}^{-1}(1-\alpha),$$ we conclude in the same way as in the proof of Proposition \ref{bootMonteCarlo}.


\subsection{Proof of Proposition \ref{MonteCarloniveauexact}}

Under $(H_0)$, from Proposition \ref{exactdistribution}, we deduce that $\X_n^{\Pi_n^1},\ldots,\X_n^{\Pi_n^B} ,\X_n$, and hence also $U^{\star 1},\ldots,U^{\star (B+1)}$, are exchangeable real-valued random variables. Then,
\begin{eqnarray*}
\proba{U^{\star (B+1)}>U^{\star \pa{\lceil (1-\alpha)(B+1)\rceil }}}&= &\proba{\sum_{b=1}^{B+1}\1{U^{\star b}<U^{\star (B+1)}}\geq (B+1)-\lfloor \alpha(B+1)\rfloor }\\
&=& \proba{\sum_{b=1}^{B+1}\1{U^{\star b}\geq U^{\star (B+1)}}\leq \lfloor \alpha(B+1)\rfloor}\\
&=& \proba{\sum_{b=1}^{B+1}\1{U^{\star b}\geq U^{\star (B+1)}}\leq \alpha (B+1)}
\end{eqnarray*}
By exchangeability of $U^{\star 1},\ldots,U^{\star (B+1)}$, applying Lemma 1 in \cite{RomanoWolf}, whose proof is given in \cite{Arlot1}, we finally obtain that:
$$\proba{U^{\star (B+1)}>U^{\star \pa{\lceil (1-\alpha)(B+1)\rceil }}}\leq \alpha.$$


\subsection{Proof of Proposition \ref{hinH}} 
\label{contcoinc}
% ---------------------------------------------------------------------------------------------------- %


Let us prove that $h=h_{\varphi_\delta^coinc}$ is continuous for the topology induced by $d$ in any $\pa{x_0,y_0}\in \mathcal{C}_\delta$ satisfying 
\begin{equation*}%\label{1indep2}
\pa{\ac{x_0^1}\cup\ac{y_0^1}} \cap \pa{\ac{x_0^2 \pm \delta}\cup\ac{y_0^2 \pm\delta}} = \emptyset
\end{equation*}

Consider a sequence $\ac{\pa{x_n, y_n}}_{n\in\N}$ of elements of $\calX^2\times\calX^2$, where $x_n = \pa{x_n^1,x_n^2}$ and $y_n = \pa{y_n^1,y_n^2}$ such that $d \pa{\pa{x_n,y_n},\pa{x_0,y_0}} \cv{n\to \infty} 0$ and $\pa{x_0,y_0}$ belongs to $\mathcal{C}_\delta$. 

We want to show that $\left| h\pa{x_n,y_n} - h\pa{x_0,y_0}\right| \cv{n\to \infty} 0$.

For an element $z$ in $\calX$, we denote by $N_z$ the count function defined by:
$$N_z(t)=\int \1{u\leq t} dN_{z}(u).$$

Since $\pa{x_0,y_0}$ is in $\mathcal{C}_\delta$, for any $t_0$ in $\ac{x_0^2\pm\delta}\cup\ac{y_0^2\pm\delta}$, 
$t_0\notin x_0^1$, which means that $N_{x_0^1}$ is continuous in $t_0$:
$$\exists\eta_{t_0} >0 \tq \forall t\in [0,1],\quad |t-t_0|\leq\eta_{t_0} \quad \Rightarrow \quad N_{x_0^1}(t) =N_{x_0^1}(t_0).$$

As $\ac{x_0^2\pm\delta}\cup\ac{y_0^2\pm\delta}$ is finite, $\eta_{x_0^1} = \min_{t_0\in \ac{x_0^2\pm\delta}\cup\ac{y_0^2\pm\delta}} \eta_{t_0} >0$ is well defined, and satisfies 
$$\forall u\in \ac{x_0^2\pm\delta}\cup \ac{y_0^2\pm\delta},\ \forall t\in [0,1],\quad |t-u|\leq\eta_{x_0^1} \quad \Rightarrow \quad N_{x_0^1}(t) =N_{x_0^1}(u).$$

By the same argument using continuity of $N_{y_0^1}$ over $\ac{x_0^2\pm\delta}\cup\ac{y_0^2\pm\delta}$, we can find $\eta_{y_0^1}>0$ such that 
$$\forall u\in \ac{x_0^2\pm\delta}\cup \ac{y_0^2\pm\delta},\ \forall t\in [0,1],\quad |t-u|\leq\eta_{y_0^1} \quad \Rightarrow \quad N_{y_0^1}(t) = N_{y_0^1}(u).$$

Since saying that $(x_0,y_0)$ belongs to $\mathcal{C}_\delta$ is equivalent to saying that $\pa{\ac{x_0^2}\cup\ac{y_0^2}} \cap \pa{\ac{x_0^1 \pm \delta}\cup\ac{y_0^1 \pm\delta}} = \emptyset$, we can construct $\eta_{x_0^2}$ and $\eta_{y_0^2}$ satisfying 

$$\forall u\in \ac{x_0^1\pm\delta}\cup \ac{y_0^1\pm\delta},\ \forall t\in [0,1],\quad 
\left\{\begin{array}{l}
|t-u|\leq\eta_{x_0^2} \quad \Rightarrow \quad N_{x_0^2}(t)=N_{x_0^2}(u), \\ 
|t-u|\leq\eta_{y_0^2} \quad \Rightarrow \quad N_{y_0^2}(t)=N_{y_0^2}(u). \end{array}\right.$$

Finally, if $\eta =\min\ac{\eta_{x_0^1},\eta_{y_0^1},\eta_{x_0^2},\eta_{y_0^2}}>0$,

\begin{equation}\label{etaconst1}
\forall s\in\ac{x_0^2 \pm \delta}\cup\ac{y_0^2\pm\delta},\ \forall t\in [0,1],\quad
|t-s|\leq\eta \quad \Rightarrow \quad 
\left\{\begin{array}{l}
N_{x_0^1}(t)= N_{x_0^1}(s),\\ 
N_{y_0^1}(t) = N_{y_0^1}(s),\end{array}\right.
\end{equation}
and
\begin{equation}\label{etaconst2}
\forall s\in\ac{x_0^1 \pm \delta}\cup\ac{y_0^1\pm\delta},\ \forall t\in [0,1],\quad
|t-s|\leq\eta \quad \Rightarrow \quad
\left\{\begin{array}{l}
N_{x_0^2}(t) = N_{x_0^2}(s), \\ 
N_{y_0^2}(t) = N_{y_0^2}(s). \end{array}\right.
\end{equation}

As $d\pa{\pa{x_n,y_n},\pa{x_0,y_0}} \cv{n\to \infty} 0$, there exists some $n_0\geq 0$ such that for any $n$ exceeding $n_0$, 
$$d\pa{\pa{x_n,y_n},\pa{x_0,y_0}}\leq \frac\eta 4.$$
From the definition of $d$, we deduce that:
$$ \exists \lambda_n^1 \in \Lambda \tq 
\left\{ \begin{array}{lr} \sup_{t\in [0,1]} \left|\lambda_n^1(t) - t\right| \leq \frac{\eta}{4}, & (1\mbox{-}i)\\
\sup_{t\in [0,1]}\left|N_{x_n^1}(t) - N_{x_0^1}\pa{\lambda_n^1(t)} \right| \leq \frac{\eta}{4}, & (1\mbox{-}ii)\end{array}\right. $$
and 
$$ \exists \lambda_n^2 \in \Lambda \tq 
\left\{ \begin{array}{lr} \sup_{t\in [0,1]} \left|\lambda_n^2(t) - t\right| \leq \frac{\eta}{4}, & (2\mbox{-}i)\\
\sup_{t\in [0,1]}\left|N_{x_n^2}(t) - N_{x_0^2}\pa{\lambda_n^2(t)} \right| \leq \frac{\eta}{4}. & (2\mbox{-}ii)\end{array}\right. $$
Notice that similar results occur for $y_n$ and $y_0$, but there are not detailed here since we will not use them explicitly.

\medskip


By definition of $h$, 
\begin{eqnarray*}
 h\pa{x_n,y_n} - h\pa{x_0,y_0} 
 &=& \frac12\iint \1{|u-v|\leq\delta} \ac{dN_{x_n^1}dN_{x_n^2} + dN_{y_n^1}dN_{y_n^2} - dN_{x_n^1}dN_{y_n^2} - dN_{y_n^1}dN_{x_n^2}}(u,v) \\
 &-&\frac12\iint \1{|u-v|\leq\delta} \ac{dN_{x_0^1}dN_{x_0^2} + dN_{y_0^1}dN_{y_0^2} - dN_{x_0^1}dN_{y_0^2} - dN_{y_0^1}dN_{x_0^2}}(u,v). \\
\end{eqnarray*}

Then,
\begin{align}\label{controlh}
\nonumber h\pa{x_n,y_n} - h\pa{x_0,y_0} &=\frac12 \iint \1{|u-v|\leq\delta} \Bigg(dN_{x_n^1}(u)\pa{dN_{x_n^2}-dN_{x_0^2}}(v) + dN_{y_n^1}(u)\pa{dN_{y_n^2}-dN_{y_0^2}}(v) \\
\nonumber &- dN_{x_n^1}(u)\pa{dN_{y_n^2}-dN_{y_0^2}}(v) - dN_{y_n^1}(u)\pa{dN_{x_n^2}-dN_{x_0^2}}(v) \\
\nonumber &+ \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v) + \pa{dN_{y_n^1}-dN_{y_0^1}}(u)\ dN_{y_0^2}(v) \\
 &-\pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{y_0^2}(v) + \pa{dN_{y_n^1}-dN_{y_0^1}}(u)\ dN_{x_0^2}(v) \Bigg).
\end{align}



By symmetry of the problem, we just need to study the terms $A_n:=\iint \1{|u-v|\leq\delta}\pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v)$, and
$B_n:=\iint \1{|u-v|\leq\delta} dN_{x_n^1}(u)\pa{dN_{x_n^2}-dN_{x_0^2}}(v) $.


% --------------------- study of \Psi_n --------------------- %

\paragraph{Study of $A_n$.}

\begin{eqnarray*}
A_n &=& \iint \1{|u-v|\leq\delta}\pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v) \\
&=& \iint \pa{\1{u\leq v+\delta} - \1{u< v-\delta}} \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v) \\
&=& \iint \1{u\leq v+\delta} \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v) - \iint \1{u< v-\delta} \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v)
\end{eqnarray*}

We have that:
 \begin{eqnarray*}
 \abs{\iint \1{u\leq v+\delta} \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v) }
 &=& \abs{\int\pa{N_{x_n^1}(v+\delta)-N_{x_0^1}(v+\delta)}dN_{x_0^2}(v)}  \\
 &\leq& \sum_{T\in x_0^2} \abs{N_{x_n^1}(T+\delta)-N_{x_0^1}(T+\delta)}\\
 &\leq &\sum_{T\in x_0^2} \left| N_{x_n^1}(T+\delta)-N_{x_0^1}\pa{\lambda_n^1(T+\delta)} \right| \\
 &&+\sum_{T\in x_0^2}\left| N_{x_0^1}\pa{\lambda_n^1(T+\delta)}-N_{x_0^1}(T+\delta) \right|.
\end{eqnarray*}
Now, using the notation $N_{x_i^1}^-(t)=\int \1{u< t} dN_{x_i^1}(u)$,
\begin{eqnarray*}
\abs{ \iint \1{u< v-\delta} \pa{dN_{x_n^1}-dN_{x_0^1}}(u)\ dN_{x_0^2}(v)} 
 &\leq& \sum_{T\in x_0^2} \abs{N_{x_n^1}^-(T-\delta)-N_{x_0^1}^-(T-\delta)}.
\end{eqnarray*}
Therefore,
 \begin{eqnarray}\label{controlAn}
\nonumber |A_n|&\leq &\sum_{T\in x_0^2} \Bigg(\left| N_{x_n^1}(T+\delta)-N_{x_0^1}\pa{\lambda_n^1(T+\delta)} \right|+\left| N_{x_0^1}\pa{\lambda_n^1(T+\delta)}-N_{x_0^1}(T+\delta) \right|\\
 &&+  \abs{N_{x_n^1}^-(T-\delta)-N_{x_0^1}^-(T-\delta)}\Bigg).
\end{eqnarray}
Let us study individually each term in the sum.

Fix $T$ in $x_0^2$.  By $(1\mbox{-}ii)$,
\begin{equation}\label{An1}
\left| N_{x_n^1}(T+\delta)-N_{x_0^1}\pa{\lambda_n^1(T+\delta)} \right| \leq \frac{\eta}{4} \leq \varepsilon.
\end{equation}
From $(1\mbox{-}i)$, we derive that $|\lambda_n^1(T+\delta)-(T+\delta)|\leq\frac{\eta}{2} \leq \eta$ which, with \eqref{etaconst1}, implies that 
\begin{equation}\label{An2}
\left|N_{x_0^1}\pa{\lambda_n^1(T+\delta)}-N_{x_0^1}(T+\delta)\right|=0.
\end{equation}

As $N_{x_n^1}^-(T-\delta) = \lim\limits_{\substack{u\to T-\delta \\ u<T-\delta}} N_{x_n^1}(u)$, there exists 
$u_T\in [T-\delta-\eta/4, T-\delta[$ such that 
$$\left| N_{x_n^1}^-(T-\delta)-N_{x_n^1}\pa{u_T} \right| \leq \varepsilon,$$ so
\begin{equation}\label{decoup-}
 \left| N_{x_n^1}^-(T-\delta)-N_{x_0^1}^-(T-\delta) \right|\leq\varepsilon+ \left| N_{x_n^1}\pa{u_T}-N_{x_0^1}\pa{\lambda_n^1(u_T)} \right|+\left| N_{x_0^1}\pa{\lambda_n^1(u_T)}-N_{x_0^1}^-(T-\delta) \right| .\end{equation}
From $(1\mbox{-}ii)$, one has $\left| N_{x_n^1}\pa{u_T}-N_{x_0^1}\pa{\lambda_n^1(u_T)} \right|\leq\frac{\eta}{4} \leq \varepsilon$.

 Then, by continuity of $N_{x_0^1}$ in $T-\delta$, first remark that $N_{x_0^1}^-(T-\delta)=N_{x_0^1}(T-\delta)$. Moreover, by $(1\mbox{-}i)$ and construction of $u_T$,
 $$\left|\lambda_n^1(u_T)-(T-\delta)\right| \leq \left|\lambda_n^1(u_T)-u_T\right| + \left|u_T-(T-\delta)\right|\leq \frac{\eta}{4}+\frac{\eta}{4} < \eta,$$
 hence, using \eqref{etaconst1},
 $\left| N_{x_0^1}\pa{\lambda_n^1(u_T)}-N_{x_0^1}^-(T-\delta) \right| =0.
$

So finally, \eqref{decoup-} gives
\begin{equation}\label{An3}
 \left| N_{x_n^1}^-(T-\delta)-N_{x_0^1}^-(T-\delta) \right| \leq 2\varepsilon.
\end{equation}

Combining \eqref{controlAn}, \eqref{An1}, \eqref{An2}, and \eqref{An3}, we obtain that:
\begin{equation}\label{majoAn}
|A_n|\leq 3\varepsilon \#x_0^2,
\end{equation}
for any $n\geq n_0$.

\medskip

\paragraph{Study of $B_n$.}

Recall that $B_n=\iint \1{|u-v|\leq\delta} dN_{x_n^1}(u)\pa{dN_{x_n^2}-dN_{x_0^2}}(v)$. \\
As for $A_n$, we can upper bound $B_n$ by a sum of several terms, that can be studied separately.
\begin{eqnarray*}
B_n &=& \iint \1{u\leq v+\delta} dN_{x_n^1}(u)\ \pa{dN_{x_n^2}-dN_{x_0^2}}(v) - \iint \1{u< v-\delta} dN_{x_n^1}(u)\ \pa{dN_{x_n^2}-dN_{x_0^2}}(v)\\
&=&  \sum_{T\in x_n^1} \pa{N_{x_n^2}\pa{T+\delta}-N_{x_0^2}\pa{T+\delta}} -\sum_{T\in x_n^1} \pa{N_{x_n^2}^-\pa{T-\delta}-N_{x_0^2}^-\pa{T-\delta}}.
\end{eqnarray*}
So
\begin{equation}\label{controlBn}
 B_n \leq |B_{n,1}|+|B_{n,2}|+|B_{n,3}|+|B_{n,4}|,
\end{equation}
with:
\begin{eqnarray*}
B_{n,1}&=&\sum_{T\in x_n^1} \pa{ N_{x_n^2}\pa{T+\delta}-N_{x_0^2}\pa{\lambda_n^2\pa{T+\delta}} },\\
B_{n,2}&=& \sum_{T\in x_n^1} \pa{N_{x_0^2}\pa{\lambda_n^2\pa{T+\delta}} - N_{x_0^2}\pa{T+\delta}},\\
B_{n,3}&=&\sum_{T\in x_n^1}  \pa{N_{x_n^2}^-\pa{T-\delta}-N_{x_0^2}^-\pa{\lambda_n^2\pa{T-\delta}}},\\
B_{n,4}&=&\sum_{T\in x_n^1} \pa{N_{x_0^2}^-\pa{\lambda_n^2\pa{T-\delta}} + N_{x_0^2}^-\pa{T-\delta}}.
\end{eqnarray*}

The control of $B_n$ is quite similar as the one of $A_n$ except that the sums are over $T\in x_n^1$ instead of $T\in x_0^1$, which prevents us to use \eqref{etaconst2} and \eqref{etaconst1} directly.

\medskip

{\bf Control of $B_{n,1}$.}

\medskip

From $(2\mbox{-}ii)$, we first deduce that
$\left| N_{x_n^2}\pa{T+\delta}-N_{x_0^2}\pa{\lambda_n^2\pa{T+\delta}} \right|\leq \varepsilon$, so

\begin{equation}\label{Bn1}
|B_{n,1}|\leq \varepsilon \#x_n^1.
\end{equation}

\medskip

{\bf Control of $B_{n,2}$.}

\medskip

One can easily see that:
\begin{eqnarray*}
B_{n,2}&=& \iint \1{v\leq \lambda_n^2\pa{u+\delta}} - \1 {v\leq u+\delta} dN_{x_0^2}(v)\ dN_{x_n^1}(u) \\
&=& \iint \pa{1- \1{u<\pa{\lambda_n^2}^{-1}(v) - \delta}} - \pa{1-\1{u< v - \delta}}dN_{x_0^2}(v)\ dN_{x_n^1}(u) \\
&=& \sum_{T\in x_0^2} \pa{N_{x_n^1}^-(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) }.
\end{eqnarray*}

Fix now $T$ in $x_0^2$. 
\begin{eqnarray*}
\left|N_{x_n^1}^-(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) \right|
&\leq & \left|N_{x_n^1}^-(T-\delta) - N_{x_0^1}(T-\delta) \right|+ \left| N_{x_0^1}(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) \right|.
\end{eqnarray*}

As shown in \eqref{decoup-}, $\left|N_{x_n^1}^-(T-\delta) - N_{x_0^1}(T-\delta) \right|\leq 2\varepsilon$. 

Furthermore, 
take $v_T\in \left[\pa{\lambda_n^2}^{-1}(T)-\delta - \eta/4, \pa{\lambda_n^2}^{-1}(T)-\delta\right[$  such that 
$$\left| N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) -N_{x_n^1}(v_T)\right|\leq \varepsilon.$$
 So,
$$
\left| N_{x_0^1}(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) \right| 
\leq \varepsilon+ \left|N_{x_n^1}(v_T)-N_{x_0^1}\pa{\lambda_n^1(v_T)}\right| +\left|N_{x_0^1}\pa{\lambda_n^1(v_T)} -  N_{x_0^1}(T-\delta) \right|.$$
By construction of $v_T$ and $\lambda_n^1$ (see $(1\mbox{-}ii)$), $\left|N_{x_n^1}(v_T)-N_{x_0^1}\pa{\lambda_n^1(v_T)}\right| \leq \varepsilon$.

Because of \eqref{etaconst1} which is feasible as $\left|\lambda_n^1(v_T) - (T-\delta)\right| \leq |\lambda_n^1(v_T) - v_T| + |v_T - (T-\delta)|\leq \frac{\eta}{4}+\frac{\eta}{4}<\eta$ by $(1\mbox{-}i)$ and the construction of $v_T$,
$\left|N_{x_0^1}\pa{\lambda_n^1(v_T)} -  N_{x_0^1}(T-\delta) \right|=0.$

Hence,
$\left| N_{x_0^1}(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) \right|\leq 2\varepsilon$. 

Finally, 
$$\left|N_{x_n^1}^-(T-\delta) - N_{x_n^1}^-(\pa{\lambda_n^2}^{-1}(T)-\delta) \right| \leq 4\varepsilon$$ 
and 
\begin{equation}\label{Bn2}
|B_{n,2}| \leq 4\varepsilon \#x_0^2.
\end{equation}

\medskip

{\bf Control of $B_{n,3}$.}

\medskip

First, for all $T$ in $x_n^1$, we find some $\nu_{n,T} \in ]0, \eta/4]$ such that 
$$\forall u \in [T-\delta - \nu_{n,T}, T-\delta[,\quad \left|N_{x_n^2}^-\pa{T-\delta} - N_{x_n^2}\pa{u}\right|\leq \varepsilon.$$ Setting $\nu_n=\min_{T\in x_n^1} \nu_{n,T}$,
\begin{eqnarray*}
\left|B_{n,3}\right|& \leq& \sum_{T\in x_n^1} \left|N_{x_n^2}^-\pa{T-\delta} - N_{x_n^2}\pa{T-\delta-\nu_n}\right| +  \sum_{T\in x_n^1} \left|N_{x_n^2}\pa{T-\delta-\nu_n} - N_{x_0^2}\pa{\lambda_n^2\pa{T-\delta-\nu_n}}\right| \\
&  &+  \left|\sum_{T\in x_n^1} \pa{N_{x_0^2}\pa{\lambda_n^2\pa{T-\delta-\nu_n}}-N_{x_0^2}^-\pa{\lambda_n^2\pa{T-\delta}}}\right|.
\end{eqnarray*}
For each $T$, $\left|N_{x_n^2}^-\pa{T-\delta} - N_{x_n^2}\pa{T-\delta-\nu_n}\right| \leq \varepsilon $ and
$\left|N_{x_n^2}\pa{T-\delta-\nu_n} - N_{x_0^2}\pa{\lambda_n^2\pa{T-\delta-\nu_n}}\right| \leq \varepsilon $ by $(2\mbox{-}ii)$.

Therefore,
$$\left|B_{n,3}\right| \leq2\varepsilon \#x_{n}^1+  \left|\sum_{T\in x_n^1} \pa{N_{x_0^2}\pa{\lambda_n^2\pa{T-\delta-\nu_n}}-N_{x_0^2}^-\pa{\lambda_n^2\pa{T-\delta}}}\right|.$$
Now,
\begin{align*}
\sum_{T\in x_n^1} \Bigg(N_{x_0^2}\Big(\lambda_n^2&\pa{T-\delta-\nu_n}\Big)-N_{x_0^2}^-\pa{\lambda_n^2\pa{T-\delta}} \Bigg)\\
&=\iint \1{v\leq \lambda_n^2\pa{u-\delta-\nu_n}} - \1{v<\lambda_n^2\pa{u-\delta}} dN_{X_n^1}(u)\ dN_{X_0^2}(v) \\
&= \sum_{T\in x_0^2}\pa{N_{x_n^1}\pa{\pa{\lambda_n^2}^{-1}(T)+\delta} - N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}.
\end{align*}

For each $T$ in $x_0^2$,
\begin{align*}
\Big|N_{x_n^1}\Big(\pa{\lambda_n^2}^{-1}(T)+\delta\Big)&- N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}\Big|\\
&\leq  \left|N_{x_n^1}\pa{\pa{\lambda_n^2}^{-1}(T)+\delta} - N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}} \right| \\
&  + \left|N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}} - N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}\right| \\
&  +  \left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}- N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}\right|\\
&\leq 2\varepsilon +\left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}- N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}\right|,
\end{align*}
where the last line comes from $(1\mbox{-}ii)$, and \eqref{etaconst1}.

We now find some $w_T\in \left[\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n-\eta/4\ ,\ \pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n\right[$ such that 
$$\left| N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}- N_{x_n^1}\pa{w_T}\right|\leq\varepsilon,$$ 
so
\begin{multline*}
 \left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}- N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}\right|\\
 \leq \left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}- N_{x_0^1}\pa{\lambda_n^1\pa{w_T}}\right| + \left| N_{x_0^1}\pa{\lambda_n^1\pa{w_T}} - N_{x_n^1}\pa{w_T} \right| +\varepsilon. 
\end{multline*}

From $(1\mbox{-}ii)$, we deduce that $\left| N_{x_0^1}\pa{\lambda_n^1\pa{w_T}} - N_{x_n^1}\pa{w_T} \right|\leq \varepsilon.$

Due to \eqref{etaconst1}, $(1\mbox{-}i)$, and the construction of $w_T$,
$$\left|\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}} - (T-\delta)\right| \leq\frac{3\eta}{4} < \eta,$$
and 
\begin{eqnarray*}
\left|\pa{\lambda_n^1\pa{w_T}} - (T-\delta)\right| 
&\leq &\left|\pa{\lambda_n^1\pa{w_T} - w_T}\right|+\left|w_T - (T-\delta)\right|\\
&\leq &\frac \eta 4+\frac \eta 4\\
&<& \eta,
\end{eqnarray*}
so 
$$\left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}}- N_{x_0^1}\pa{\lambda_n^1\pa{w_T}}\right|=0.$$

As a consequence,
$$\Big|N_{x_n^1}\Big(\pa{\lambda_n^2}^{-1}(T)+\delta\Big)- N_{x_n^1}^-\pa{\pa{\lambda_n^2}^{-1}(T)+\delta+\nu_n}\Big|\leq 4\varepsilon,$$
 and
 
\begin{equation}\label{Bn3}
\left|B_{n,3}\right| \leq2\varepsilon \#x_{n}^1+  4\varepsilon  \#x_0^2.
\end{equation}

\paragraph{}


{\bf Control of $B_{n,4}$.}
 
 \medskip
 
 \begin{eqnarray*}
B_{n,4}
&=& \iint \pa{\1{v<\lambda_n^2\pa{u-\delta}} - \1{v < u-\delta} }dN_{x_0^2}(v)\ dN_{x_n^1}(u) \\
&=& \sum_{T\in x_0^2} \pa{N_{x_n^1}(T+\delta) - N_{x_n^1}\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}}
\end{eqnarray*}

Let us fix  $T$ in $x_0^2$. We have 
\begin{eqnarray*}
\left|N_{x_n^1}(T+\delta) - N_{x_n^1}\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}\right|
&\leq & \left| N_{x_n^1}(T+\delta) - N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}}\right| \\
&  +  & \left| N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}} - N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}}\right| \\
&  +  & \left| N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}} - N_{x_n^1}\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}\right|.
\end{eqnarray*}
The first and the last terms are upper bounded by $\varepsilon$ due to $(1\mbox{-}ii)$. 

Furthermore, since $$N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}} = N_{x_0^1}\pa{T+\delta} = N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}},$$
by applying \eqref{etaconst1} and using $(1\mbox{-}i)$ and $(2\mbox{-}i)$.
$$\left| N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}} - N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}}\right|=0.$$

%Indeed, since $T\in x_0^2$,
%$$ \left|\lambda_n^1\pa{T+\delta} - \pa{T+\delta}\right| \leq \frac{\eta}{4} < \eta$$ 
%implies $N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}} = N_{x_0^1}\pa{T+\delta}$, and
%\begin{eqnarray*}
%\left| \lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta} - \pa{T+\delta}\right| 
%&\leq& \underbrace{\left| \lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta} - \pa{\pa{\lambda_n^2}^{-1}(T)+\delta}\right|}_{\leq \frac{\eta}{4} \textrm{ by }(1\mbox{-}i)} 
%+ \underbrace{\left|\pa{\lambda_n^2}^{-1}(T)+\not{\delta} - \pa{T+\not{\delta}}\right|}_{\leq \frac{\eta}{4}\textrm{ by }(2\mbox{-}i)}\\ 
%& < & \eta
%\end{eqnarray*}
%implies $N_{x_0^1}\pa{\lambda_n^1\pa{\pa{\lambda_n^2}^{-1}(T)+\delta}} = N_{x_0^1}\pa{T+\delta} = N_{x_0^1}\pa{\lambda_n^1\pa{T+\delta}}.$

So finally, 
\begin{equation}\label{Bn4}
\left|B_{n,4}\right| \leq 2\varepsilon \#{x_0^2}.
\end{equation}

Combining \eqref{controlBn}, \eqref{Bn1}, \eqref{Bn2}, \eqref{Bn3}, and \eqref{Bn4}, we can conclude that:
\begin{equation}\label{majoBn}
|B_n|\leq 3\varepsilon \#x_{n}^1+  10\varepsilon  \#x_0^2.
\end{equation}

We now just need to remark that $\pa{\#{x_n^1}}_{n\geq n_0}$ is bounded because it converges to $\#{x_0^1}$. 
Indeed, since $\#{x_n^1}=N_{x_n^1}(1)$, $\#{x_0^1}=N_{x_0^1}(1)$ and for every $n$, $\lambda_n^1(1) = 1$, 
\begin{eqnarray*}
\left|\#{x_n^1} - \#{x_0^1} \right| &=& \left| N_{x_n^1}(1) - N_{x_0^1}(1)\right| \\
									&=& \left| N_{x_n^1}(1) - N_{x_0^1}\pa{\lambda_n^1(1)}\right| \\
									&\cv{n\to \infty}& 0 .
\end{eqnarray*}
With \eqref{controlh}, \eqref{majoAn}, and \eqref{majoBn}, this concludes the proof of Proposition \ref{hinH}.

 
 
 
% ---------------------------------------------------------------------------------------------------- %
\section*{Acknowledgments}
% ---------------------------------------------------------------------------------------------------- %

We are grateful to F. Grammont for fruitful discussions. This research is partly supported by the french Agence Nationale de la Recherche (ANR 2011 BS01 010 01 projet Calibration) and by the PEPS BMI 2012-2013 {\it Estimation of dependence graphs for thalamo-cortical neurons and multivariate Hawkes processes.}


\begin{thebibliography}{10}

\bibitem{agbst}
M.~A. Arcones and E.~Giné.
\newblock On the bootstrap of {U} and {V} statistics.
\newblock {\em The {A}nnals of {S}tatistics}, 20(2):655--674, 1992.

\bibitem{Arlot1}
S.~Arlot, G.~Blanchard, and E.~Roquain.
\newblock Some nonasymptotic results on resampling in high dimension, i:
  Confidence regions.
\newblock {\em The Annals of Statistics}, pages 51--82, 2010.

\bibitem{Arlot2}
S.~Arlot, G.~Blanchard, and E.~Roquain.
\newblock Some nonasymptotic results on resampling in high dimension, ii:
  Multiple tests.
\newblock {\em The Annals of Statistics}, pages 83--99, 2010.

\bibitem{Bickel-Freedman}
P.~J. Bickel and D.~A. Freedman.
\newblock Some asymptotic theory for the bootstrap.
\newblock {\em The Annals of Statistics}, 9(6):1196--1217, 1981.

\bibitem{Billingsley2}
P.~Billingsley.
\newblock {\em Convergence of probability measures}, volume 493.
\newblock Wiley-Interscience, 2009.

\bibitem{BKR61}
J.~R. Blum, J.~Kiefer, and M.~Rosenblatt.
\newblock Distribution free tests of independence based on the sample
  distribution function.
\newblock {\em The {A}nnals of {M}athematical {S}tatistics}, 32(2):485--498,
  1961.

\bibitem{BM}
P.~Br{\'e}maud and L.~Massouli{\'e}.
\newblock Stability of nonlinear {H}awkes processes.
\newblock {\em The Annals of Probability}, 24(3):1563--1588, 1996.

\bibitem{Bretagnolle}
J.~Bretagnolle.
\newblock Lois limites du bootstrap de certaines fonctionnelles.
\newblock {\em {A}nnales de l'institut {H}enri {P}oincaré ({B})
  {P}robabilités et {S}tatistiques}, 19(3):281--296, 1983.

\bibitem{Brown}
B.~M. Brown.
\newblock Martingale central limit theorems.
\newblock {\em The Annals of Mathematical Statistics}, pages 59--66, 1971.

\bibitem{CSWH}
L.~Carstensen, A.~Sandelin, O.~Winther, and N.~R. Hansen.
\newblock Multivariate {H}awkes process models of the occurrence of regulatory
  elements and an analysis of the pilot {ENCODE} regions.
\newblock {\em BMC Bioinformatics}, 11(456), 2010.

\bibitem{ChungRomano}
E.~Chung and J.~P. Romano.
\newblock Exact and asymptotically robust permutation tests.
\newblock {\em The {A}nnals of {S}tatistics}, 41(2):484--507, 2013.

\bibitem{DV}
D.~J. Daley and D.~Vere-Jones.
\newblock {\em An introduction to the theory of point processes. {V}ol. {I}}.
\newblock Probability and its Applications (New York). Springer-Verlag, New
  York, second edition, 2003.
\newblock Elementary theory and methods.

\bibitem{Dehling94}
H.~Dehling and T.~Mikosch.
\newblock Random quadratic forms and the bootstrap for {U}-statistics.
\newblock {\em Journal of Multivariate Analysis}, 51(2):392--413, 1994.

\bibitem{Dudley}
R.~M. Dudley.
\newblock {\em Real Analysis and Probability}.
\newblock Number~74 in Cambridge studies in advanced mathematics. Cambridge
  University Press, 2nd edition, 2002.

\bibitem{Efron}
B.~Efron.
\newblock Bootstrap methods: another look at the jackknife.
\newblock {\em The {A}nnals of {S}tatistics}, 7(1):1--26, 1979.

\bibitem{Fisher}
R.~A. Fisher.
\newblock {\em The design of experiments}.
\newblock 1935.

\bibitem{Fromont}
M.~Fromont.
\newblock Model selection by bootstrap penalization for classification.
\newblock {\em Machine Learning}, pages 165--207, 2007.

\bibitem{FLRB}
M.~Fromont, B.~Laurent, and P.~Reynaud-Bouret.
\newblock The two-sample problem for poisson processes: Adaptive tests with a
  nonasymptotic wild bootstrap approach.
\newblock {\em The Annals of Statistics}, pages 1431--1461, 2013.

\bibitem{GerPerkel}
G.~L. Gerstein and D.~H. Perkel.
\newblock Simultaneous recorded trains of action potentials: analysis and
  functional interpretation.
\newblock {\em Science}, 164:828--830, 1969.

\bibitem{Gretton2}
A.~Gretton, K.~Fukumizu, C.~H. Teo, L.~Song, B.~Schölkopf, and A.~J. Smola.
\newblock A kernel statistical test of independence.
\newblock {\em {NIPS} {P}roceedings}, 20:585--592, 2007.

\bibitem{Gretton3}
A.~Gretton and L.~Gy{ö}rfi.
\newblock Consistent nonparametric tests of independence.
\newblock {\em The {J}ournal of {M}achine {L}earning {R}esearch},
  11:1391--1423, 2010.

\bibitem{Gretton1}
A.~Gretton, R.~Herbrich, A.~J. Smola, O.~Bousquet, and B.~Schölkopf.
\newblock Kernel methods for measuring independence.
\newblock {\em The {J}ournal of {M}achine {L}earning {R}esearch}, 6:2075--2129,
  2005.

\bibitem{Grunt}
S.~Gr{\"u}n.
\newblock {\em Unitary joint-events in multiple-neuron spiking activity:
  {D}etection, significance and interpretation.}
\newblock PhD thesis, Thun: Verlag Harri Deutsch, 1996.

\bibitem{grunrev}
S.~Gr\"un.
\newblock Data-driven significance estimation for precise spike correlation.
\newblock {\em Journal of Neurophysiology}, 101:1126--1140, 2009.

\bibitem{GrunB}
S.~Gr{\"u}n, M.~Diesmann, and A.~M. Aertsen.
\newblock {\em Analysis of parallel spike trains}, chapter {U}nitary {E}vents
  analysis.
\newblock {S}pringer {S}eries in {C}omputational {N}euroscience, 2010.

\bibitem{GDGRA}
S.~Gr{\"u}n, M.~Diesmann, F.~Grammont, A.~Riehle, and A.~M. Aertsen.
\newblock Detecting unitary events without discretization of time.
\newblock {\em {J}ournal of {N}euroscience {M}ethods}, 93:67--79, 1999.

\bibitem{GS}
G.~Gusto and S.~Schbath.
\newblock F{ADO}: a statistical method to detect favored or avoided distances
  between occurrences of motifs using the {H}awkes' model.
\newblock {\em Statistical Applications in Genetics and Molecular Biology},
  4:Art. 24, 28 pp. (electronic), 2005.

\bibitem{HRBR}
N.~R. Hansen, P.~Reynaud-Bouret, and V.~Rivoirard.
\newblock {L}asso and probabilistic inequalities for multivariate point
  processes.
\newblock {\em To appear in Bernoulli}, 2014.
\newblock http://arxiv.org/abs/1208.0570.

\bibitem{Hoeffding48bis}
W.~Hoeffding.
\newblock A class of statistics with asymptotically normal distribution.
\newblock {\em The Annals of Mathematical Statistics}, 19(3):293--325, 1948.

\bibitem{Hoeffding48}
W.~Hoeffding.
\newblock A non-parametric test of independence.
\newblock {\em The {A}nnals of {M}athematical {S}tatistics}, 19(4):546--557,
  1948.

\bibitem{Hoeffding52}
W.~Hoeffding.
\newblock The large-sample power of tests based on permutation of the
  observations.
\newblock {\em The Annals of Mathematical Statistics}, 23(2):169--192, 1952.

\bibitem{Hoeffding61}
W.~Hoeffding.
\newblock The strong law of large numbers for {U}-statistics.
\newblock Institute of Statistics, Mimeograph Series No. 302, 1961.

\bibitem{HorvathHuskova}
L.~Horv{á}th and M.~Hu{š}kov{á}.
\newblock Testing for changes using permutations of {U}-statistics.
\newblock {\em Journal of {S}tatistical {P}lanning and {I}nference},
  128(2):351--371, 2005.

\bibitem{Hot}
H.~Hotelling and M.~R. Pabst.
\newblock Rank correlation and tests of significance involving no assumption of
  normality.
\newblock {\em The {A}nnals of {M}athematical {S}tatistics}, 7(1):29--43, 1936.

\bibitem{Kendall}
M.~G. Kendall.
\newblock A new measure of rank correlation.
\newblock {\em Biometrika}, 30(1--2):81--93, 1938.

\bibitem{LeuchtNeumann09}
A.~Leucht and M.~H. Neumann.
\newblock Consistency of general bootstrap methods for degenerate {U}-type and
  {V}-type statistics.
\newblock {\em Journal of Multivariate Analysis}, 100(8):1622--1633, 2009.

\bibitem{ogatathin}
Y.~Ogata.
\newblock On {L}ewis' simulation method for point processes.
\newblock {\em IEEE Transactions on Information Theory}, 27(1):23--31, 1981.

\bibitem{Pearson1}
K.~Pearson.
\newblock On the criterion that a given system of deviations from the probable
  in the case of a correlated system of variables is such that it can be
  reasonably supposed to have arisen from random sampling.
\newblock {\em The {L}ondon, {E}dinburgh, and {D}ublin {P}hilosophical
  {M}agazine and {J}ournal of {S}cience}, 50(302):157--175, 1900.

\bibitem{Pearson2}
K.~Pearson.
\newblock On the probability that two independent distributions of frequency
  are really samples from the same population.
\newblock {\em {B}iometrika}, 8:250--254, 1911.

\bibitem{PSCR}
V.~Pernice, B.~Staude, S.~Cardanobile, and S.~Rotter.
\newblock How structure determines correlations in neuronal networks.
\newblock {\em {PLoS} {C}omputational {B}iology}, 7(5):e1002059, 2012.

\bibitem{Pipaet2003}
G.~Pipa, M.~Diesmann, and S.~Gr{\"u}n.
\newblock Significance of joint-spike events based on trial-shuffling by
  efficient combinatorial methods.
\newblock {\em {C}omplexity}, 8(4):1--8, 2003.

\bibitem{Pipa2003}
G.~Pipa and S.~Gr{\"u}n.
\newblock Non-parametric significance estimation of joint-spike events by
  shuffling and resampling.
\newblock {\em {N}eurocomputing}, 52--54:31--37, 2003.

\bibitem{Pitman1}
E.~J. Pitman.
\newblock Significance tests which may be applied to samples from any
  populations.
\newblock {\em Suppl. {J}journal of the {R}oyal {S}statistical {S}society},
  4:117--130, 1937.

\bibitem{Pitman2}
E.~J. Pitman.
\newblock Significance tests which may be applied to samples from any
  populations: {II}. {T}he correlation coefficient test.
\newblock {\em Suppl. {J}journal of the {R}oyal {S}statistical {S}society},
  4:225--232, 1937.

\bibitem{Pitman3}
E.~J. Pitman.
\newblock Significance tests which may be applied to samples from any
  populations: {III}. {T}he analysis of variance test.
\newblock {\em Biometrika}, 29:322--335, 1938.

\bibitem{Pouzat09}
C.~Pouzat and A.~Chaffiol.
\newblock Automatic spike train analysis and report generation. an
  implementation with r, r2html and star.
\newblock {\em Journal of Neuroscience Methods}, pages 119--144, 2009.

\bibitem{RRGT}
P.~Reynaud-Bouret, V.~Rivoirard, F.~Grammont, and C.~Tuleau-Malot.
\newblock Goodness-of-fit tests and nonparametric adaptive estimation for spike
  train analysis.
\newblock Technical report, http://hal.archives-ouvertes.fr/hal-00789127, 2013.

\bibitem{ieee}
P.~Reynaud-Bouret, V.~Rivoirard, and C.~Tuleau-Malot.
\newblock Inference of functional connectivity in neurosciences via {H}awkes
  processes.
\newblock In {\em 1st IEEE Global Conference on Signal and Information
  Processing}, 2013, Austin Texas.

\bibitem{RomanoTR}
J.~P. Romano.
\newblock Bootstrap and randomization tests of some nonparametric hypotheses.
\newblock Technical Report 270, Dept. Statistics, Standford Univ., 1987.

\bibitem{Romano89}
J.~P. Romano.
\newblock Bootstrap and randomization tests of some nonparametric hypotheses.
\newblock {\em The Annals of Statistics}, 17(1):141--159, 1989.

\bibitem{RomanoWolf}
J.~P. Romano and M.~Wolf.
\newblock Exact and approximate stepdown methods for multiple hypothesis
  testing.
\newblock {\em Journal of the American Statistical Association},
  100(469):94--108, 2005.

\bibitem{RubinVitale80}
H.~Rubin and R.~A. Vitale.
\newblock Asymptotic distribution of symmetric statistics.
\newblock {\em The Annals of Statistics}, pages 165--170, 1980.

\bibitem{LaureCh}
L.~Sansonnet and C.~Tuleau-Malot.
\newblock A model of {P}oissonian interactions and detection of dependence.
\newblock Technical report, http://hal.archives-ouvertes.fr/hal-00780598, 2013.

\bibitem{Scheffe}
H.~Scheffe.
\newblock Statistical inference in the non-parametric case.
\newblock {\em The {A}nnals of {M}athematical {S}tatistics}, 14(4):305--332,
  1943.

\bibitem{Schilling05}
R.~L. Schilling.
\newblock {\em Measures, integrals and martingales}, volume~13.
\newblock Cambridge University Press, 2005.

\bibitem{Singer93}
W.~Singer.
\newblock Synchronization of cortical activity and its putative role in
  information processing and learning.
\newblock {\em Annual Review of Physiology}, 55:349--374, 1993.

\bibitem{MTGAUE}
C.~Tuleau-Malot, A.~Rouis, F.~Grammont, and P.~Reynaud-Bouret.
\newblock Multiple tests based on a {G}aussian approximation of the {U}nitary
  {E}vents method.
\newblock Technical report, http://hal.archives-ouvertes.fr/hal-00757323, 2012.

\bibitem{vdvwellner96}
A.~W. Van~der Vaart and J.~A. Wellner.
\newblock {\em Weak Convergence and Empirical Processes}.
\newblock Springer, New York, 1996.

\bibitem{vandervaart}
A.W. Van~der Vaart.
\newblock {\em Asymptotic statistics}.
\newblock Cambridge series in Statistical and Probabilistic Mathematics, 1998.

\bibitem{Varadarajan58}
V.~S. Varadarajan.
\newblock On the convergence of sample probability distributions.
\newblock {\em Sankhyà: {T}he {I}ndian {J}ournal of {S}tatistics (1933-1960)},
  19(1--2):23--26, 1958.

\bibitem{venturaboot}
V.~Ventura.
\newblock {\em Analysis of parallel spike trains}, chapter Bootstrap tests of
  hypotheses.
\newblock {S}pringer {S}eries in {C}omputational {N}euroscience, 2010.

\bibitem{Wolfowitz}
J.~Wolfowitz.
\newblock Additive partition functions and a class of statistical hypotheses.
\newblock {\em The {A}nnals of Mathematical {S}tatistics}, 13(3):247--279,
  2042.

\end{thebibliography}




\end{document}












